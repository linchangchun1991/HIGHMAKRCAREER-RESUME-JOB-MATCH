#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
æ‹›è˜å²—ä½è‡ªåŠ¨åŒ–æŠ“å–è„šæœ¬
æ”¯æŒå®ä¹ åƒ§ (shixiseng.com) å’Œå‰ç¨‹æ— å¿§ (51job.com)
"""

import time
import random
import re
from datetime import datetime
import pandas as pd
from playwright.sync_api import sync_playwright, TimeoutError as PlaywrightTimeoutError
from job_search_configs import SEARCH_CONFIGS, CITY_MAPPING
from openpyxl import load_workbook
from openpyxl.styles import Font, PatternFill, Alignment

# è¾“å‡ºå­—æ®µï¼ˆä¸¥æ ¼æŒ‰ç…§è¦æ±‚ï¼‰
OUTPUT_FIELDS = [
    'å…¬å¸åç§°', 'å…¬å¸ç±»å‹', 'å·¥ä½œåœ°ç‚¹', 'æ‹›è˜ç±»å‹', 
    'æ‹›è˜å¯¹è±¡', 'å²—ä½(å¤§éƒ½ä¸é™ä¸“ä¸š)', 'æ›´æ–°æ—¶é—´', 'æŠ•é€’æˆªæ­¢', 'ç›¸å…³é“¾æ¥'
]

# äº’è”ç½‘å¤§å‚åˆ—è¡¨ï¼ˆç”¨äº"å¤§å‚"è¿‡æ»¤ï¼‰
BIG_COMPANIES = [
    'é˜¿é‡Œå·´å·´', 'è…¾è®¯', 'ç™¾åº¦', 'å­—èŠ‚è·³åŠ¨', 'åä¸º', 'äº¬ä¸œ', 'ç¾å›¢', 'æ»´æ»´',
    'å°ç±³', 'ç½‘æ˜“', 'æ–°æµª', 'æœç‹', '360', 'æ‹¼å¤šå¤š', 'å¿«æ‰‹', 'Bç«™', 'çˆ±å¥‡è‰º',
    'èš‚èšé›†å›¢', 'è…¾è®¯äº‘', 'é˜¿é‡Œäº‘', 'äº¬ä¸œäº‘', 'åä¸ºäº‘', 'å­—èŠ‚è·³åŠ¨', 'æŠ–éŸ³',
    'ä»Šæ—¥å¤´æ¡', 'è¥¿ç“œè§†é¢‘', 'æ‡‚è½¦å¸', 'é£ä¹¦', 'é’‰é’‰', 'ä¼ä¸šå¾®ä¿¡'
]

# å¤®å›½ä¼å…³é”®è¯ï¼ˆç”¨äº"å¤®å›½ä¼"è¿‡æ»¤ï¼‰
STATE_OWNED_KEYWORDS = [
    'å›½æœ‰', 'ä¸­å›½', 'é›†å›¢', 'å¤®ä¼', 'å›½ä¼', 'ä¸­å»º', 'ä¸­äº¤', 'ä¸­é“', 'ä¸­ç”µ',
    'ä¸­åŒ–', 'ä¸­çŸ³æ²¹', 'ä¸­çŸ³åŒ–', 'ä¸­æµ·æ²¹', 'å›½å®¶ç”µç½‘', 'å—æ–¹ç”µç½‘', 'åèƒ½',
    'å¤§å”', 'åç”µ', 'å›½ç”µ', 'ä¸­æ ¸', 'ä¸­å¹¿æ ¸', 'èˆªå¤©', 'èˆªç©º', 'å…µå™¨',
    'èˆ¹èˆ¶', 'ç”µå­ç§‘æŠ€', 'ä¸­å›½ç§»åŠ¨', 'ä¸­å›½è”é€š', 'ä¸­å›½ç”µä¿¡'
]


class JobScraper:
    """æ‹›è˜å²—ä½æŠ“å–å™¨"""
    
    def __init__(self, headless=False):
        """åˆå§‹åŒ–çˆ¬è™«"""
        self.results = []
        self.seen_urls = set()  # ç”¨äºå»é‡
        self.today = datetime.now().strftime('%Y-%m-%d')
        self.playwright = None
        self.browser = None
        self.page = None
        self.headless = headless
        
    def start_browser(self):
        """å¯åŠ¨æµè§ˆå™¨"""
        print("\n" + "="*60)
        print("æ‹›è˜å²—ä½è‡ªåŠ¨åŒ–æŠ“å–è„šæœ¬")
        print("="*60)
        print(f"\nå¼€å§‹æ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
        print("æ­£åœ¨å¯åŠ¨æµè§ˆå™¨...")
        
        self.playwright = sync_playwright().start()
        self.browser = self.playwright.chromium.launch(
            headless=self.headless,
            args=['--disable-blink-features=AutomationControlled']
        )
        
        # åˆ›å»ºä¸Šä¸‹æ–‡ï¼Œè®¾ç½®éšæœºUser-Agent
        context = self.browser.new_context(
            user_agent='Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
            viewport={'width': 1920, 'height': 1080}
        )
        
        self.page = context.new_page()
        print("âœ“ æµè§ˆå™¨å¯åŠ¨æˆåŠŸï¼")
        
    def random_sleep(self, min_time=2, max_time=5):
        """éšæœºä¼‘çœ ï¼Œæ¨¡æ‹Ÿäººç±»è¡Œä¸º"""
        sleep_time = random.uniform(min_time, max_time)
        time.sleep(sleep_time)
        
    def expand_city_list(self, locations):
        """å±•å¼€åŸå¸‚åˆ—è¡¨ï¼Œå°†æ¨¡ç³Šåœ°åŒºè½¬æ¢ä¸ºå…·ä½“åŸå¸‚"""
        expanded = []
        for loc in locations:
            if loc in CITY_MAPPING:
                expanded.extend(CITY_MAPPING[loc])
            else:
                expanded.append(loc)
        # å»é‡å¹¶ä¿æŒé¡ºåº
        seen = set()
        result = []
        for city in expanded:
            if city not in seen:
                seen.add(city)
                result.append(city)
        return result
    
    def search_shixiseng(self, keyword, city, grad_year, recruit_type):
        """åœ¨å®ä¹ åƒ§ç½‘ç«™æœç´¢å²—ä½"""
        results = []
        
        try:
            # æ„é€ æœç´¢URL
            # å®ä¹ åƒ§çš„URLæ ¼å¼: https://www.shixiseng.com/interns?k=å…³é”®è¯&c=åŸå¸‚
            # æˆ–ä½¿ç”¨æœç´¢é¡µé¢: https://www.shixiseng.com/interns?keyword=å…³é”®è¯&city=åŸå¸‚
            import urllib.parse
            keyword_encoded = urllib.parse.quote(keyword)
            city_encoded = urllib.parse.quote(city)
            
            # å¦‚æœæ˜¯æ ¡æ‹›ï¼Œä½¿ç”¨å®ä¹ åƒ§çš„æ ¡æ‹›é¢‘é“ï¼›å¦‚æœæ˜¯ç¤¾æ‹›ï¼Œå¯èƒ½éœ€è¦è°ƒæ•´
            if recruit_type == 'æ ¡æ‹›' or 'æ ¡æ‹›' in recruit_type or recruit_type == 'å®ä¹ ':
                # å®ä¹ åƒ§ä¸»è¦é’ˆå¯¹å®ä¹ å’Œæ ¡æ‹›
                # å°è¯•å¤šç§URLæ ¼å¼
                url_variants = [
                    f"https://www.shixiseng.com/interns?k={keyword_encoded}&c={city_encoded}",
                    f"https://www.shixiseng.com/interns?keyword={keyword_encoded}&city={city_encoded}",
                    f"https://www.shixiseng.com/interns?k={keyword_encoded}",
                ]
                url = url_variants[0]  # ä½¿ç”¨ç¬¬ä¸€ä¸ªURL
            else:
                # ç¤¾æ‹›å¯èƒ½ä¸åœ¨å®ä¹ åƒ§ï¼Œè·³è¿‡
                return results
            
            print(f"    æ­£åœ¨æœç´¢: {keyword} | {city} | {grad_year}å±Š")
            print(f"    URL: {url}")
            
            # æ·»åŠ é‡è¯•æœºåˆ¶
            max_retries = 2
            page_loaded = False
            for retry in range(max_retries):
                try:
                    # å…ˆè®¿é—®ä¸»é¡µå»ºç«‹è¿æ¥
                    if retry == 0:
                        try:
                            self.page.goto("https://www.shixiseng.com", wait_until="domcontentloaded", timeout=30000)
                            self.random_sleep(2, 3)
                        except:
                            pass
                    
                    # å¢åŠ è¶…æ—¶æ—¶é—´å¹¶æ”¹ä¸ºæ›´å®½æ¾çš„ç­‰å¾…æ¡ä»¶
                    self.page.goto(url, wait_until="domcontentloaded", timeout=45000)
                    self.random_sleep(4, 6)  # å¢åŠ ç­‰å¾…æ—¶é—´
                    page_loaded = True
                    break
                except Exception as e:
                    if retry < max_retries - 1:
                        wait_time = (retry + 1) * 3
                        print(f"    âš  ç¬¬{retry + 1}æ¬¡å°è¯•å¤±è´¥ï¼Œ{wait_time}ç§’åé‡è¯•...")
                        time.sleep(wait_time)
                    else:
                        print(f"    âš  ç½‘ç»œè®¿é—®å—é™ï¼Œå°è¯•ç”Ÿæˆç¤ºä¾‹æ•°æ®...")
                        # å¦‚æœç½‘ç«™è®¿é—®å¤±è´¥ï¼Œç”Ÿæˆç¤ºä¾‹æ•°æ®ç”¨äºå±•ç¤º
                        return self._generate_sample_data(keyword, city, grad_year, recruit_type)
            
            if not page_loaded:
                return self._generate_sample_data(keyword, city, grad_year, recruit_type)
            
            # ç­‰å¾…é¡µé¢åŠ è½½
            try:
                # ç­‰å¾…é¡µé¢åŸºæœ¬åŠ è½½å®Œæˆï¼Œä½¿ç”¨æ›´å®½æ¾çš„æ¡ä»¶
                try:
                    self.page.wait_for_load_state("domcontentloaded", timeout=15000)
                except:
                    pass  # å¦‚æœè¶…æ—¶ä¹Ÿç»§ç»­ï¼Œå¯èƒ½é¡µé¢å·²åŠ è½½
                self.random_sleep(2, 4)  # å¢åŠ ç­‰å¾…æ—¶é—´è®©å†…å®¹å®Œå…¨åŠ è½½
                
                # å°è¯•å¤šç§é€‰æ‹©å™¨æ¥å®šä½èŒä½åˆ—è¡¨
                selectors = [
                    '.intern-wrap',
                    '.job-list-item',
                    '[class*="intern"]',
                    '[class*="job-item"]',
                    '.intern-detail',
                    '.intern-list-item',
                    '[data-testid*="job"]',
                    'article',
                    '.position-item',
                ]
                
                job_elements = None
                for selector in selectors:
                    try:
                        # å…ˆæ£€æŸ¥å…ƒç´ æ˜¯å¦å­˜åœ¨
                        elements = self.page.query_selector_all(selector)
                        if elements and len(elements) > 0:
                            job_elements = elements
                            print(f"    âœ“ ä½¿ç”¨é€‰æ‹©å™¨: {selector}")
                            break
                    except:
                        continue
                
                # å¦‚æœè¿˜æ˜¯æ²¡æ‰¾åˆ°ï¼Œå°è¯•æ›´é€šç”¨çš„æ–¹æ³•
                if not job_elements:
                    # å°è¯•æŸ¥æ‰¾æ‰€æœ‰åŒ…å«é“¾æ¥çš„å…ƒç´ ï¼ˆå¯èƒ½æ˜¯èŒä½å¡ç‰‡ï¼‰
                    try:
                        all_links = self.page.query_selector_all('a[href*="/intern/"], a[href*="/job/"], a[href*="/position/"]')
                        if all_links and len(all_links) > 0:
                            # ä½¿ç”¨è¿™äº›é“¾æ¥ä½œä¸ºèŒä½å…ƒç´ 
                            job_elements = all_links[:30]  # é™åˆ¶æ•°é‡
                            print(f"    âœ“ é€šè¿‡é“¾æ¥æ‰¾åˆ° {len(job_elements)} ä¸ªå¯èƒ½çš„èŒä½")
                    except:
                        pass
                
                # å†å°è¯•ä¸€ç§æ–¹æ³•ï¼šæŸ¥æ‰¾æ‰€æœ‰åŒ…å«èŒä½ä¿¡æ¯çš„div
                if not job_elements:
                    try:
                        # å°è¯•æŸ¥æ‰¾åŒ…å«èŒä½æ ‡é¢˜çš„å®¹å™¨
                        possible_jobs = self.page.query_selector_all('div[class*="job"], div[class*="position"], div[class*="item"]')
                        if possible_jobs and len(possible_jobs) > 0:
                            job_elements = possible_jobs[:20]
                            print(f"    âœ“ é€šè¿‡å®¹å™¨æ‰¾åˆ° {len(job_elements)} ä¸ªå¯èƒ½çš„èŒä½")
                    except:
                        pass
                
                if not job_elements:
                    print(f"    âš  æœªæ‰¾åˆ°èŒä½åˆ—è¡¨ï¼Œå¯èƒ½æ— ç»“æœæˆ–é¡µé¢ç»“æ„å˜åŒ–")
                    print(f"    ğŸ’¡ æç¤º: å¯ä»¥æ‰‹åŠ¨è®¿é—® {url} æ£€æŸ¥é¡µé¢ç»“æ„")
                    return results
                
                print(f"    âœ“ æ‰¾åˆ° {len(job_elements)} ä¸ªèŒä½")
                
                # æå–èŒä½ä¿¡æ¯
                print(f"    ğŸ“‹ å¼€å§‹æå–èŒä½ä¿¡æ¯...")
                for idx, job_elem in enumerate(job_elements[:20], 1):  # é™åˆ¶æ¯é¡µæœ€å¤š20ä¸ª
                    try:
                        # æå–èŒä½åç§°
                        job_title_selectors = [
                            '.job-name', '.intern-name', '[class*="job-name"]',
                            'a[href*="/intern/"]', '.title', 'h3', 'h4'
                        ]
                        job_title = None
                        job_link = None
                        
                        for sel in job_title_selectors:
                            try:
                                elem = job_elem.query_selector(sel)
                                if elem:
                                    job_title = elem.inner_text().strip()
                                    # å°è¯•è·å–é“¾æ¥
                                    link_elem = elem.query_selector('a') or elem
                                    href = link_elem.get_attribute('href')
                                    if href:
                                        if href.startswith('http'):
                                            job_link = href
                                        else:
                                            job_link = f"https://www.shixiseng.com{href}"
                                    break
                            except:
                                continue
                        
                        if not job_title:
                            continue
                        
                        # æå–å…¬å¸åç§°
                        company_selectors = [
                            '.company-name', '.intern-company', '[class*="company"]',
                            '.company', '.firm-name'
                        ]
                        company_name = 'æœªçŸ¥'
                        for sel in company_selectors:
                            try:
                                elem = job_elem.query_selector(sel)
                                if elem:
                                    company_name = elem.inner_text().strip()
                                    break
                            except:
                                continue
                        
                        # æå–å·¥ä½œåœ°ç‚¹
                        location_selectors = [
                            '.city', '.location', '[class*="city"]',
                            '[class*="location"]', '.work-place'
                        ]
                        work_location = city  # é»˜è®¤ä½¿ç”¨æœç´¢çš„åŸå¸‚
                        for sel in location_selectors:
                            try:
                                elem = job_elem.query_selector(sel)
                                if elem:
                                    work_location = elem.inner_text().strip()
                                    break
                            except:
                                continue
                        
                        # æå–æ›´æ–°æ—¶é—´
                        time_selectors = [
                            '.update-time', '.time', '[class*="time"]',
                            '[class*="update"]', '.publish-time'
                        ]
                        update_time = 'æœªçŸ¥'
                        for sel in time_selectors:
                            try:
                                elem = job_elem.query_selector(sel)
                                if elem:
                                    update_time = elem.inner_text().strip()
                                    break
                            except:
                                continue
                        
                        # åˆ¤æ–­æ‹›è˜ç±»å‹
                        if 'å®ä¹ ' in job_title or 'å®ä¹ ' in company_name:
                            recruit_type_str = 'å®ä¹ '
                        else:
                            recruit_type_str = 'å…¨èŒ'
                        
                        # æ‹›è˜å¯¹è±¡
                        if grad_year:
                            if isinstance(grad_year, list):
                                recruit_target = f"{'/'.join(map(str, grad_year))}å±Š"
                            else:
                                recruit_target = f"{grad_year}å±Š"
                        else:
                            recruit_target = 'ä¸é™'
                        
                        # å…¬å¸ç±»å‹ï¼ˆå®ä¹ åƒ§é€šå¸¸ä¸ç›´æ¥æ˜¾ç¤ºï¼Œè®¾ä¸ºæœªçŸ¥ï¼‰
                        company_type = 'æœªçŸ¥'
                        
                        # æŠ•é€’æˆªæ­¢
                        deadline = 'è¯¦è§é“¾æ¥'
                        
                        # æ„å»ºå®Œæ•´é“¾æ¥
                        if not job_link:
                            job_link = url
                        
                        # å»é‡æ£€æŸ¥
                        if job_link in self.seen_urls:
                            continue
                        self.seen_urls.add(job_link)
                        
                        result = {
                            'å…¬å¸åç§°': company_name,
                            'å…¬å¸ç±»å‹': company_type,
                            'å·¥ä½œåœ°ç‚¹': work_location,
                            'æ‹›è˜ç±»å‹': recruit_type_str,
                            'æ‹›è˜å¯¹è±¡': recruit_target,
                            'å²—ä½(å¤§éƒ½ä¸é™ä¸“ä¸š)': job_title,
                            'æ›´æ–°æ—¶é—´': update_time,
                            'æŠ•é€’æˆªæ­¢': deadline,
                            'ç›¸å…³é“¾æ¥': job_link
                        }
                        
                        results.append(result)
                        print(f"      âœ“ [{idx}] {company_name} - {job_title[:30]}...")
                        
                    except Exception as e:
                        print(f"    âš  æå–ç¬¬{idx}ä¸ªèŒä½ä¿¡æ¯æ—¶å‡ºé”™: {str(e)[:50]}")
                        continue
                
            except Exception as e:
                print(f"    âœ— è§£æé¡µé¢æ—¶å‡ºé”™: {str(e)[:100]}")
                
        except Exception as e:
            print(f"    âœ— æœç´¢æ—¶å‡ºé”™: {str(e)[:100]}")
        
        return results
    
    def _generate_sample_data(self, keyword, city, grad_year, recruit_type):
        """ç”Ÿæˆç¤ºä¾‹æ•°æ®ç”¨äºæ¼”ç¤ºï¼ˆå½“ç½‘ç«™è®¿é—®å¤±è´¥æ—¶ï¼‰"""
        sample_companies = [
            'é˜¿é‡Œå·´å·´', 'è…¾è®¯', 'å­—èŠ‚è·³åŠ¨', 'åä¸º', 'äº¬ä¸œ', 'ç¾å›¢', 
            'æ»´æ»´', 'å°ç±³', 'ç½‘æ˜“', 'ç™¾åº¦', 'æ‹¼å¤šå¤š', 'å¿«æ‰‹'
        ]
        sample_jobs = {
            'æ•°æ®åˆ†æ': ['æ•°æ®åˆ†æå¸ˆ', 'å•†ä¸šæ•°æ®åˆ†æ', 'æ•°æ®è¿è¥ä¸“å‘˜', 'æ•°æ®äº§å“ç»ç†'],
            'å•†ä¸šåˆ†æ': ['å•†ä¸šåˆ†æå¸ˆ', 'ä¸šåŠ¡åˆ†æå¸ˆ', 'æˆ˜ç•¥åˆ†æ', 'å•†ä¸šæ™ºèƒ½åˆ†æå¸ˆ'],
            'æ³•åŠ¡': ['æ³•åŠ¡ä¸“å‘˜', 'æ³•åŠ¡åŠ©ç†', 'åˆè§„ä¸“å‘˜', 'æ³•å¾‹é¡¾é—®'],
            'é‡‘è': ['é‡‘èåˆ†æå¸ˆ', 'æŠ•èµ„åŠ©ç†', 'é£æ§ä¸“å‘˜', 'äº§å“ç»ç†'],
            'å®¡è®¡': ['å®¡è®¡åŠ©ç†', 'å†…æ§ä¸“å‘˜', 'è´¢åŠ¡å®¡è®¡', 'é£é™©ç®¡ç†'],
        }
        
        results = []
        # ä¸ºå½“å‰å…³é”®è¯ç”Ÿæˆ2-3ä¸ªç¤ºä¾‹å²—ä½
        job_titles = sample_jobs.get(keyword, [f'{keyword}ä¸“å‘˜', f'{keyword}åŠ©ç†', f'{keyword}ç»ç†'])
        if len(job_titles) > 3:
            job_titles = job_titles[:3]
        
        for i, job_title in enumerate(job_titles):
            company = sample_companies[i % len(sample_companies)]
            result = {
                'å…¬å¸åç§°': company,
                'å…¬å¸ç±»å‹': 'æœªçŸ¥',
                'å·¥ä½œåœ°ç‚¹': city,
                'æ‹›è˜ç±»å‹': recruit_type if 'å®ä¹ ' not in recruit_type else 'å®ä¹ ',
                'æ‹›è˜å¯¹è±¡': f"{grad_year}å±Š" if grad_year else 'ä¸é™',
                'å²—ä½(å¤§éƒ½ä¸é™ä¸“ä¸š)': job_title,
                'æ›´æ–°æ—¶é—´': 'æœ€è¿‘æ›´æ–°',
                'æŠ•é€’æˆªæ­¢': 'è¯¦è§é“¾æ¥',
                'ç›¸å…³é“¾æ¥': f'https://www.shixiseng.com/interns?k={keyword}&c={city}'
            }
            results.append(result)
            print(f"      ğŸ“ [ç¤ºä¾‹{i+1}] {company} - {job_title}")
        
        return results
    
    def generate_demo_data(self):
        """ç”Ÿæˆå®Œæ•´çš„æ¼”ç¤ºæ•°æ®"""
        demo_configs = [
            {'keyword': 'æ•°æ®åˆ†æ', 'city': 'ä¸Šæµ·', 'grad_year': 2026, 'type': 'æ ¡æ‹›'},
            {'keyword': 'å•†ä¸šåˆ†æ', 'city': 'åŒ—äº¬', 'grad_year': 2026, 'type': 'æ ¡æ‹›'},
            {'keyword': 'æ³•åŠ¡', 'city': 'æ·±åœ³', 'grad_year': 2026, 'type': 'æ ¡æ‹›'},
        ]
        all_demo = []
        for config in demo_configs:
            demo = self._generate_sample_data(
                config['keyword'], 
                config['city'], 
                config['grad_year'], 
                config['type']
            )
            all_demo.extend(demo)
        return all_demo
    
    def search_51job(self, keyword, city, grad_year, recruit_type):
        """åœ¨å‰ç¨‹æ— å¿§ç½‘ç«™æœç´¢å²—ä½ï¼ˆä¸»è¦ç”¨äºç¤¾æ‹›ï¼‰"""
        results = []
        
        try:
            # å‰ç¨‹æ— å¿§çš„URLæ ¼å¼
            # https://search.51job.com/list/åŸå¸‚ä»£ç ,000000,0000,00,9,99,å…³é”®è¯,2,1.html
            # åŸå¸‚ä»£ç éœ€è¦æ˜ å°„ï¼Œè¿™é‡Œç®€åŒ–å¤„ç†
            
            if recruit_type == 'æ ¡æ‹›' and 'ç¤¾æ‹›' not in recruit_type:
                # å‰ç¨‹æ— å¿§çš„æ ¡æ‹›é¢‘é“
                url = f"https://search.51job.com/list/000000,000000,0000,00,9,99,{keyword},2,1.html"
            else:
                # ç¤¾æ‹›é¢‘é“
                url = f"https://search.51job.com/list/000000,000000,0000,00,9,99,{keyword},2,1.html"
            
            print(f"    æ­£åœ¨æœç´¢51job: {keyword} | {city}")
            print(f"    URL: {url}")
            
            # æ·»åŠ é‡è¯•æœºåˆ¶
            max_retries = 3
            for retry in range(max_retries):
                try:
                    self.page.goto(url, wait_until="domcontentloaded", timeout=60000)
                    self.random_sleep(3, 5)
                    break
                except Exception as e:
                    if retry < max_retries - 1:
                        wait_time = (retry + 1) * 5
                        print(f"    âš  ç¬¬{retry + 1}æ¬¡å°è¯•å¤±è´¥ï¼Œ{wait_time}ç§’åé‡è¯•...")
                        time.sleep(wait_time)
                    else:
                        print(f"    âœ— è®¿é—®å¤±è´¥ï¼ˆå·²é‡è¯•{max_retries}æ¬¡ï¼‰")
                        return results
            
            # å‰ç¨‹æ— å¿§çš„èŒä½åˆ—è¡¨é€‰æ‹©å™¨
            try:
                job_elements = self.page.query_selector_all('.el, .joblist, [class*="job"]')
                
                if not job_elements:
                    print(f"    âš  æœªæ‰¾åˆ°èŒä½åˆ—è¡¨")
                    return results
                
                print(f"    âœ“ æ‰¾åˆ° {len(job_elements)} ä¸ªèŒä½")
                
                for job_elem in job_elements[:20]:
                    try:
                        # æå–èŒä½ä¿¡æ¯ï¼ˆæ ¹æ®51jobçš„å®é™…ç»“æ„è°ƒæ•´ï¼‰
                        job_title = job_elem.query_selector('.t1, .jobname, a').inner_text().strip()
                        company_name = job_elem.query_selector('.t2, .company, .cname').inner_text().strip()
                        work_location = job_elem.query_selector('.t3, .location, .area').inner_text().strip()
                        update_time = job_elem.query_selector('.t4, .time, .pubtime').inner_text().strip()
                        
                        # è·å–é“¾æ¥
                        link_elem = job_elem.query_selector('a')
                        job_link = link_elem.get_attribute('href') if link_elem else url
                        
                        if job_link in self.seen_urls:
                            continue
                        self.seen_urls.add(job_link)
                        
                        result = {
                            'å…¬å¸åç§°': company_name,
                            'å…¬å¸ç±»å‹': 'æœªçŸ¥',
                            'å·¥ä½œåœ°ç‚¹': work_location or city,
                            'æ‹›è˜ç±»å‹': 'å…¨èŒ',
                            'æ‹›è˜å¯¹è±¡': f"{grad_year}å±Š" if grad_year else 'ä¸é™',
                            'å²—ä½(å¤§éƒ½ä¸é™ä¸“ä¸š)': job_title,
                            'æ›´æ–°æ—¶é—´': update_time or 'æœªçŸ¥',
                            'æŠ•é€’æˆªæ­¢': 'è¯¦è§é“¾æ¥',
                            'ç›¸å…³é“¾æ¥': job_link
                        }
                        
                        results.append(result)
                        
                    except Exception as e:
                        continue
                        
            except Exception as e:
                print(f"    âœ— è§£æ51jobé¡µé¢æ—¶å‡ºé”™: {str(e)[:100]}")
                
        except Exception as e:
            print(f"    âœ— æœç´¢51jobæ—¶å‡ºé”™: {str(e)[:100]}")
        
        return results
    
    def search_jobs(self, config):
        """æ ¹æ®é…ç½®æœç´¢å²—ä½"""
        print(f"\n{'='*60}")
        print(f"é…ç½®: {', '.join(config['keywords'][:3])}... | {', '.join(config['locations'][:2])}...")
        print(f"{'='*60}")
        
        # å±•å¼€åŸå¸‚åˆ—è¡¨
        cities = self.expand_city_list(config['locations'])
        keywords = config['keywords']
        grad_year = config['grad_year']
        recruit_type = config['recruit_type']
        
        config_results = []
        
        # éå†å…³é”®è¯å’ŒåŸå¸‚
        for keyword in keywords:
            for city in cities:
                # ä¼˜å…ˆä½¿ç”¨å®ä¹ åƒ§ï¼ˆé€‚åˆæ ¡æ‹›/å®ä¹ ï¼‰
                if recruit_type == 'æ ¡æ‹›' or 'æ ¡æ‹›' in recruit_type or grad_year:
                    shixiseng_results = self.search_shixiseng(keyword, city, grad_year, recruit_type)
                    config_results.extend(shixiseng_results)
                    self.random_sleep(3, 6)  # æ¯æ¬¡æœç´¢åä¼‘çœ 
                
                # å¦‚æœæ˜¯ç¤¾æ‹›ï¼Œä½¿ç”¨51job
                if recruit_type == 'ç¤¾æ‹›' or 'ç¤¾æ‹›' in recruit_type:
                    job51_results = self.search_51job(keyword, city, grad_year, recruit_type)
                    config_results.extend(job51_results)
                    self.random_sleep(3, 6)
        
        print(f"  âœ“ æœ¬é…ç½®å…±æŠ“å– {len(config_results)} ä¸ªèŒä½")
        return config_results
    
    def filter_results(self, df, config):
        """æ ¹æ®é…ç½®çš„å¤‡æ³¨ä¿¡æ¯è¿‡æ»¤ç»“æœ"""
        if df.empty:
            return df
        
        notes = config.get('notes', '') or ''
        company_type_req = config.get('company_type', '') or ''
        
        # å¤®å›½ä¼è¿‡æ»¤
        if company_type_req and ('å¤®å›½ä¼' in company_type_req or 'å›½å¤®ä¼' in company_type_req):
            try:
                mask = df['å…¬å¸åç§°'].str.contains('|'.join(STATE_OWNED_KEYWORDS), case=False, na=False)
                df = df[mask]
                print(f"  âœ“ å¤®å›½ä¼è¿‡æ»¤åå‰©ä½™ {len(df)} ä¸ªèŒä½")
            except:
                pass
        
        # å¤§å‚è¿‡æ»¤
        if notes and ('å¤§å‚' in notes or 'å¤§å…¬å¸' in notes):
            try:
                mask = df['å…¬å¸åç§°'].isin(BIG_COMPANIES) | df['å…¬å¸åç§°'].str.contains('|'.join(BIG_COMPANIES), case=False, na=False)
                df = df[mask]
                print(f"  âœ“ å¤§å‚è¿‡æ»¤åå‰©ä½™ {len(df)} ä¸ªèŒä½")
            except:
                pass
        
        # å››å¤§è¿‡æ»¤
        if notes and 'å››å¤§' in notes:
            try:
                four_big = ['æ™®åæ°¸é“', 'å¾·å‹¤', 'å®‰æ°¸', 'æ¯•é©¬å¨', 'PwC', 'Deloitte', 'EY', 'KPMG']
                mask = df['å…¬å¸åç§°'].str.contains('|'.join(four_big), case=False, na=False)
                df = df[mask]
                print(f"  âœ“ å››å¤§è¿‡æ»¤åå‰©ä½™ {len(df)} ä¸ªèŒä½")
            except:
                pass
        
        return df
    
    def close_browser(self):
        """å…³é—­æµè§ˆå™¨"""
        if self.browser:
            self.browser.close()
        if self.playwright:
            self.playwright.stop()
        print("\nâœ“ æµè§ˆå™¨å·²å…³é—­")
    
    def save_to_excel(self, df, filename=None):
        """ä¿å­˜ç»“æœåˆ°Excel"""
        if filename is None:
            filename = f"job_hunting_results_{self.today}.xlsx"
        
        # ç¡®ä¿æ‰€æœ‰å­—æ®µéƒ½å­˜åœ¨
        for field in OUTPUT_FIELDS:
            if field not in df.columns:
                df[field] = ''
        
        # æŒ‰æŒ‡å®šé¡ºåºæ’åˆ—åˆ—
        df = df[OUTPUT_FIELDS]
        
        # ä¿å­˜åˆ°Excel
        df.to_excel(filename, index=False, engine='openpyxl')
        
        # ç¾åŒ–Excelæ ¼å¼
        try:
            wb = load_workbook(filename)
            ws = wb.active
            
            # è®¾ç½®è¡¨å¤´æ ·å¼
            header_fill = PatternFill(start_color="366092", end_color="366092", fill_type="solid")
            header_font = Font(bold=True, color="FFFFFF", size=11)
            
            for cell in ws[1]:
                cell.fill = header_fill
                cell.font = header_font
                cell.alignment = Alignment(horizontal="center", vertical="center")
            
            # è®¾ç½®åˆ—å®½
            column_widths = {
                'A': 25,  # å…¬å¸åç§°
                'B': 15,  # å…¬å¸ç±»å‹
                'C': 15,  # å·¥ä½œåœ°ç‚¹
                'D': 12,  # æ‹›è˜ç±»å‹
                'E': 12,  # æ‹›è˜å¯¹è±¡
                'F': 30,  # å²—ä½
                'G': 15,  # æ›´æ–°æ—¶é—´
                'H': 15,  # æŠ•é€’æˆªæ­¢
                'I': 50,  # ç›¸å…³é“¾æ¥
            }
            
            for col, width in column_widths.items():
                ws.column_dimensions[col].width = width
            
            # è®¾ç½®è¡Œé«˜
            ws.row_dimensions[1].height = 25
            
            wb.save(filename)
        except Exception as e:
            print(f"âš  ç¾åŒ–Excelæ—¶å‡ºé”™: {str(e)}")
        
        print(f"\nâœ“ æ•°æ®å·²ä¿å­˜è‡³: {filename}")
        print(f"  å…± {len(df)} æ¡è®°å½•")
    
    def generate_demo_data(self):
        """ç”Ÿæˆç¤ºä¾‹æ•°æ®ç”¨äºæ¼”ç¤ºï¼ˆå½“æ— æ³•æŠ“å–çœŸå®æ•°æ®æ—¶ï¼‰"""
        demo_jobs = [
            {
                'å…¬å¸åç§°': 'è…¾è®¯ç§‘æŠ€ï¼ˆæ·±åœ³ï¼‰æœ‰é™å…¬å¸',
                'å…¬å¸ç±»å‹': 'äº’è”ç½‘',
                'å·¥ä½œåœ°ç‚¹': 'æ·±åœ³',
                'æ‹›è˜ç±»å‹': 'æ ¡æ‹›',
                'æ‹›è˜å¯¹è±¡': '2026å±Š',
                'å²—ä½(å¤§éƒ½ä¸é™ä¸“ä¸š)': 'æ•°æ®åˆ†æå¸ˆ',
                'æ›´æ–°æ—¶é—´': '2025-12-08',
                'æŠ•é€’æˆªæ­¢': '2025-12-31',
                'ç›¸å…³é“¾æ¥': 'https://www.shixiseng.com/intern/demo1'
            },
            {
                'å…¬å¸åç§°': 'é˜¿é‡Œå·´å·´ï¼ˆä¸­å›½ï¼‰ç½‘ç»œæŠ€æœ¯æœ‰é™å…¬å¸',
                'å…¬å¸ç±»å‹': 'äº’è”ç½‘',
                'å·¥ä½œåœ°ç‚¹': 'æ­å·',
                'æ‹›è˜ç±»å‹': 'æ ¡æ‹›',
                'æ‹›è˜å¯¹è±¡': '2026å±Š',
                'å²—ä½(å¤§éƒ½ä¸é™ä¸“ä¸š)': 'äº§å“ç»ç†',
                'æ›´æ–°æ—¶é—´': '2025-12-08',
                'æŠ•é€’æˆªæ­¢': '2025-12-30',
                'ç›¸å…³é“¾æ¥': 'https://www.shixiseng.com/intern/demo2'
            },
            {
                'å…¬å¸åç§°': 'å­—èŠ‚è·³åŠ¨ç§‘æŠ€æœ‰é™å…¬å¸',
                'å…¬å¸ç±»å‹': 'äº’è”ç½‘',
                'å·¥ä½œåœ°ç‚¹': 'åŒ—äº¬',
                'æ‹›è˜ç±»å‹': 'æ ¡æ‹›',
                'æ‹›è˜å¯¹è±¡': '2026å±Š',
                'å²—ä½(å¤§éƒ½ä¸é™ä¸“ä¸š)': 'å†…å®¹è¿è¥',
                'æ›´æ–°æ—¶é—´': '2025-12-07',
                'æŠ•é€’æˆªæ­¢': '2025-12-29',
                'ç›¸å…³é“¾æ¥': 'https://www.shixiseng.com/intern/demo3'
            },
            {
                'å…¬å¸åç§°': 'åä¸ºæŠ€æœ¯æœ‰é™å…¬å¸',
                'å…¬å¸ç±»å‹': 'ç§‘æŠ€',
                'å·¥ä½œåœ°ç‚¹': 'æ·±åœ³',
                'æ‹›è˜ç±»å‹': 'æ ¡æ‹›',
                'æ‹›è˜å¯¹è±¡': '2026å±Š',
                'å²—ä½(å¤§éƒ½ä¸é™ä¸“ä¸š)': 'è½¯ä»¶å·¥ç¨‹å¸ˆ',
                'æ›´æ–°æ—¶é—´': '2025-12-08',
                'æŠ•é€’æˆªæ­¢': '2025-12-31',
                'ç›¸å…³é“¾æ¥': 'https://www.shixiseng.com/intern/demo4'
            },
            {
                'å…¬å¸åç§°': 'ç¾å›¢',
                'å…¬å¸ç±»å‹': 'äº’è”ç½‘',
                'å·¥ä½œåœ°ç‚¹': 'åŒ—äº¬',
                'æ‹›è˜ç±»å‹': 'æ ¡æ‹›',
                'æ‹›è˜å¯¹è±¡': '2026å±Š',
                'å²—ä½(å¤§éƒ½ä¸é™ä¸“ä¸š)': 'å•†ä¸šåˆ†æ',
                'æ›´æ–°æ—¶é—´': '2025-12-08',
                'æŠ•é€’æˆªæ­¢': '2025-12-28',
                'ç›¸å…³é“¾æ¥': 'https://www.shixiseng.com/intern/demo5'
            },
        ]
        return demo_jobs
    
    def run(self):
        """è¿è¡Œä¸»ç¨‹åº"""
        try:
            self.start_browser()
            
            all_results = []
            total_configs = len(SEARCH_CONFIGS)
            
            for idx, config in enumerate(SEARCH_CONFIGS, 1):
                print(f"\n[{idx}/{total_configs}] å¤„ç†é…ç½® {idx}...")
                try:
                    results = self.search_jobs(config)
                    if results:
                        df = pd.DataFrame(results)
                        # åº”ç”¨è¿‡æ»¤
                        df = self.filter_results(df, config)
                        if not df.empty:
                            all_results.append(df)
                except Exception as e:
                    print(f"  âœ— å¤„ç†é…ç½®æ—¶å‡ºé”™: {str(e)[:100]}")
                    continue
            
            # åˆå¹¶æ‰€æœ‰ç»“æœ
            if all_results:
                final_df = pd.concat(all_results, ignore_index=True)
                # æœ€ç»ˆå»é‡ï¼ˆåŸºäºURLï¼‰
                final_df = final_df.drop_duplicates(subset=['ç›¸å…³é“¾æ¥'], keep='first')
                
                # ä¿å­˜ç»“æœ
                self.save_to_excel(final_df)
                
                # æ‰“å°æŠ“å–åˆ°çš„å²—ä½ä¿¡æ¯æ‘˜è¦
                print("\n" + "="*60)
                print("ğŸ“Š æŠ“å–ç»“æœæ‘˜è¦")
                print("="*60)
                print(f"âœ… å…±æŠ“å–åˆ° {len(final_df)} ä¸ªå²—ä½")
                print("\nğŸ“‹ å²—ä½åˆ—è¡¨é¢„è§ˆï¼ˆå‰10ä¸ªï¼‰ï¼š")
                print("-"*60)
                for idx, row in final_df.head(10).iterrows():
                    print(f"\nã€å²—ä½ {idx+1}ã€‘")
                    print(f"  å…¬å¸åç§°: {row['å…¬å¸åç§°']}")
                    print(f"  å²—ä½åç§°: {row['å²—ä½(å¤§éƒ½ä¸é™ä¸“ä¸š)']}")
                    print(f"  å·¥ä½œåœ°ç‚¹: {row['å·¥ä½œåœ°ç‚¹']}")
                    print(f"  æ‹›è˜ç±»å‹: {row['æ‹›è˜ç±»å‹']} | æ‹›è˜å¯¹è±¡: {row['æ‹›è˜å¯¹è±¡']}")
                    print(f"  æ›´æ–°æ—¶é—´: {row['æ›´æ–°æ—¶é—´']}")
                    print(f"  é“¾æ¥: {row['ç›¸å…³é“¾æ¥'][:60]}...")
            else:
                print("\nâš  æœªæŠ“å–åˆ°ä»»ä½•æ•°æ®")
                # å¦‚æœç½‘ç»œé—®é¢˜ï¼Œç”Ÿæˆç¤ºä¾‹æ•°æ®ç”¨äºæ¼”ç¤º
                print("ğŸ’¡ ç”Ÿæˆç¤ºä¾‹æ•°æ®ç”¨äºæ¼”ç¤ºç¨‹åºåŠŸèƒ½...")
                demo_data = self.generate_demo_data()
                if demo_data:
                    demo_df = pd.DataFrame(demo_data)
                    self.save_to_excel(demo_df, filename=f"job_hunting_results_demo_{self.today}.xlsx")
                    print("\nğŸ“‹ ç¤ºä¾‹å²—ä½æ•°æ®ï¼š")
                    print("-"*60)
                    for idx, item in enumerate(demo_data, 1):
                        print(f"\nã€ç¤ºä¾‹å²—ä½ {idx}ã€‘")
                        print(f"  å…¬å¸åç§°: {item['å…¬å¸åç§°']}")
                        print(f"  å²—ä½åç§°: {item['å²—ä½(å¤§éƒ½ä¸é™ä¸“ä¸š)']}")
                        print(f"  å·¥ä½œåœ°ç‚¹: {item['å·¥ä½œåœ°ç‚¹']}")
                        print(f"  æ‹›è˜ç±»å‹: {item['æ‹›è˜ç±»å‹']} | æ‹›è˜å¯¹è±¡: {item['æ‹›è˜å¯¹è±¡']}")
            
        except Exception as e:
            print(f"\nâœ— è¿è¡Œå‡ºé”™: {str(e)}")
        finally:
            self.close_browser()


def main():
    """ä¸»å‡½æ•°"""
    scraper = JobScraper(headless=True)  # è®¾ç½®ä¸ºTrueå¯åå°è¿è¡Œï¼ˆæµ‹è¯•æ¨¡å¼ï¼‰
    scraper.run()


if __name__ == '__main__':
    main()

