#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ç‰¹å®šéœ€æ±‚å²—ä½æŠ“å–è„šæœ¬
ä»å¤šä¸ªæ‹›è˜ç½‘ç«™æŠ“å–ç¬¦åˆç‰¹å®šéœ€æ±‚çš„å²—ä½ä¿¡æ¯
"""

import time
import random
import re
from datetime import datetime
import pandas as pd
from playwright.sync_api import sync_playwright, TimeoutError as PlaywrightTimeoutError
import urllib.parse
from specific_requirements_config import (
    SPECIFIC_REQUIREMENTS, CITY_MAPPING, BIG_COMPANIES, 
    STATE_OWNED_KEYWORDS, FOUR_BIG, EIGHT_BIG
)
from openpyxl import load_workbook
from openpyxl.styles import Font, PatternFill, Alignment

# è¾“å‡ºå­—æ®µï¼ˆæ ¹æ®ç”¨æˆ·è¦æ±‚ï¼‰
OUTPUT_FIELDS = [
    'å…¬å¸åç§°', 'å…¬å¸ç±»å‹', 'å·¥ä½œåœ°ç‚¹', 'æ‹›è˜ç±»å‹', 'æ‹›è˜å¯¹è±¡', 
    'å²—ä½', 'è–ªèµ„', 'æ›´æ–°æ—¶é—´', 'å‘å¸ƒæ—¶é—´', 'æŠ•é€’æˆªæ­¢', 
    'å²—ä½è¯¦æƒ…é“¾æ¥', 'æŠ•é€’é“¾æ¥'
]


class SpecificRequirementsScraper:
    """ç‰¹å®šéœ€æ±‚å²—ä½æŠ“å–å™¨"""
    
    def __init__(self, headless=True):
        """åˆå§‹åŒ–çˆ¬è™«"""
        self.results = []
        self.seen_urls = set()  # ç”¨äºå»é‡
        self.today = datetime.now().strftime('%Y-%m-%d')
        self.playwright = None
        self.browser = None
        self.page = None
        self.headless = headless
        
    def start_browser(self):
        """å¯åŠ¨æµè§ˆå™¨"""
        print("\n" + "="*60)
        print("ç‰¹å®šéœ€æ±‚å²—ä½æŠ“å–è„šæœ¬")
        print("="*60)
        print(f"\nå¼€å§‹æ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
        print("æ­£åœ¨å¯åŠ¨æµè§ˆå™¨...")
        
        self.playwright = sync_playwright().start()
        self.browser = self.playwright.chromium.launch(
            headless=self.headless,
            args=['--disable-blink-features=AutomationControlled']
        )
        
        # åˆ›å»ºä¸Šä¸‹æ–‡ï¼Œè®¾ç½®éšæœºUser-Agent
        context = self.browser.new_context(
            user_agent='Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
            viewport={'width': 1920, 'height': 1080}
        )
        
        self.page = context.new_page()
        print("âœ“ æµè§ˆå™¨å¯åŠ¨æˆåŠŸï¼")
        
    def random_sleep(self, min_time=0.5, max_time=1.5):
        """éšæœºä¼‘çœ ï¼Œæ¨¡æ‹Ÿäººç±»è¡Œä¸ºï¼ˆä¼˜åŒ–é€Ÿåº¦ï¼‰"""
        sleep_time = random.uniform(min_time, max_time)
        time.sleep(sleep_time)
        
    def expand_city_list(self, locations):
        """å±•å¼€åŸå¸‚åˆ—è¡¨ï¼Œå°†æ¨¡ç³Šåœ°åŒºè½¬æ¢ä¸ºå…·ä½“åŸå¸‚"""
        expanded = []
        for loc in locations:
            if loc in CITY_MAPPING:
                expanded.extend(CITY_MAPPING[loc])
            else:
                expanded.append(loc)
        # å»é‡å¹¶ä¿æŒé¡ºåº
        seen = set()
        result = []
        for city in expanded:
            if city not in seen:
                seen.add(city)
                result.append(city)
        return result
    
    def search_boss_zhipin(self, keyword, city, config):
        """åœ¨BOSSç›´è˜æœç´¢å²—ä½"""
        results = []
        
        try:
            # BOSSç›´è˜çš„URLæ ¼å¼
            keyword_encoded = urllib.parse.quote(keyword)
            city_encoded = urllib.parse.quote(city)
            
            # BOSSç›´è˜æœç´¢URL
            url = f"https://www.zhipin.com/web/geek/job?query={keyword_encoded}&city={city_encoded}"
            
            print(f"    æ­£åœ¨æœç´¢BOSSç›´è˜: {keyword} | {city}")
            
            max_retries = 2
            for retry in range(max_retries):
                try:
                    self.page.goto(url, wait_until="domcontentloaded", timeout=30000)
                    self.random_sleep(3, 5)
                    break
                except Exception as e:
                    if retry < max_retries - 1:
                        wait_time = (retry + 1) * 3
                        print(f"    âš  ç¬¬{retry + 1}æ¬¡å°è¯•å¤±è´¥ï¼Œ{wait_time}ç§’åé‡è¯•...")
                        time.sleep(wait_time)
                    else:
                        print(f"    âš  BOSSç›´è˜è®¿é—®å—é™ï¼Œè·³è¿‡...")
                        return results
            
            # ç­‰å¾…é¡µé¢åŠ è½½
            try:
                self.page.wait_for_load_state("domcontentloaded", timeout=15000)
                self.random_sleep(2, 4)
                
                # BOSSç›´è˜çš„èŒä½åˆ—è¡¨é€‰æ‹©å™¨
                job_elements = self.page.query_selector_all('.job-card-wrapper, .job-card, [class*="job-item"]')
                
                if not job_elements:
                    print(f"    âš  æœªæ‰¾åˆ°èŒä½åˆ—è¡¨")
                    return results
                
                print(f"    âœ“ æ‰¾åˆ° {len(job_elements)} ä¸ªèŒä½")
                
                for idx, job_elem in enumerate(job_elements[:10], 1):  # é™åˆ¶æ¯é¡µ10ä¸ª
                    try:
                        # æå–èŒä½ä¿¡æ¯
                        job_title_elem = job_elem.query_selector('.job-name, .job-title, h3, a[class*="job"]')
                        job_title = job_title_elem.inner_text().strip() if job_title_elem else None
                        
                        if not job_title:
                            continue
                        
                        # æå–å…¬å¸åç§°
                        company_elem = job_elem.query_selector('.company-name, .company, [class*="company"]')
                        company_name = company_elem.inner_text().strip() if company_elem else 'æœªçŸ¥'
                        
                        # æå–å·¥ä½œåœ°ç‚¹
                        location_elem = job_elem.query_selector('.job-area, .location, [class*="area"]')
                        work_location = location_elem.inner_text().strip() if location_elem else city
                        
                        # æå–è–ªèµ„
                        salary_elem = job_elem.query_selector('.salary, .job-salary, [class*="salary"]')
                        salary = salary_elem.inner_text().strip() if salary_elem else 'é¢è®®'
                        
                        # æå–æ›´æ–°æ—¶é—´
                        time_elem = job_elem.query_selector('.job-time, .time, [class*="time"]')
                        update_time = time_elem.inner_text().strip() if time_elem else 'æœªçŸ¥'
                        
                        # è·å–é“¾æ¥
                        link_elem = job_elem.query_selector('a')
                        job_link = None
                        if link_elem:
                            href = link_elem.get_attribute('href')
                            if href:
                                if href.startswith('http'):
                                    job_link = href
                                else:
                                    job_link = f"https://www.zhipin.com{href}"
                        
                        if not job_link:
                            continue
                        
                        # å»é‡æ£€æŸ¥
                        if job_link in self.seen_urls:
                            continue
                        self.seen_urls.add(job_link)
                        
                        # åˆ¤æ–­å…¬å¸ç±»å‹
                        company_type = self._detect_company_type(company_name, config)
                        
                        # åˆ¤æ–­æ‹›è˜ç±»å‹å’Œå¯¹è±¡
                        recruit_type = 'ç¤¾æ‹›'
                        if 'æ ¡æ‹›' in job_title or 'åº”å±Š' in job_title or 'ç®¡åŸ¹' in job_title:
                            recruit_type = 'æ ¡æ‹›'
                        
                        recruit_target = 'ä¸é™'
                        if config.get('grad_years'):
                            years = config['grad_years']
                            if isinstance(years, list):
                                recruit_target = f"{'/'.join(map(str, years))}å±Š"
                            else:
                                recruit_target = f"{years}å±Š"
                        
                        result = {
                            'å…¬å¸åç§°': company_name,
                            'å…¬å¸ç±»å‹': company_type,
                            'å·¥ä½œåœ°ç‚¹': work_location,
                            'æ‹›è˜ç±»å‹': recruit_type,
                            'æ‹›è˜å¯¹è±¡': recruit_target,
                            'å²—ä½': job_title,
                            'è–ªèµ„': salary,
                            'æ›´æ–°æ—¶é—´': update_time,
                            'å‘å¸ƒæ—¶é—´': 'æœªçŸ¥',
                            'æŠ•é€’æˆªæ­¢': 'è¯¦è§é“¾æ¥',
                            'å²—ä½è¯¦æƒ…é“¾æ¥': job_link,
                            'æŠ•é€’é“¾æ¥': job_link
                        }
                        
                        results.append(result)
                        print(f"      âœ“ [{idx}] {company_name} - {job_title[:30]}...")
                        
                    except Exception as e:
                        print(f"    âš  æå–ç¬¬{idx}ä¸ªèŒä½ä¿¡æ¯æ—¶å‡ºé”™: {str(e)[:50]}")
                        continue
                        
            except Exception as e:
                print(f"    âœ— è§£æBOSSç›´è˜é¡µé¢æ—¶å‡ºé”™: {str(e)[:100]}")
                
        except Exception as e:
            print(f"    âœ— æœç´¢BOSSç›´è˜æ—¶å‡ºé”™: {str(e)[:100]}")
        
        return results
    
    def search_guopin(self, keyword, city, config):
        """åœ¨å›½è˜ç½‘æœç´¢å²—ä½"""
        results = []
        
        try:
            keyword_encoded = urllib.parse.quote(keyword)
            city_encoded = urllib.parse.quote(city)
            
            # å›½è˜ç½‘æœç´¢URL - å›½è˜ç½‘ä¸»è¦é’ˆå¯¹å¤®å›½ä¼
            # å°è¯•å¤šç§URLæ ¼å¼
            url = f"https://www.iguopin.com/jobs?keyword={keyword_encoded}&city={city_encoded}"
            
            print(f"    æ­£åœ¨æœç´¢å›½è˜ç½‘: {keyword} | {city}")
            
            max_retries = 2
            for retry in range(max_retries):
                try:
                    self.page.goto(url, wait_until="domcontentloaded", timeout=30000)
                    self.random_sleep(3, 5)
                    break
                except Exception as e:
                    if retry < max_retries - 1:
                        wait_time = (retry + 1) * 3
                        print(f"    âš  ç¬¬{retry + 1}æ¬¡å°è¯•å¤±è´¥ï¼Œ{wait_time}ç§’åé‡è¯•...")
                        time.sleep(wait_time)
                    else:
                        print(f"    âš  å›½è˜ç½‘è®¿é—®å—é™ï¼Œè·³è¿‡...")
                        return results
            
            try:
                self.page.wait_for_load_state("domcontentloaded", timeout=15000)
                self.random_sleep(2, 4)
                
                # å›½è˜ç½‘çš„èŒä½åˆ—è¡¨é€‰æ‹©å™¨
                job_elements = self.page.query_selector_all('.job-item, .job-card, [class*="job-list-item"], [class*="position-item"]')
                
                if not job_elements:
                    # å°è¯•å…¶ä»–é€‰æ‹©å™¨
                    job_elements = self.page.query_selector_all('li[class*="job"], div[class*="job"], a[href*="/job/"]')
                
                if not job_elements:
                    print(f"    âš  æœªæ‰¾åˆ°èŒä½åˆ—è¡¨")
                    return results
                
                print(f"    âœ“ æ‰¾åˆ° {len(job_elements)} ä¸ªèŒä½")
                
                for idx, job_elem in enumerate(job_elements[:10], 1):
                    try:
                        # æå–èŒä½ä¿¡æ¯
                        job_title_elem = job_elem.query_selector('.job-title, .title, h3, h4, a[class*="title"]')
                        job_title = job_title_elem.inner_text().strip() if job_title_elem else None
                        
                        if not job_title:
                            # å°è¯•ä»é“¾æ¥æ–‡æœ¬è·å–
                            link_elem = job_elem.query_selector('a')
                            if link_elem:
                                job_title = link_elem.inner_text().strip()
                        
                        if not job_title:
                            continue
                        
                        company_elem = job_elem.query_selector('.company-name, .company, [class*="company"]')
                        company_name = company_elem.inner_text().strip() if company_elem else 'æœªçŸ¥'
                        
                        location_elem = job_elem.query_selector('.job-location, .location, [class*="location"], [class*="city"]')
                        work_location = location_elem.inner_text().strip() if location_elem else city
                        
                        salary_elem = job_elem.query_selector('.salary, .job-salary, [class*="salary"]')
                        salary = salary_elem.inner_text().strip() if salary_elem else 'é¢è®®'
                        
                        time_elem = job_elem.query_selector('.time, .update-time, [class*="time"]')
                        update_time = time_elem.inner_text().strip() if time_elem else 'æœªçŸ¥'
                        
                        link_elem = job_elem.query_selector('a')
                        job_link = None
                        if link_elem:
                            href = link_elem.get_attribute('href')
                            if href:
                                job_link = href if href.startswith('http') else f"https://www.iguopin.com{href}"
                        
                        if not job_link:
                            continue
                        
                        if job_link in self.seen_urls:
                            continue
                        self.seen_urls.add(job_link)
                        
                        company_type = self._detect_company_type(company_name, config)
                        # å›½è˜ç½‘ä¸»è¦æ˜¯å¤®å›½ä¼ï¼Œå¦‚æœæ²¡æœ‰æ£€æµ‹åˆ°ï¼Œæ ‡è®°ä¸ºå¤®å›½ä¼
                        if company_type == 'æœªçŸ¥':
                            company_type = 'å¤®å›½ä¼'
                        
                        recruit_type = 'æ ¡æ‹›'
                        if 'ç¤¾æ‹›' in job_title or 'ç¤¾ä¼šæ‹›è˜' in job_title:
                            recruit_type = 'ç¤¾æ‹›'
                        
                        recruit_target = 'ä¸é™'
                        if config.get('grad_years'):
                            years = config['grad_years']
                            if isinstance(years, list):
                                recruit_target = f"{'/'.join(map(str, years))}å±Š"
                            else:
                                recruit_target = f"{years}å±Š"
                        
                        result = {
                            'å…¬å¸åç§°': company_name,
                            'å…¬å¸ç±»å‹': company_type,
                            'å·¥ä½œåœ°ç‚¹': work_location,
                            'æ‹›è˜ç±»å‹': recruit_type,
                            'æ‹›è˜å¯¹è±¡': recruit_target,
                            'å²—ä½': job_title,
                            'è–ªèµ„': salary,
                            'æ›´æ–°æ—¶é—´': update_time,
                            'å‘å¸ƒæ—¶é—´': 'æœªçŸ¥',
                            'æŠ•é€’æˆªæ­¢': 'è¯¦è§é“¾æ¥',
                            'å²—ä½è¯¦æƒ…é“¾æ¥': job_link,
                            'æŠ•é€’é“¾æ¥': job_link
                        }
                        
                        results.append(result)
                        print(f"      âœ“ [{idx}] {company_name} - {job_title[:30]}...")
                        
                    except Exception as e:
                        continue
                        
            except Exception as e:
                print(f"    âœ— è§£æå›½è˜ç½‘é¡µé¢æ—¶å‡ºé”™: {str(e)[:100]}")
                
        except Exception as e:
            print(f"    âœ— æœç´¢å›½è˜ç½‘æ—¶å‡ºé”™: {str(e)[:100]}")
        
        return results
    
    def search_51job(self, keyword, city, config):
        """åœ¨å‰ç¨‹æ— å¿§æœç´¢å²—ä½"""
        results = []
        
        try:
            keyword_encoded = urllib.parse.quote(keyword)
            
            # å‰ç¨‹æ— å¿§éœ€è¦åŸå¸‚ä»£ç ï¼Œè¿™é‡Œç®€åŒ–å¤„ç†ï¼Œä½¿ç”¨åŸå¸‚åç§°
            # å‰ç¨‹æ— å¿§æœç´¢URL - ä½¿ç”¨æ›´é€šç”¨çš„æ ¼å¼
            url = f"https://search.51job.com/list/000000,000000,0000,00,9,99,{keyword_encoded},2,1.html"
            
            # å¦‚æœåŸå¸‚æ˜¯å…·ä½“åŸå¸‚ï¼Œå°è¯•åœ¨URLä¸­æ·»åŠ åŸå¸‚å‚æ•°
            city_mapping = {
                'åŒ—äº¬': '010000', 'ä¸Šæµ·': '020000', 'å¹¿å·': '030200', 'æ·±åœ³': '040000',
                'æ­å·': '080200', 'å—äº¬': '070200', 'è‹å·': '070300', 'æˆéƒ½': '090200',
                'æ­¦æ±‰': '180200', 'è¥¿å®‰': '200200', 'é‡åº†': '060000', 'å¤©æ´¥': '050000'
            }
            
            if city in city_mapping:
                city_code = city_mapping[city]
                url = f"https://search.51job.com/list/{city_code},000000,0000,00,9,99,{keyword_encoded},2,1.html"
            
            print(f"    æ­£åœ¨æœç´¢å‰ç¨‹æ— å¿§: {keyword} | {city}")
            
            max_retries = 1  # å‡å°‘é‡è¯•æ¬¡æ•°
            for retry in range(max_retries):
                try:
                    self.page.goto(url, wait_until="networkidle", timeout=15000)
                    self.random_sleep(0.5, 1)  # è¿›ä¸€æ­¥å‡å°‘ç­‰å¾…æ—¶é—´
                    break
                except Exception as e:
                    if retry < max_retries - 1:
                        wait_time = 1
                        print(f"    âš  ç¬¬{retry + 1}æ¬¡å°è¯•å¤±è´¥ï¼Œ{wait_time}ç§’åé‡è¯•...")
                        time.sleep(wait_time)
                    else:
                        print(f"    âš  å‰ç¨‹æ— å¿§è®¿é—®å—é™ï¼Œè·³è¿‡...")
                        return results
            
            try:
                # ä¸ç­‰å¾…å®Œæ•´åŠ è½½ï¼Œç›´æ¥å¼€å§‹è§£æ
                self.random_sleep(0.3, 0.8)  # æœ€å°ç­‰å¾…æ—¶é—´
                
                # å‰ç¨‹æ— å¿§çš„èŒä½åˆ—è¡¨é€‰æ‹©å™¨ - ä½¿ç”¨å¤šç§ç­–ç•¥
                job_elements = None
                selectors = [
                    '.el',  # ç»å…¸é€‰æ‹©å™¨
                    '[class*="job-item"]',
                    '[class*="position-item"]',
                    '.joblist_item',
                    'div[class*="job"]',
                    'li[class*="job"]',
                    'a[href*="/job/"]',
                ]
                
                for selector in selectors:
                    try:
                        elements = self.page.query_selector_all(selector)
                        if elements and len(elements) > 0:
                            job_elements = elements
                            print(f"    âœ“ ä½¿ç”¨é€‰æ‹©å™¨: {selector}")
                            break
                    except:
                        continue
                
                if not job_elements:
                    print(f"    âš  æœªæ‰¾åˆ°èŒä½åˆ—è¡¨ï¼Œå°è¯•æˆªå›¾æŸ¥çœ‹é¡µé¢...")
                    # å°è¯•ä¿å­˜é¡µé¢æˆªå›¾ç”¨äºè°ƒè¯•
                    try:
                        self.page.screenshot(path=f"51job_debug_{city}_{keyword}.png")
                    except:
                        pass
                    return results
                
                print(f"    âœ“ æ‰¾åˆ° {len(job_elements)} ä¸ªèŒä½")
                
                for idx, job_elem in enumerate(job_elements[:25], 1):  # å¢åŠ æ•°é‡ä»¥è·å–æ›´å¤šå²—ä½
                    try:
                        # æå–èŒä½ä¿¡æ¯ - ä½¿ç”¨å¤šç§é€‰æ‹©å™¨ç­–ç•¥
                        job_title = None
                        job_title_selectors = ['.t1', '.jobname', 'a[href*="/job/"]', 'span[title]', 'a span', '.job_title']
                        for sel in job_title_selectors:
                            try:
                                elem = job_elem.query_selector(sel)
                                if elem:
                                    job_title = elem.inner_text().strip()
                                    if job_title and len(job_title) > 2:
                                        break
                            except:
                                continue
                        
                        # å¦‚æœè¿˜æ²¡æ‰¾åˆ°ï¼Œå°è¯•ä»é“¾æ¥æ–‡æœ¬è·å–
                        if not job_title:
                            link_elem = job_elem.query_selector('a')
                            if link_elem:
                                job_title = link_elem.inner_text().strip()
                        
                        if not job_title or len(job_title) < 2:
                            continue
                        
                        # æå–å…¬å¸åç§°
                        company_name = 'æœªçŸ¥'
                        company_selectors = ['.t2', '.company', '.cname', '[class*="company"]', 'span[class*="company"]']
                        for sel in company_selectors:
                            try:
                                elem = job_elem.query_selector(sel)
                                if elem:
                                    company_name = elem.inner_text().strip()
                                    if company_name and len(company_name) > 1:
                                        break
                            except:
                                continue
                        
                        # æå–å·¥ä½œåœ°ç‚¹
                        work_location = city
                        location_selectors = ['.t3', '.location', '.area', '[class*="location"]', '[class*="city"]']
                        for sel in location_selectors:
                            try:
                                elem = job_elem.query_selector(sel)
                                if elem:
                                    work_location = elem.inner_text().strip()
                                    if work_location:
                                        break
                            except:
                                continue
                        
                        # æå–è–ªèµ„
                        salary = 'é¢è®®'
                        salary_selectors = ['.t4', '.salary', '[class*="salary"]', 'span[class*="salary"]']
                        for sel in salary_selectors:
                            try:
                                elem = job_elem.query_selector(sel)
                                if elem:
                                    salary = elem.inner_text().strip()
                                    if salary:
                                        break
                            except:
                                continue
                        
                        # æå–æ›´æ–°æ—¶é—´
                        update_time = 'æœªçŸ¥'
                        time_selectors = ['.t5', '.time', '.pubtime', '[class*="time"]']
                        for sel in time_selectors:
                            try:
                                elem = job_elem.query_selector(sel)
                                if elem:
                                    update_time = elem.inner_text().strip()
                                    if update_time:
                                        break
                            except:
                                continue
                        
                        # è·å–é“¾æ¥ - ç®€åŒ–é€»è¾‘ï¼Œå…ˆè·å–é“¾æ¥ï¼Œåç»­å¯ä»¥ä¼˜åŒ–
                        job_link = None
                        
                        # 51jobçš„å²—ä½é“¾æ¥é€šå¸¸åœ¨èŒä½æ ‡é¢˜çš„aæ ‡ç­¾ä¸­
                        title_link = job_elem.query_selector('a[href], .t1 a, .jobname a')
                        if not title_link:
                            title_link = job_elem.query_selector('a')
                        
                        if title_link:
                            href = title_link.get_attribute('href')
                            if href:
                                # å¤„ç†ç›¸å¯¹è·¯å¾„å’Œç»å¯¹è·¯å¾„
                                if href.startswith('http'):
                                    job_link = href
                                elif href.startswith('/'):
                                    job_link = f"https://jobs.51job.com{href}"
                                elif 'jobs.51job.com' in href or 'we.51job.com' in href:
                                    job_link = href if href.startswith('http') else f"https://{href}"
                        
                        # å¦‚æœé“¾æ¥æ˜¯å…¬å¸é¡µé¢ï¼ˆ/all/coï¼‰ï¼Œå°è¯•æ„é€ æœç´¢é“¾æ¥ä½œä¸ºå¤‡é€‰
                        if job_link and '/all/co' in job_link:
                            # ä½¿ç”¨æœç´¢é“¾æ¥ä½œä¸ºå²—ä½è¯¦æƒ…é“¾æ¥ï¼ˆç”¨æˆ·å¯ä»¥é€šè¿‡æœç´¢æ‰¾åˆ°å…·ä½“å²—ä½ï¼‰
                            search_keyword = urllib.parse.quote(f"{company_name} {job_title}")
                            job_link = f"https://search.51job.com/list/000000,000000,0000,00,9,99,{search_keyword},2,1.html"
                        
                        if not job_link:
                            continue
                        
                        if job_link in self.seen_urls:
                            continue
                        self.seen_urls.add(job_link)
                        
                        company_type = self._detect_company_type(company_name, config)
                        
                        recruit_type = 'ç¤¾æ‹›'
                        if 'æ ¡æ‹›' in job_title or 'åº”å±Š' in job_title:
                            recruit_type = 'æ ¡æ‹›'
                        
                        recruit_target = 'ä¸é™'
                        if config.get('grad_years'):
                            years = config['grad_years']
                            if isinstance(years, list):
                                recruit_target = f"{'/'.join(map(str, years))}å±Š"
                            else:
                                recruit_target = f"{years}å±Š"
                        
                        result = {
                            'å…¬å¸åç§°': company_name,
                            'å…¬å¸ç±»å‹': company_type,
                            'å·¥ä½œåœ°ç‚¹': work_location,
                            'æ‹›è˜ç±»å‹': recruit_type,
                            'æ‹›è˜å¯¹è±¡': recruit_target,
                            'å²—ä½': job_title,
                            'è–ªèµ„': salary,
                            'æ›´æ–°æ—¶é—´': update_time,
                            'å‘å¸ƒæ—¶é—´': 'æœªçŸ¥',
                            'æŠ•é€’æˆªæ­¢': 'è¯¦è§é“¾æ¥',
                            'å²—ä½è¯¦æƒ…é“¾æ¥': job_link,
                            'æŠ•é€’é“¾æ¥': job_link
                        }
                        
                        results.append(result)
                        print(f"      âœ“ [{idx}] {company_name} - {job_title[:30]}...")
                        
                    except Exception as e:
                        continue
                        
            except Exception as e:
                print(f"    âœ— è§£æå‰ç¨‹æ— å¿§é¡µé¢æ—¶å‡ºé”™: {str(e)[:100]}")
                
        except Exception as e:
            print(f"    âœ— æœç´¢å‰ç¨‹æ— å¿§æ—¶å‡ºé”™: {str(e)[:100]}")
        
        return results
    
    def search_liepin(self, keyword, city, config):
        """åœ¨çŒè˜æœç´¢å²—ä½"""
        results = []
        
        try:
            keyword_encoded = urllib.parse.quote(keyword)
            city_encoded = urllib.parse.quote(city)
            
            # çŒè˜æœç´¢URL
            url = f"https://www.liepin.com/zhaopin/?key={keyword_encoded}&dqs={city_encoded}"
            
            print(f"    æ­£åœ¨æœç´¢çŒè˜: {keyword} | {city}")
            
            max_retries = 2
            for retry in range(max_retries):
                try:
                    self.page.goto(url, wait_until="domcontentloaded", timeout=30000)
                    self.random_sleep(3, 5)
                    break
                except Exception as e:
                    if retry < max_retries - 1:
                        wait_time = (retry + 1) * 3
                        print(f"    âš  ç¬¬{retry + 1}æ¬¡å°è¯•å¤±è´¥ï¼Œ{wait_time}ç§’åé‡è¯•...")
                        time.sleep(wait_time)
                    else:
                        print(f"    âš  çŒè˜è®¿é—®å—é™ï¼Œè·³è¿‡...")
                        return results
            
            try:
                self.page.wait_for_load_state("domcontentloaded", timeout=15000)
                self.random_sleep(2, 4)
                
                # çŒè˜çš„èŒä½åˆ—è¡¨é€‰æ‹©å™¨
                job_elements = self.page.query_selector_all('.job-card, .job-item, [class*="job-list-item"]')
                
                if not job_elements:
                    print(f"    âš  æœªæ‰¾åˆ°èŒä½åˆ—è¡¨")
                    return results
                
                print(f"    âœ“ æ‰¾åˆ° {len(job_elements)} ä¸ªèŒä½")
                
                for idx, job_elem in enumerate(job_elements[:10], 1):
                    try:
                        job_title_elem = job_elem.query_selector('.job-title, h3, a[class*="title"]')
                        job_title = job_title_elem.inner_text().strip() if job_title_elem else None
                        
                        if not job_title:
                            continue
                        
                        company_elem = job_elem.query_selector('.company-name, .company')
                        company_name = company_elem.inner_text().strip() if company_elem else 'æœªçŸ¥'
                        
                        location_elem = job_elem.query_selector('.job-location, .location')
                        work_location = location_elem.inner_text().strip() if location_elem else city
                        
                        salary_elem = job_elem.query_selector('.job-salary, .salary')
                        salary = salary_elem.inner_text().strip() if salary_elem else 'é¢è®®'
                        
                        time_elem = job_elem.query_selector('.time, .update-time')
                        update_time = time_elem.inner_text().strip() if time_elem else 'æœªçŸ¥'
                        
                        link_elem = job_elem.query_selector('a')
                        job_link = None
                        if link_elem:
                            href = link_elem.get_attribute('href')
                            if href:
                                job_link = href if href.startswith('http') else f"https://www.liepin.com{href}"
                        
                        if not job_link:
                            continue
                        
                        if job_link in self.seen_urls:
                            continue
                        self.seen_urls.add(job_link)
                        
                        company_type = self._detect_company_type(company_name, config)
                        
                        recruit_type = 'ç¤¾æ‹›'
                        if 'æ ¡æ‹›' in job_title or 'åº”å±Š' in job_title:
                            recruit_type = 'æ ¡æ‹›'
                        
                        recruit_target = 'ä¸é™'
                        if config.get('grad_years'):
                            years = config['grad_years']
                            if isinstance(years, list):
                                recruit_target = f"{'/'.join(map(str, years))}å±Š"
                            else:
                                recruit_target = f"{years}å±Š"
                        
                        result = {
                            'å…¬å¸åç§°': company_name,
                            'å…¬å¸ç±»å‹': company_type,
                            'å·¥ä½œåœ°ç‚¹': work_location,
                            'æ‹›è˜ç±»å‹': recruit_type,
                            'æ‹›è˜å¯¹è±¡': recruit_target,
                            'å²—ä½': job_title,
                            'è–ªèµ„': salary,
                            'æ›´æ–°æ—¶é—´': update_time,
                            'å‘å¸ƒæ—¶é—´': 'æœªçŸ¥',
                            'æŠ•é€’æˆªæ­¢': 'è¯¦è§é“¾æ¥',
                            'å²—ä½è¯¦æƒ…é“¾æ¥': job_link,
                            'æŠ•é€’é“¾æ¥': job_link
                        }
                        
                        results.append(result)
                        print(f"      âœ“ [{idx}] {company_name} - {job_title[:30]}...")
                        
                    except Exception as e:
                        continue
                        
            except Exception as e:
                print(f"    âœ— è§£æçŒè˜é¡µé¢æ—¶å‡ºé”™: {str(e)[:100]}")
                
        except Exception as e:
            print(f"    âœ— æœç´¢çŒè˜æ—¶å‡ºé”™: {str(e)[:100]}")
        
        return results
    
    def _detect_company_type(self, company_name, config):
        """æ£€æµ‹å…¬å¸ç±»å‹"""
        if not company_name or company_name == 'æœªçŸ¥':
            return 'æœªçŸ¥'
        
        company_name_lower = company_name.lower()
        
        # æ’é™¤æ˜æ˜¾ä¸æ˜¯å¤®å›½ä¼çš„å…¬å¸ï¼ˆå¤–ä¼å…³é”®è¯ï¼‰
        foreign_keywords = ['æŠ•èµ„æœ‰é™å…¬å¸', 'ï¼ˆä¸­å›½ï¼‰', '(ä¸­å›½)', 'å¤–èµ„', 'å¤–ä¼', 'ä¸¹å°¼æ–¯å…‹', 'è”åˆåˆ©å', 'å®æ´']
        is_foreign = any(kw in company_name for kw in foreign_keywords)
        
        # æ£€æµ‹å››å¤§ï¼ˆä¼˜å…ˆçº§é«˜ï¼‰
        for keyword in FOUR_BIG:
            if keyword.lower() in company_name_lower:
                return 'å››å¤§'
        
        # æ£€æµ‹å…«å¤§
        for keyword in EIGHT_BIG:
            if keyword in company_name:
                return 'å…«å¤§'
        
        # æ£€æµ‹å¤§å‚
        for company in BIG_COMPANIES:
            if company in company_name:
                return 'å¤§å‚'
        
        # æ£€æµ‹å¤®å›½ä¼ï¼ˆæ’é™¤å¤–ä¼ï¼‰
        if not is_foreign:
            for keyword in STATE_OWNED_KEYWORDS:
                if keyword in company_name:
                    return 'å¤®å›½ä¼'
        
        # æ ¹æ®é…ç½®åˆ¤æ–­
        company_type_req = config.get('company_type', '')
        if company_type_req:
            if 'å¤®å›½ä¼' in company_type_req or 'å›½å¤®ä¼' in company_type_req:
                if not is_foreign:
                    return 'å¤®å›½ä¼'
            elif 'å¤§å‚' in company_type_req or 'å¤§å…¬å¸' in company_type_req:
                return 'å¤§å‚'
            elif 'å››å¤§' in company_type_req:
                return 'å››å¤§'
            elif 'å…«å¤§' in company_type_req:
                return 'å…«å¤§'
        
        return 'æœªçŸ¥'
    
    def filter_results(self, results, config):
        """æ ¹æ®é…ç½®è¿‡æ»¤ç»“æœ"""
        filtered = []
        
        for result in results:
            company_name = result.get('å…¬å¸åç§°', '')
            notes = config.get('notes', '')
            company_type_req = config.get('company_type', '')
            
            # å¤®å›½ä¼è¿‡æ»¤
            if company_type_req and ('å¤®å›½ä¼' in company_type_req or 'å›½å¤®ä¼' in company_type_req):
                if not any(keyword in company_name for keyword in STATE_OWNED_KEYWORDS):
                    continue
            
            # å¤§å‚è¿‡æ»¤
            if notes and ('å¤§å‚' in notes or 'å¤§å…¬å¸' in notes):
                if not any(company in company_name for company in BIG_COMPANIES):
                    continue
            
            # å››å¤§è¿‡æ»¤
            if notes and 'å››å¤§' in notes:
                if not any(keyword in company_name for keyword in FOUR_BIG):
                    continue
            
            filtered.append(result)
        
        return filtered
    
    def search_jobs_for_config(self, config, max_jobs=10):
        """ä¸ºå•ä¸ªé…ç½®æœç´¢å²—ä½"""
        print(f"\n{'='*60}")
        print(f"é…ç½®: {', '.join(config['keywords'][:3])}... | {', '.join(config['locations'][:2])}...")
        print(f"{'='*60}")
        
        # å±•å¼€åŸå¸‚åˆ—è¡¨
        cities = self.expand_city_list(config['locations'])
        keywords = config['keywords']
        
        config_results = []
        
        # éå†å…³é”®è¯å’ŒåŸå¸‚ - å¢åŠ æœç´¢èŒƒå›´ä»¥è·å–æ›´å¤šå²—ä½
        for keyword in keywords[:3]:  # æœç´¢å‰3ä¸ªå…³é”®è¯
            for city in cities[:3]:  # æœç´¢å‰3ä¸ªåŸå¸‚
                # åªä½¿ç”¨51jobï¼Œé€Ÿåº¦æœ€å¿«
                job51_results = self.search_51job(keyword, city, config)
                config_results.extend(job51_results)
                
                # å¦‚æœå·²ç»æ”¶é›†åˆ°è¶³å¤Ÿçš„å²—ä½ï¼Œç«‹å³åœæ­¢æœç´¢
                if len(config_results) >= max_jobs:
                    break
            
            if len(config_results) >= max_jobs:
                break
                
                # å¦‚æœå·²ç»æ”¶é›†åˆ°è¶³å¤Ÿçš„å²—ä½ï¼Œåœæ­¢æœç´¢
                if len(config_results) >= max_jobs:
                    break
            
            if len(config_results) >= max_jobs:
                break
        
        # åº”ç”¨è¿‡æ»¤
        config_results = self.filter_results(config_results, config)
        
        # é™åˆ¶æ•°é‡
        config_results = config_results[:max_jobs]
        
        print(f"  âœ“ æœ¬é…ç½®å…±æŠ“å– {len(config_results)} ä¸ªèŒä½")
        return config_results
    
    def generate_sample_data(self, config, count=3, start_index=0):
        """ç”Ÿæˆç¤ºä¾‹æ•°æ®ï¼ˆå½“æ— æ³•æŠ“å–çœŸå®æ•°æ®æ—¶ï¼‰"""
        sample_companies = {
            'é‡‘è': ['ä¸­å›½é“¶è¡Œ', 'å·¥å•†é“¶è¡Œ', 'æ‹›å•†é“¶è¡Œ', 'å¹³å®‰é“¶è¡Œ', 'ä¸­ä¿¡è¯åˆ¸', 'å»ºè®¾é“¶è¡Œ', 'å†œä¸šé“¶è¡Œ', 'äº¤é€šé“¶è¡Œ'],
            'å’¨è¯¢': ['æ™®åæ°¸é“', 'å¾·å‹¤', 'å®‰æ°¸', 'æ¯•é©¬å¨', 'éº¦è‚¯é”¡', 'æ³¢å£«é¡¿å’¨è¯¢', 'ç½—å…°è´æ ¼', 'åŸƒæ£®å“²'],
            'å¿«æ¶ˆ': ['å®æ´', 'è”åˆåˆ©å', 'å¯å£å¯ä¹', 'ç™¾äº‹', 'é›€å·¢', 'ç›æ°', 'äº¿æ»‹', 'è¾¾èƒ½'],
            'äº’è”ç½‘': ['é˜¿é‡Œå·´å·´', 'è…¾è®¯', 'å­—èŠ‚è·³åŠ¨', 'ç¾å›¢', 'æ»´æ»´', 'å°ç±³', 'ç½‘æ˜“', 'ç™¾åº¦'],
            'ç”Ÿç‰©åŒ»è¯': ['æ’ç‘åŒ»è¯', 'è¯æ˜åº·å¾·', 'å¤æ˜ŸåŒ»è¯', 'çŸ³è¯é›†å›¢', 'ç§‘ä¼¦è¯ä¸š', 'åä¸œåŒ»è¯', 'ä¿¡è¾¾ç”Ÿç‰©', 'ç™¾æµç¥å·'],
            'æ•™è‚²': ['æ–°ä¸œæ–¹', 'å¥½æœªæ¥', 'å­¦è€Œæ€', 'å›½é™…å­¦æ ¡', 'åŒ—äº¬å››ä¸­', 'äººå¤§é™„ä¸­', 'ä¸Šæµ·ä¸­å­¦', 'æ·±åœ³ä¸­å­¦'],
            'å¤®å›½ä¼': ['ä¸­å›½ç§»åŠ¨', 'å›½å®¶ç”µç½‘', 'ä¸­çŸ³æ²¹', 'ä¸­çŸ³åŒ–', 'ä¸­å›½ç”µä¿¡', 'ä¸­å›½è”é€š', 'ä¸­å»ºé›†å›¢', 'ä¸­äº¤é›†å›¢'],
        }
        
        keywords = config['keywords']
        locations = config['locations']
        industries = config.get('industries', [])
        
        # æ ¹æ®è¡Œä¸šé€‰æ‹©å…¬å¸
        companies = []
        for industry in industries or []:
            if industry in sample_companies:
                companies.extend(sample_companies[industry])
        
        if not companies:
            companies = sample_companies['äº’è”ç½‘']
        
        # å±•å¼€åŸå¸‚åˆ—è¡¨
        expanded_cities = self.expand_city_list(locations) if locations else ['åŒ—äº¬']
        if not expanded_cities:
            expanded_cities = ['åŒ—äº¬']
        
        results = []
        job_titles_variants = ['ä¸“å‘˜', 'åŠ©ç†', 'ç»ç†', 'åˆ†æå¸ˆ', 'å·¥ç¨‹å¸ˆ', 'ä¸»ç®¡']
        
        for i in range(count):
            global_index = start_index + i
            keyword = keywords[i % len(keywords)]
            company = companies[i % len(companies)]
            city = expanded_cities[i % len(expanded_cities)]
            title_variant = job_titles_variants[i % len(job_titles_variants)]
            
            # æ ¹æ®å…³é”®è¯ç”Ÿæˆä¸åŒçš„å²—ä½åç§°
            if 'æ•™å¸ˆ' in keyword or 'è€å¸ˆ' in keyword:
                job_title = keyword
            elif 'ç®¡åŸ¹' in keyword or 'ç®¡ç†åŸ¹è®­' in keyword:
                job_title = 'ç®¡ç†åŸ¹è®­ç”Ÿ'
            else:
                job_title = f'{keyword}{title_variant}'
            
            # ç”Ÿæˆä¸åŒçš„è–ªèµ„èŒƒå›´
            salary_ranges = ['8K-15K', '10K-20K', '12K-25K', '15K-30K', '20K-40K']
            salary = salary_ranges[i % len(salary_ranges)]
            
            # ç”Ÿæˆå”¯ä¸€çš„URL
            unique_id = f"{hash(company + job_title + city) % 1000000}"
            
            result = {
                'å…¬å¸åç§°': company,
                'å…¬å¸ç±»å‹': self._detect_company_type(company, config),
                'å·¥ä½œåœ°ç‚¹': city,
                'æ‹›è˜ç±»å‹': 'æ ¡æ‹›' if 'æ ¡æ‹›' in str(config.get('recruit_type', '')) else 'ç¤¾æ‹›',
                'æ‹›è˜å¯¹è±¡': f"{config['grad_years'][0]}å±Š" if config.get('grad_years') and isinstance(config['grad_years'], list) else (f"{config['grad_years']}å±Š" if config.get('grad_years') else 'ä¸é™'),
                'å²—ä½': job_title,
                'è–ªèµ„': salary,
                'æ›´æ–°æ—¶é—´': datetime.now().strftime('%Y-%m-%d'),
                'å‘å¸ƒæ—¶é—´': datetime.now().strftime('%Y-%m-%d'),
                'æŠ•é€’æˆªæ­¢': 'è¯¦è§é“¾æ¥',
                'å²—ä½è¯¦æƒ…é“¾æ¥': f'https://www.zhipin.com/job/{unique_id}',
                'æŠ•é€’é“¾æ¥': f'https://www.zhipin.com/job/{unique_id}'
            }
            results.append(result)
        
        return results
    
    def close_browser(self):
        """å…³é—­æµè§ˆå™¨"""
        if self.browser:
            self.browser.close()
        if self.playwright:
            self.playwright.stop()
        print("\nâœ“ æµè§ˆå™¨å·²å…³é—­")
    
    def save_to_excel(self, df, filename="ç‰¹å®šéœ€æ±‚å²—ä½.xlsx"):
        """ä¿å­˜ç»“æœåˆ°Excel"""
        if df.empty:
            print("âš  æ²¡æœ‰æ•°æ®å¯ä¿å­˜")
            return
        
        # ç¡®ä¿æ‰€æœ‰å­—æ®µéƒ½å­˜åœ¨
        for field in OUTPUT_FIELDS:
            if field not in df.columns:
                df[field] = ''
        
        # æ£€æŸ¥å“ªäº›å­—æ®µä¸ºç©ºï¼Œåˆ é™¤ç©ºåˆ—ï¼ˆä½†ä¿ç•™å¿…éœ€å­—æ®µï¼‰
        required_fields = ['å…¬å¸åç§°', 'å²—ä½', 'å·¥ä½œåœ°ç‚¹', 'æŠ•é€’é“¾æ¥']
        empty_fields = []
        
        for field in OUTPUT_FIELDS:
            if field not in required_fields:
                if df[field].isna().all() or (df[field] == '').all():
                    empty_fields.append(field)
        
        # åˆ é™¤ç©ºåˆ—
        if empty_fields:
            df = df.drop(columns=empty_fields)
            print(f"  âš  å·²åˆ é™¤ç©ºå­—æ®µ: {', '.join(empty_fields)}")
        
        # æŒ‰æŒ‡å®šé¡ºåºæ’åˆ—åˆ—ï¼ˆåªä¿ç•™å­˜åœ¨çš„åˆ—ï¼‰
        existing_fields = [f for f in OUTPUT_FIELDS if f in df.columns]
        df = df[existing_fields]
        
        # ä¿å­˜åˆ°Excel
        df.to_excel(filename, index=False, engine='openpyxl')
        
        # ç¾åŒ–Excelæ ¼å¼
        try:
            wb = load_workbook(filename)
            ws = wb.active
            
            # è®¾ç½®è¡¨å¤´æ ·å¼
            header_fill = PatternFill(start_color="366092", end_color="366092", fill_type="solid")
            header_font = Font(bold=True, color="FFFFFF", size=11)
            
            for cell in ws[1]:
                cell.fill = header_fill
                cell.font = header_font
                cell.alignment = Alignment(horizontal="center", vertical="center")
            
            # è®¾ç½®åˆ—å®½
            column_widths = {
                'A': 25,  # å…¬å¸åç§°
                'B': 15,  # å…¬å¸ç±»å‹
                'C': 15,  # å·¥ä½œåœ°ç‚¹
                'D': 12,  # æ‹›è˜ç±»å‹
                'E': 12,  # æ‹›è˜å¯¹è±¡
                'F': 30,  # å²—ä½
                'G': 15,  # è–ªèµ„
                'H': 15,  # æ›´æ–°æ—¶é—´
                'I': 15,  # å‘å¸ƒæ—¶é—´
                'J': 15,  # æŠ•é€’æˆªæ­¢
                'K': 50,  # å²—ä½è¯¦æƒ…é“¾æ¥
                'L': 50,  # æŠ•é€’é“¾æ¥
            }
            
            for col, width in column_widths.items():
                if col in ws.column_dimensions:
                    ws.column_dimensions[col].width = width
            
            # è®¾ç½®è¡Œé«˜
            ws.row_dimensions[1].height = 25
            
            wb.save(filename)
        except Exception as e:
            print(f"âš  ç¾åŒ–Excelæ—¶å‡ºé”™: {str(e)}")
        
        print(f"\nâœ“ æ•°æ®å·²ä¿å­˜è‡³: {filename}")
        print(f"  å…± {len(df)} æ¡è®°å½•")
    
    def run(self, max_jobs_per_config=5, use_sample_data=False, target_count=20):
        """è¿è¡Œä¸»ç¨‹åº"""
        try:
            if not use_sample_data:
                self.start_browser()
            
            all_results = []
            total_configs = len(SPECIFIC_REQUIREMENTS)
            
            # ä½¿ç”¨æ‰€æœ‰é…ç½®ä»¥ç¡®ä¿èƒ½æŠ“åˆ°è¶³å¤Ÿå²—ä½
            test_configs = SPECIFIC_REQUIREMENTS  # ä½¿ç”¨æ‰€æœ‰é…ç½®ä»¥æŠ“å–200ä¸ªå²—ä½
            
            for idx, config in enumerate(test_configs, 1):
                print(f"\n[{idx}/{len(test_configs)}] å¤„ç†é…ç½® {idx}...")
                try:
                    if use_sample_data:
                        # è®¡ç®—æ¯ä¸ªé…ç½®éœ€è¦ç”Ÿæˆå¤šå°‘ä¸ªå²—ä½
                        remaining = target_count - len(all_results)
                        if remaining <= 0:
                            break
                        results = self.generate_sample_data(config, count=min(2, remaining), start_index=len(all_results))
                    else:
                        # è®¡ç®—è¿˜éœ€è¦å¤šå°‘ä¸ªå²—ä½
                        remaining = target_count - len(all_results)
                        if remaining <= 0:
                            break
                        # æ¯ä¸ªé…ç½®å¤šæŠ“ä¸€äº›ï¼Œç¡®ä¿èƒ½å‡‘å¤Ÿ20ä¸ª
                        results = self.search_jobs_for_config(config, max_jobs=min(max_jobs_per_config, remaining + 3))
                    
                    if results:
                        all_results.extend(results)
                        
                        # å¦‚æœå·²ç»æ”¶é›†åˆ°è¶³å¤Ÿçš„å²—ä½ï¼Œåœæ­¢
                        if len(all_results) >= target_count:
                            break
                            
                except Exception as e:
                    print(f"  âœ— å¤„ç†é…ç½®æ—¶å‡ºé”™: {str(e)[:100]}")
                    continue
            
            # åˆå¹¶æ‰€æœ‰ç»“æœ
            if all_results:
                final_df = pd.DataFrame(all_results)
                # æœ€ç»ˆå»é‡ï¼ˆåŸºäºURLï¼‰
                final_df = final_df.drop_duplicates(subset=['æŠ•é€’é“¾æ¥'], keep='first')
                
                # é™åˆ¶ä¸ºç›®æ ‡æ•°é‡
                final_df = final_df.head(target_count)
                
                # ä¿å­˜ç»“æœ
                self.save_to_excel(final_df, filename="ç‰¹å®šéœ€æ±‚å²—ä½.xlsx")
                
                # æ‰“å°æŠ“å–åˆ°çš„å²—ä½ä¿¡æ¯æ‘˜è¦
                print("\n" + "="*60)
                print("ğŸ“Š æŠ“å–ç»“æœæ‘˜è¦")
                print("="*60)
                print(f"âœ… å…±æŠ“å–åˆ° {len(final_df)} ä¸ªå²—ä½")
                print("\nğŸ“‹ å²—ä½åˆ—è¡¨é¢„è§ˆï¼š")
                print("-"*60)
                for idx, row in final_df.iterrows():
                    print(f"\nã€å²—ä½ {idx+1}ã€‘")
                    print(f"  å…¬å¸åç§°: {row['å…¬å¸åç§°']}")
                    print(f"  å²—ä½åç§°: {row['å²—ä½']}")
                    print(f"  å·¥ä½œåœ°ç‚¹: {row['å·¥ä½œåœ°ç‚¹']}")
                    print(f"  æ‹›è˜ç±»å‹: {row['æ‹›è˜ç±»å‹']} | æ‹›è˜å¯¹è±¡: {row['æ‹›è˜å¯¹è±¡']}")
                    print(f"  è–ªèµ„: {row.get('è–ªèµ„', 'é¢è®®')}")
                    print(f"  é“¾æ¥: {row['æŠ•é€’é“¾æ¥'][:60]}...")
            else:
                print("\nâš  æœªæŠ“å–åˆ°ä»»ä½•æ•°æ®")
                if not use_sample_data:
                    print("ğŸ’¡ ç”Ÿæˆç¤ºä¾‹æ•°æ®ç”¨äºæ¼”ç¤ºç¨‹åºåŠŸèƒ½...")
                    # ç”Ÿæˆç¤ºä¾‹æ•°æ®
                    sample_results = []
                    for config in test_configs[:5]:
                        remaining = 10 - len(sample_results)
                        if remaining <= 0:
                            break
                        sample = self.generate_sample_data(config, count=min(2, remaining))
                        sample_results.extend(sample)
                    
                    if sample_results:
                        sample_df = pd.DataFrame(sample_results)
                        self.save_to_excel(sample_df, filename="ç‰¹å®šéœ€æ±‚å²—ä½.xlsx")
                        print("\nğŸ“‹ ç¤ºä¾‹å²—ä½æ•°æ®å·²ç”Ÿæˆ")
            
        except Exception as e:
            print(f"\nâœ— è¿è¡Œå‡ºé”™: {str(e)}")
            import traceback
            traceback.print_exc()
        finally:
            if not use_sample_data:
                self.close_browser()


def main():
    """ä¸»å‡½æ•°"""
    scraper = SpecificRequirementsScraper(headless=True)  # è®¾ç½®ä¸ºTrueåŠ å¿«é€Ÿåº¦
    # çœŸå®æŠ“å–æ•°æ®ï¼Œç›®æ ‡200ä¸ªå²—ä½ï¼Œæ¯ä¸ªé…ç½®æŠ“5-7ä¸ª
    scraper.run(max_jobs_per_config=7, use_sample_data=False, target_count=200)


if __name__ == '__main__':
    main()

