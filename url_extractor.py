import time
import random
import traceback
import re
from urllib.parse import urlparse

import pandas as pd
from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.chrome.service import Service
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from webdriver_manager.chrome import ChromeDriverManager
from selenium.common.exceptions import (
    TimeoutException,
    NoSuchElementException,
    ElementClickInterceptedException,
    StaleElementReferenceException,
    InvalidSessionIdException,
)
from bs4 import BeautifulSoup


# é…ç½®
INPUT_FILE = "/Users/changchun/Desktop/æœ€æ–°å­¦å‘˜éœ€æ±‚æŠ•é€’è¡¨æ ¼.xlsx"  # è¾“å…¥Excelæ–‡ä»¶è·¯å¾„
OUTPUT_FILE = "/Users/changchun/Desktop/æ‹›è˜ä¿¡æ¯æ±‡æ€».xlsx"  # è¾“å‡ºExcelæ–‡ä»¶è·¯å¾„
LINK_COLUMN = "çœŸå®æŠ•é€’é“¾æ¥"  # é“¾æ¥æ‰€åœ¨çš„åˆ—åï¼Œå¦‚æœåˆ—åä¸åŒè¯·ä¿®æ”¹


def create_driver():
    """å¯åŠ¨ Chrome æµè§ˆå™¨"""
    import subprocess
    
    # æ¸…ç†å¯èƒ½å­˜åœ¨çš„åƒµå°¸è¿›ç¨‹
    try:
        subprocess.run(["pkill", "-f", "chromedriver"], 
                      stdout=subprocess.DEVNULL, 
                      stderr=subprocess.DEVNULL, 
                      timeout=2)
        time.sleep(1)
    except Exception:
        pass
    
    options = webdriver.ChromeOptions()
    options.add_argument("--start-maximized")
    options.add_argument("--disable-blink-features=AutomationControlled")
    options.add_argument("--no-sandbox")
    options.add_argument("--disable-dev-shm-usage")
    # å¯é€‰ï¼šæ— å¤´æ¨¡å¼ï¼ˆä¸æ˜¾ç¤ºæµè§ˆå™¨çª—å£ï¼‰
    # options.add_argument("--headless")
    
    try:
        driver_path = ChromeDriverManager().install()
        service = Service(driver_path)
        driver = webdriver.Chrome(service=service, options=options)
        return driver
    except Exception as e:
        print(f"ä½¿ç”¨ webdriver_manager å¯åŠ¨å¤±è´¥: {e}")
        print("å°è¯•ä½¿ç”¨ç³»ç»Ÿé»˜è®¤ ChromeDriver...")
        driver = webdriver.Chrome(options=options)
        return driver


def random_sleep(min_sec=2, max_sec=4):
    """é˜²åçˆ¬ï¼šéšæœºç­‰å¾…"""
    time.sleep(random.uniform(min_sec, max_sec))


def extract_company_name(driver, soup):
    """æå–æ‹›è˜å…¬å¸åç§°"""
    company_name = ""
    
    # ç­–ç•¥1: æŸ¥æ‰¾åŒ…å«"å…¬å¸"ã€"ä¼ä¸š"ç­‰å…³é”®è¯çš„å…ƒç´ 
    company_keywords = ['å…¬å¸', 'ä¼ä¸š', 'é›†å›¢', 'è‚¡ä»½', 'æœ‰é™', 'é“¶è¡Œ', 'ä¿é™©', 'è¯åˆ¸', 'ç§‘æŠ€', 'æœ‰é™å…¬å¸']
    
    # å°è¯•å¤šç§é€‰æ‹©å™¨
    selectors = [
        # é€šè¿‡classæŸ¥æ‰¾
        "[class*='company']",
        "[class*='ä¼ä¸š']",
        "[class*='corp']",
        "[class*='firm']",
        # é€šè¿‡idæŸ¥æ‰¾
        "[id*='company']",
        "[id*='ä¼ä¸š']",
        # æ ‡é¢˜å…ƒç´ 
        "h1, h2, h3",
        # å¸¸è§çš„å…¬å¸åä½ç½®
        ".title, .name, .company-name, .enterprise-name",
    ]
    
    for selector in selectors:
        try:
            elements = driver.find_elements(By.CSS_SELECTOR, selector)
            for elem in elements:
                text = elem.text.strip()
                if not text or len(text) > 100:
                    continue
                # æ£€æŸ¥æ˜¯å¦åŒ…å«å…¬å¸å…³é”®è¯
                if any(keyword in text for keyword in company_keywords):
                    # æå–å…¬å¸åï¼ˆé€šå¸¸åœ¨å…¬å¸å…³é”®è¯ä¹‹å‰ï¼‰
                    for keyword in company_keywords:
                        if keyword in text:
                            # å°è¯•æå–å…¬å¸åéƒ¨åˆ†
                            parts = text.split(keyword)
                            if parts[0]:
                                company_name = (parts[0] + keyword).strip()
                                if 2 <= len(company_name) <= 50:
                                    return company_name
                            # å¦‚æœåˆ†å‰²å¤±è´¥ï¼Œä½¿ç”¨æ•´ä¸ªæ–‡æœ¬
                            if 2 <= len(text) <= 50:
                                company_name = text
                                return company_name
        except Exception:
            continue
    
    # ç­–ç•¥2: ä»é¡µé¢æ ‡é¢˜æå–
    try:
        title = driver.title
        if title:
            # ç§»é™¤å¸¸è§çš„åç¼€
            title = title.replace("æ‹›è˜", "").replace("æ ¡æ‹›", "").replace("å²—ä½", "").strip()
            if any(keyword in title for keyword in company_keywords):
                if 2 <= len(title) <= 50:
                    company_name = title
                    return company_name
    except Exception:
        pass
    
    # ç­–ç•¥3: ä»URLæå–ï¼ˆæŸäº›ç½‘ç«™URLåŒ…å«å…¬å¸åï¼‰
    try:
        url = driver.current_url
        domain = urlparse(url).netloc
        # æå–å­åŸŸåæˆ–è·¯å¾„ä¸­çš„å…¬å¸å
        parts = domain.split('.')
        if len(parts) > 2:
            potential_name = parts[0]
            if 2 <= len(potential_name) <= 20 and not potential_name.isdigit():
                company_name = potential_name
    except Exception:
        pass
    
    return company_name[:50] if company_name else ""


def extract_job_title(driver, soup):
    """æå–æ‹›è˜å²—ä½"""
    job_title = ""
    
    # å²—ä½å…³é”®è¯
    job_keywords = ['å²—ä½', 'èŒä½', 'æ‹›è˜', 'æ ¡æ‹›', 'èŒä½åç§°', 'å²—ä½åç§°', 'Job', 'Position', 'Title']
    
    # å°è¯•å¤šç§é€‰æ‹©å™¨
    selectors = [
        "[class*='job']",
        "[class*='position']",
        "[class*='èŒä½']",
        "[class*='å²—ä½']",
        "[id*='job']",
        "[id*='position']",
        "h1, h2, h3, h4",
        ".title, .job-title, .position-title, .name",
    ]
    
    for selector in selectors:
        try:
            elements = driver.find_elements(By.CSS_SELECTOR, selector)
            for elem in elements:
                text = elem.text.strip()
                if not text or len(text) > 100:
                    continue
                # æ£€æŸ¥æ˜¯å¦åŒ…å«å²—ä½å…³é”®è¯
                if any(keyword in text for keyword in job_keywords):
                    # æ¸…ç†æ–‡æœ¬
                    text = text.replace("æ‹›è˜", "").replace("æ ¡æ‹›", "").strip()
                    if 2 <= len(text) <= 100:
                        job_title = text
                        return job_title
        except Exception:
            continue
    
    # ç­–ç•¥2: ä»é¡µé¢æ ‡é¢˜æå–
    try:
        title = driver.title
        if title:
            # ç§»é™¤å…¬å¸åï¼Œä¿ç•™å²—ä½å
            title = re.sub(r'.*?æ‹›è˜', '', title)
            title = re.sub(r'.*?æ ¡æ‹›', '', title)
            title = title.strip()
            if 2 <= len(title) <= 100:
                job_title = title
                return job_title
    except Exception:
        pass
    
    return job_title[:100] if job_title else ""


def extract_base_location(driver, soup):
    """æå–Baseåœ°ç‚¹"""
    base_location = ""
    
    # åœ°ç‚¹å…³é”®è¯
    location_keywords = ['åœ°ç‚¹', 'å·¥ä½œåœ°ç‚¹', 'Base', 'Location', 'å·¥ä½œåŸå¸‚', 'åŸå¸‚', 'åœ°å€', 'å·¥ä½œåœ°å€']
    city_keywords = ['åŒ—äº¬', 'ä¸Šæµ·', 'å¹¿å·', 'æ·±åœ³', 'æ­å·', 'å—äº¬', 'æˆéƒ½', 'æ­¦æ±‰', 'è¥¿å®‰', 
                     'è‹å·', 'å¤©æ´¥', 'é‡åº†', 'é’å²›', 'å¤§è¿', 'å¦é—¨', 'å®æ³¢', 'æ— é”¡', 'é•¿æ²™',
                     'éƒ‘å·', 'æµå—', 'åˆè‚¥', 'ç¦å·', 'çŸ³å®¶åº„', 'å“ˆå°”æ»¨', 'é•¿æ˜¥', 'æ²ˆé˜³',
                     'æ±Ÿè‹', 'æµ™æ±Ÿ', 'å¹¿ä¸œ', 'å±±ä¸œ', 'æ²³å—', 'å››å·', 'æ¹–åŒ—', 'é™•è¥¿', 'æ¹–å—']
    
    # å°è¯•å¤šç§é€‰æ‹©å™¨
    selectors = [
        "[class*='location']",
        "[class*='åœ°ç‚¹']",
        "[class*='city']",
        "[class*='address']",
        "[id*='location']",
        "[id*='åœ°ç‚¹']",
        ".location, .city, .address, .base",
    ]
    
    for selector in selectors:
        try:
            elements = driver.find_elements(By.CSS_SELECTOR, selector)
            for elem in elements:
                text = elem.text.strip()
                if not text or len(text) > 30:
                    continue
                # æ£€æŸ¥æ˜¯å¦åŒ…å«åœ°ç‚¹å…³é”®è¯
                if any(keyword in text for keyword in location_keywords + city_keywords):
                    # æå–åŸå¸‚å
                    for city in city_keywords:
                        if city in text:
                            base_location = city
                            return base_location
                    # å¦‚æœæ²¡æœ‰åŒ¹é…åˆ°åŸå¸‚ï¼Œä½¿ç”¨æ•´ä¸ªæ–‡æœ¬
                    if 2 <= len(text) <= 30:
                        base_location = text
                        return base_location
        except Exception:
            continue
    
    # ç­–ç•¥2: ä»æ–‡æœ¬ä¸­æ­£åˆ™æå–
    try:
        page_text = driver.find_element(By.TAG_NAME, "body").text
        # æŸ¥æ‰¾åŸå¸‚å
        for city in city_keywords:
            if city in page_text:
                # æŸ¥æ‰¾åŸå¸‚é™„è¿‘çš„ä¸Šä¸‹æ–‡
                pattern = rf'[å·¥ä½œåœ°ç‚¹|Base|åœ°ç‚¹|åŸå¸‚].*?{city}'
                matches = re.findall(pattern, page_text)
                if matches:
                    base_location = city
                    return base_location
    except Exception:
        pass
    
    return base_location[:30] if base_location else ""


def extract_publish_time(driver, soup):
    """æå–å‘å¸ƒæ—¶é—´"""
    publish_time = ""
    
    # æ—¶é—´å…³é”®è¯
    time_keywords = ['å‘å¸ƒæ—¶é—´', 'å‘å¸ƒæ—¥æœŸ', 'æ›´æ–°æ—¥æœŸ', 'å‘å¸ƒæ—¶é—´', 'Publish', 'Date', 'Time', 'æ›´æ–°']
    
    # æ—¥æœŸæ ¼å¼æ¨¡å¼
    date_patterns = [
        r'(\d{4}-\d{1,2}-\d{1,2})',  # 2025-12-05
        r'(\d{4}/\d{1,2}/\d{1,2})',  # 2025/12/05
        r'(\d{4}å¹´\d{1,2}æœˆ\d{1,2}æ—¥)',  # 2025å¹´12æœˆ5æ—¥
        r'(\d{4}\.\d{1,2}\.\d{1,2})',  # 2025.12.05
    ]
    
    # å°è¯•å¤šç§é€‰æ‹©å™¨
    selectors = [
        "[class*='time']",
        "[class*='date']",
        "[class*='æ—¶é—´']",
        "[class*='æ—¥æœŸ']",
        "[id*='time']",
        "[id*='date']",
        ".time, .date, .publish-time, .update-time",
    ]
    
    for selector in selectors:
        try:
            elements = driver.find_elements(By.CSS_SELECTOR, selector)
            for elem in elements:
                text = elem.text.strip()
                if not text:
                    continue
                # æ£€æŸ¥æ˜¯å¦åŒ…å«æ—¶é—´å…³é”®è¯æˆ–æ—¥æœŸæ ¼å¼
                if any(keyword in text for keyword in time_keywords) or re.search(r'\d{4}[-/å¹´]\d{1,2}', text):
                    # æå–æ—¥æœŸ
                    for pattern in date_patterns:
                        matches = re.findall(pattern, text)
                        if matches:
                            publish_time = matches[0]
                            return publish_time
        except Exception:
            continue
    
    # ç­–ç•¥2: ä»æ•´ä¸ªé¡µé¢æ–‡æœ¬ä¸­æå–æ—¥æœŸ
    try:
        page_text = driver.find_element(By.TAG_NAME, "body").text
        for pattern in date_patterns:
            matches = re.findall(pattern, page_text)
            if matches:
                # å–ç¬¬ä¸€ä¸ªåŒ¹é…çš„æ—¥æœŸ
                publish_time = matches[0]
                return publish_time
    except Exception:
        pass
    
    return publish_time[:20] if publish_time else ""


def extract_apply_link(driver, soup):
    """æå–æŠ•é€’é“¾æ¥"""
    apply_link = ""
    
    # æŠ•é€’å…³é”®è¯
    apply_keywords = ['æŠ•é€’', 'ç”³è¯·', 'ç«‹å³æŠ•é€’', 'ç«‹å³ç”³è¯·', 'Apply', 'Submit', 'æŠ•é€’ç®€å†', 'ç”³è¯·èŒä½']
    
    # å°è¯•å¤šç§é€‰æ‹©å™¨
    selectors = [
        "a[href*='apply']",
        "a[href*='æŠ•é€’']",
        "a[href*='ç”³è¯·']",
        "button[onclick*='apply']",
        "a[class*='apply']",
        "button[class*='apply']",
        "a[class*='æŠ•é€’']",
        "button[class*='æŠ•é€’']",
    ]
    
    for selector in selectors:
        try:
            elements = driver.find_elements(By.CSS_SELECTOR, selector)
            for elem in elements:
                text = elem.text.strip()
                href = elem.get_attribute("href") or elem.get_attribute("onclick") or ""
                # æ£€æŸ¥æ˜¯å¦åŒ…å«æŠ•é€’å…³é”®è¯
                if any(keyword in text for keyword in apply_keywords) or any(keyword in href.lower() for keyword in ['apply', 'æŠ•é€’', 'ç”³è¯·']):
                    if href and href.startswith("http"):
                        apply_link = href
                        return apply_link
        except Exception:
            continue
    
    # ç­–ç•¥2: æŸ¥æ‰¾åŒ…å«"æŠ•é€’"æ–‡æœ¬çš„é“¾æ¥
    try:
        links = driver.find_elements(By.TAG_NAME, "a")
        for link in links:
            text = link.text.strip()
            href = link.get_attribute("href") or ""
            if any(keyword in text for keyword in apply_keywords) and href.startswith("http"):
                apply_link = href
                return apply_link
    except Exception:
        pass
    
    # ç­–ç•¥3: å¦‚æœæ‰¾ä¸åˆ°ï¼Œè¿”å›å½“å‰URL
    try:
        current_url = driver.current_url
        if current_url and current_url.startswith("http"):
            apply_link = current_url
    except Exception:
        pass
    
    return apply_link[:500] if apply_link else ""


def extract_company_type(driver, soup):
    """æå–ä¼ä¸šç±»å‹"""
    company_type = ""
    
    # ä¼ä¸šç±»å‹å…³é”®è¯
    type_keywords = {
        'å¤®/å›½ä¼': ['å¤®/å›½ä¼', 'å¤®å›½ä¼', 'å¤®ä¼', 'å›½ä¼', 'å›½æœ‰ä¼ä¸š', 'ä¸­å¤®ä¼ä¸š'],
        'å†…èµ„': ['å†…èµ„', 'æ°‘è¥ä¼ä¸š', 'æ°‘è¥'],
        'å¤–èµ„': ['å¤–èµ„', 'å¤–ä¼', 'Foreign'],
        'åˆèµ„': ['åˆèµ„', 'ä¸­å¤–åˆèµ„'],
        'ä¸Šå¸‚å…¬å¸': ['ä¸Šå¸‚å…¬å¸', 'ä¸Šå¸‚'],
    }
    
    # å°è¯•å¤šç§é€‰æ‹©å™¨
    selectors = [
        "[class*='type']",
        "[class*='ç±»å‹']",
        "[class*='tag']",
        "[class*='label']",
        "[class*='badge']",
        ".tag, .label, .badge, .type",
    ]
    
    for selector in selectors:
        try:
            elements = driver.find_elements(By.CSS_SELECTOR, selector)
            for elem in elements:
                text = elem.text.strip()
                if not text or len(text) > 20:
                    continue
                # æ£€æŸ¥æ˜¯å¦åŒ…å«ä¼ä¸šç±»å‹å…³é”®è¯
                for type_name, keywords in type_keywords.items():
                    if any(keyword in text for keyword in keywords):
                        company_type = type_name
                        return company_type
        except Exception:
            continue
    
    # ç­–ç•¥2: ä»é¡µé¢æ–‡æœ¬ä¸­æŸ¥æ‰¾
    try:
        page_text = driver.find_element(By.TAG_NAME, "body").text
        for type_name, keywords in type_keywords.items():
            if any(keyword in page_text for keyword in keywords):
                company_type = type_name
                return company_type
    except Exception:
        pass
    
    return company_type[:20] if company_type else ""


def extract_job_info_from_url(driver, url):
    """ä»URLæå–æ‹›è˜ä¿¡æ¯"""
    result = {
        "å…¬å¸åç§°": "",
        "å²—ä½": "",
        "ä¼ä¸šç±»å‹": "",
        "å‘å¸ƒæ—¶é—´": "",
        "Baseåœ°ç‚¹": "",
        "æŠ•é€’é“¾æ¥": "",
        "åŸå§‹é“¾æ¥": url,
    }
    
    try:
        print(f"   æ­£åœ¨è®¿é—®: {url[:60]}...")
        driver.get(url)
        random_sleep(2, 3)  # ç­‰å¾…é¡µé¢åŠ è½½
        
        # è·å–é¡µé¢æºç 
        html = driver.page_source
        soup = BeautifulSoup(html, 'html.parser')
        
        # æå–å„é¡¹ä¿¡æ¯
        result["å…¬å¸åç§°"] = extract_company_name(driver, soup)
        result["å²—ä½"] = extract_job_title(driver, soup)
        result["ä¼ä¸šç±»å‹"] = extract_company_type(driver, soup)
        result["å‘å¸ƒæ—¶é—´"] = extract_publish_time(driver, soup)
        result["Baseåœ°ç‚¹"] = extract_base_location(driver, soup)
        result["æŠ•é€’é“¾æ¥"] = extract_apply_link(driver, soup)
        
        # å¦‚æœæŠ•é€’é“¾æ¥ä¸ºç©ºï¼Œä½¿ç”¨åŸå§‹URL
        if not result["æŠ•é€’é“¾æ¥"]:
            result["æŠ•é€’é“¾æ¥"] = url
        
        print(f"   âœ… æå–å®Œæˆ: å…¬å¸={result['å…¬å¸åç§°'] or '(ç©º)'}, "
              f"å²—ä½={result['å²—ä½'] or '(ç©º)'}, "
              f"åœ°ç‚¹={result['Baseåœ°ç‚¹'] or '(ç©º)'}")
        
    except TimeoutException:
        print(f"   âš ï¸  é¡µé¢åŠ è½½è¶…æ—¶")
    except Exception as e:
        print(f"   âŒ æå–å¤±è´¥: {e}")
        traceback.print_exc()
    
    return result


def read_excel_links(file_path, link_column):
    """è¯»å–Excelæ–‡ä»¶ä¸­çš„é“¾æ¥"""
    try:
        df = pd.read_excel(file_path, engine='openpyxl')
        
        # æ£€æŸ¥åˆ—æ˜¯å¦å­˜åœ¨
        if link_column not in df.columns:
            print(f"âŒ é”™è¯¯: æ‰¾ä¸åˆ°åˆ— '{link_column}'")
            print(f"å¯ç”¨åˆ—: {', '.join(df.columns.tolist())}")
            return []
        
        # æå–é“¾æ¥ï¼ˆå»é™¤ç©ºå€¼ï¼‰
        links = df[link_column].dropna().tolist()
        # è¿‡æ»¤æ‰éURLçš„æ¡ç›®
        links = [link for link in links if isinstance(link, str) and (link.startswith("http://") or link.startswith("https://"))]
        
        print(f"âœ… ä»Excelè¯»å–åˆ° {len(links)} ä¸ªæœ‰æ•ˆé“¾æ¥")
        return links
        
    except Exception as e:
        print(f"âŒ è¯»å–Excelæ–‡ä»¶å¤±è´¥: {e}")
        traceback.print_exc()
        return []


def save_results(results, file_path):
    """ä¿å­˜ç»“æœåˆ°Excel"""
    try:
        if results:
            df = pd.DataFrame(results)
            # é‡æ–°æ’åˆ—åˆ—çš„é¡ºåº
            columns_order = ["å…¬å¸åç§°", "å²—ä½", "ä¼ä¸šç±»å‹", "å‘å¸ƒæ—¶é—´", "Baseåœ°ç‚¹", "æŠ•é€’é“¾æ¥", "åŸå§‹é“¾æ¥"]
            # åªä¿ç•™å­˜åœ¨çš„åˆ—
            columns_order = [col for col in columns_order if col in df.columns]
            df = df[columns_order]
            df.to_excel(file_path, index=False, engine='openpyxl')
            print(f"âœ… å·²ä¿å­˜ {len(results)} æ¡æ•°æ®åˆ°: {file_path}")
            return True
        else:
            # åˆ›å»ºç©ºæ–‡ä»¶
            df = pd.DataFrame(columns=["å…¬å¸åç§°", "å²—ä½", "ä¼ä¸šç±»å‹", "å‘å¸ƒæ—¶é—´", "Baseåœ°ç‚¹", "æŠ•é€’é“¾æ¥", "åŸå§‹é“¾æ¥"])
            df.to_excel(file_path, index=False, engine='openpyxl')
            print("âš ï¸  æ²¡æœ‰æ•°æ®ï¼Œå·²åˆ›å»ºç©ºæ–‡ä»¶")
            return False
    except Exception as e:
        print(f"âŒ ä¿å­˜æ–‡ä»¶å¤±è´¥: {e}")
        traceback.print_exc()
        return False


def main():
    driver = None
    all_results = []
    
    try:
        print("=" * 60)
        print("æ‹›è˜ä¿¡æ¯æå–ç¨‹åº")
        print("=" * 60)
        
        # è¯»å–Excelæ–‡ä»¶
        print(f"\n1. è¯»å–Excelæ–‡ä»¶: {INPUT_FILE}")
        links = read_excel_links(INPUT_FILE, LINK_COLUMN)
        
        if not links:
            print("âŒ æ²¡æœ‰æ‰¾åˆ°æœ‰æ•ˆé“¾æ¥ï¼Œç¨‹åºé€€å‡º")
            return
        
        # å¯åŠ¨æµè§ˆå™¨
        print("\n2. å¯åŠ¨æµè§ˆå™¨...")
        driver = create_driver()
        time.sleep(2)
        
        # å¤„ç†æ¯ä¸ªé“¾æ¥
        print(f"\n3. å¼€å§‹å¤„ç† {len(links)} ä¸ªé“¾æ¥...")
        for idx, url in enumerate(links, 1):
            try:
                print(f"\n[{idx}/{len(links)}] å¤„ç†é“¾æ¥...")
                result = extract_job_info_from_url(driver, url)
                all_results.append(result)
                
                # æ¯10æ¡ä¿å­˜ä¸€æ¬¡ï¼ˆé˜²æ­¢æ•°æ®ä¸¢å¤±ï¼‰
                if len(all_results) % 10 == 0:
                    save_results(all_results, OUTPUT_FILE)
                    print(f"   ğŸ’¾ å·²ä¿å­˜ {len(all_results)} æ¡æ•°æ®ï¼ˆå®šæœŸä¿å­˜ï¼‰")
                
                # é˜²åçˆ¬ç­‰å¾…
                random_sleep(2, 4)
                
            except InvalidSessionIdException as e:
                print(f"\nâš ï¸  æµè§ˆå™¨ä¼šè¯æ–­å¼€: {e}")
                print("å°è¯•ä¿å­˜å·²æå–çš„æ•°æ®...")
                save_results(all_results, OUTPUT_FILE)
                break
            except Exception as e:
                print(f"   âŒ å¤„ç†é“¾æ¥æ—¶å‡ºé”™: {e}")
                # å³ä½¿å‡ºé”™ä¹Ÿä¿å­˜ä¸€ä¸ªç©ºç»“æœ
                all_results.append({
                    "å…¬å¸åç§°": "",
                    "å²—ä½": "",
                    "ä¼ä¸šç±»å‹": "",
                    "å‘å¸ƒæ—¶é—´": "",
                    "Baseåœ°ç‚¹": "",
                    "æŠ•é€’é“¾æ¥": "",
                    "åŸå§‹é“¾æ¥": url,
                })
                traceback.print_exc()
                random_sleep(1, 2)
                continue
        
        # æœ€ç»ˆä¿å­˜
        print("\n4. ä¿å­˜æœ€ç»ˆç»“æœ...")
        save_results(all_results, OUTPUT_FILE)
        
        # æ˜¾ç¤ºç»“æœ
        print("\n" + "=" * 60)
        print("æå–å®Œæˆï¼")
        print("=" * 60)
        print(f"å…±å¤„ç† {len(links)} ä¸ªé“¾æ¥")
        print(f"æˆåŠŸæå– {len([r for r in all_results if r['å…¬å¸åç§°'] or r['å²—ä½']])} æ¡æœ‰æ•ˆæ•°æ®")
        print(f"æ–‡ä»¶ä¿å­˜ä½ç½®: {OUTPUT_FILE}")
        
        if all_results:
            print("\nå‰5æ¡æ•°æ®é¢„è§ˆï¼š")
            for i, item in enumerate(all_results[:5], 1):
                print(f"  {i}. å…¬å¸={item['å…¬å¸åç§°'] or '(ç©º)'}, "
                      f"å²—ä½={item['å²—ä½'] or '(ç©º)'}, "
                      f"ä¼ä¸šç±»å‹={item['ä¼ä¸šç±»å‹'] or '(ç©º)'}, "
                      f"å‘å¸ƒæ—¶é—´={item['å‘å¸ƒæ—¶é—´'] or '(ç©º)'}, "
                      f"åœ°ç‚¹={item['Baseåœ°ç‚¹'] or '(ç©º)'}")
        
    except KeyboardInterrupt:
        print("\n\nâš ï¸  ç”¨æˆ·ä¸­æ–­ç¨‹åº")
        save_results(all_results, OUTPUT_FILE)
    except Exception as e:
        print(f"\nâŒ ç¨‹åºå¼‚å¸¸: {e}")
        traceback.print_exc()
        save_results(all_results, OUTPUT_FILE)
    finally:
        if driver:
            try:
                print("\nå…³é—­æµè§ˆå™¨...")
                driver.quit()
            except Exception:
                pass


if __name__ == "__main__":
    main()

