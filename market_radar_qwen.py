#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
æµ·é©¬èŒåŠ Â·å¸‚åœºé›·è¾¾æ—¥æŠ¥
åŸºäº DrissionPage + é˜¿é‡Œé€šä¹‰åƒé—® (Qwen) çš„å¸‚åœºæƒ…æŠ¥è‡ªåŠ¨åŒ–ç³»ç»Ÿ

================================================================================
ã€é‡è¦ã€‘è¿è¡Œå‰å¿…é¡»æ“ä½œï¼šå¯åŠ¨ Chrome è°ƒè¯•æ¨¡å¼
================================================================================

å°çº¢ä¹¦é‡‡é›†éœ€è¦æ¥ç®¡æ‚¨æœ¬åœ°å·²ç™»å½•çš„ Chrome æµè§ˆå™¨ï¼Œè¯·å…ˆæ‰§è¡Œä»¥ä¸‹æ“ä½œï¼š

ã€Mac ç³»ç»Ÿã€‘
åœ¨ç»ˆç«¯æ‰§è¡Œï¼š
    open -n /Applications/Google\ Chrome.app --args --remote-debugging-port=9222

ã€Windows ç³»ç»Ÿã€‘
åœ¨å‘½ä»¤æç¤ºç¬¦ï¼ˆCMDï¼‰æˆ– PowerShell æ‰§è¡Œï¼š
    "C:\Program Files\Google\Chrome\Application\chrome.exe" --remote-debugging-port=9222

ã€Linux ç³»ç»Ÿã€‘
åœ¨ç»ˆç«¯æ‰§è¡Œï¼š
    google-chrome --remote-debugging-port=9222

æ‰§è¡Œåï¼ŒChrome ä¼šä»¥è°ƒè¯•æ¨¡å¼å¯åŠ¨ï¼Œç„¶åï¼š
1. æ‰‹åŠ¨ç™»å½•å°çº¢ä¹¦è´¦å·ï¼ˆhttps://www.xiaohongshu.comï¼‰
2. ä¿æŒ Chrome æµè§ˆå™¨æ‰“å¼€çŠ¶æ€
3. å†è¿è¡Œæœ¬è„šæœ¬

å¦‚æœå¿˜è®°å¯åŠ¨è°ƒè¯•æ¨¡å¼ï¼Œè„šæœ¬ä¼šæç¤ºé”™è¯¯å¹¶é€€å‡ºã€‚
================================================================================
"""

import json
import time
import random
import re
import os
import pandas as pd
from datetime import datetime, timedelta
from typing import List, Dict, Any, Optional
from DrissionPage import ChromiumPage, ChromiumOptions
import dashscope
from dashscope import Generation
import logging

# å¯¼å…¥å¤‡ç”¨æ–¹æ¡ˆæ‰€éœ€çš„åº“
try:
    import requests
    from bs4 import BeautifulSoup
    BING_BACKUP_AVAILABLE = True
except ImportError:
    BING_BACKUP_AVAILABLE = False
    logger.warning("requests æˆ– BeautifulSoup æœªå®‰è£…ï¼ŒBing å¤‡ç”¨æ–¹æ¡ˆä¸å¯ç”¨")

# é…ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('market_radar_qwen.log', encoding='utf-8'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)

# ==================== ç¡¬ç¼–ç é…ç½® ====================
# é˜¿é‡Œé€šä¹‰åƒé—® API Key
DASHSCOPE_API_KEY = "sk-668c28bae516493d9ea8a3662118ec98"

# ç«å“å…³é”®è¯åˆ—è¡¨
KEYWORDS = ['DBCèŒæ¢¦', 'é€”é¸½æ±‚èŒ', 'Offerå…ˆç”Ÿ', 'çˆ±æ€ç›Š', 'æµ·é©¬èŒåŠ ']

# æ—¶é—´èŒƒå›´ï¼ˆæœ€è¿‘3å¤©ï¼‰
DAYS_BACK = 3
CURRENT_DATE = datetime.now().strftime("%Y-%m-%d")


class MarketRadarQwen:
    """å¸‚åœºé›·è¾¾ç³»ç»Ÿï¼ˆåŸºäº Qwenï¼‰"""
    
    def __init__(self, use_debug_port: bool = True):
        """
        åˆå§‹åŒ–æµè§ˆå™¨å’Œ AI å®¢æˆ·ç«¯
        
        Args:
            use_debug_port: æ˜¯å¦ä½¿ç”¨è°ƒè¯•ç«¯å£è¿æ¥å·²æ‰“å¼€çš„ Chromeï¼ˆç”¨äºå°çº¢ä¹¦é‡‡é›†ï¼‰
        """
        # é…ç½® DashScope
        dashscope.api_key = DASHSCOPE_API_KEY
        logger.info("é˜¿é‡Œé€šä¹‰åƒé—® (Qwen) åˆå§‹åŒ–æˆåŠŸ")
        
        # é…ç½®æµè§ˆå™¨
        try:
            if use_debug_port:
                # å°è¯•è¿æ¥æœ¬åœ° 9222 ç«¯å£çš„ Chromeï¼ˆç”¨äºå°çº¢ä¹¦é‡‡é›†ï¼‰
                try:
                    self.page = ChromiumPage(addr='127.0.0.1:9222')
                    logger.info("æˆåŠŸè¿æ¥åˆ°æœ¬åœ° Chrome è°ƒè¯•ç«¯å£ (9222)")
                    logger.info("æç¤ºï¼šè¯·ç¡®ä¿ Chrome å·²ä»¥è°ƒè¯•æ¨¡å¼å¯åŠ¨ï¼Œå¹¶å·²ç™»å½•å°çº¢ä¹¦è´¦å·")
                except Exception as e:
                    logger.warning(f"è¿æ¥æœ¬åœ° Chrome è°ƒè¯•ç«¯å£å¤±è´¥: {str(e)}")
                    logger.warning("=" * 80)
                    logger.warning("ã€é‡è¦æç¤ºã€‘è¯·å…ˆå¯åŠ¨ Chrome è°ƒè¯•æ¨¡å¼ï¼š")
                    logger.warning("Mac: open -n /Applications/Google\\ Chrome.app --args --remote-debugging-port=9222")
                    logger.warning("Windows: \"C:\\Program Files\\Google\\Chrome\\Application\\chrome.exe\" --remote-debugging-port=9222")
                    logger.warning("=" * 80)
                    # é™çº§ä¸ºæ™®é€šæ¨¡å¼
                    options = ChromiumOptions()
                    options.headless(False)
                    options.set_argument('--disable-blink-features=AutomationControlled')
                    self.page = ChromiumPage(addr_or_opts=options)
                    logger.info("å·²é™çº§ä¸ºæ™®é€šæµè§ˆå™¨æ¨¡å¼ï¼ˆå°çº¢ä¹¦é‡‡é›†å¯èƒ½å—é™ï¼‰")
            else:
                # æ™®é€šæ¨¡å¼
                options = ChromiumOptions()
                options.headless(False)
                options.set_argument('--disable-blink-features=AutomationControlled')
                self.page = ChromiumPage(addr_or_opts=options)
                logger.info("æµè§ˆå™¨åˆå§‹åŒ–æˆåŠŸï¼ˆæ™®é€šæ¨¡å¼ï¼‰")
        except Exception as e:
            logger.error(f"æµè§ˆå™¨åˆå§‹åŒ–å¤±è´¥: {str(e)}")
            logger.error("æç¤ºï¼šå¦‚æœé‡åˆ°è¿æ¥é”™è¯¯ï¼Œè¯·å…ˆæ‰‹åŠ¨å¯åŠ¨ Chrome è°ƒè¯•æ¨¡å¼")
            self.page = None
        
        # å­˜å‚¨é‡‡é›†çš„æ•°æ®
        self.douyin_data = []
        self.xhs_data = []
        self.wechat_data = []
        
        logger.info(f"å¸‚åœºé›·è¾¾ç³»ç»Ÿåˆå§‹åŒ–å®Œæˆï¼Œå½“å‰æ—¥æœŸ: {CURRENT_DATE}")
    
    def parse_time(self, time_str: str) -> Optional[datetime]:
        """
        è§£ææ—¶é—´å­—ç¬¦ä¸²ï¼Œè¿”å› datetime å¯¹è±¡
        
        Args:
            time_str: æ—¶é—´å­—ç¬¦ä¸²ï¼Œå¯èƒ½æ˜¯"2å°æ—¶å‰"ã€"æ˜¨å¤©"ã€"2025-12-10"ç­‰æ ¼å¼
        
        Returns:
            datetime å¯¹è±¡ï¼Œå¦‚æœæ— æ³•è§£æè¿”å› None
        """
        if not time_str:
            return None
        
        time_str = time_str.strip()
        now = datetime.now()
        
        try:
            # å¤„ç†"Xå°æ—¶å‰"ã€"Xåˆ†é’Ÿå‰"
            if "åˆ†é’Ÿå‰" in time_str:
                match = re.search(r'(\d+)åˆ†é’Ÿå‰', time_str)
                if match:
                    minutes = int(match.group(1))
                    return now - timedelta(minutes=minutes)
            
            if "å°æ—¶å‰" in time_str:
                match = re.search(r'(\d+)å°æ—¶å‰', time_str)
                if match:
                    hours = int(match.group(1))
                    return now - timedelta(hours=hours)
            
            # å¤„ç†"æ˜¨å¤©"
            if "æ˜¨å¤©" in time_str:
                return now - timedelta(days=1)
            
            # å¤„ç†"Xå¤©å‰"
            match = re.search(r'(\d+)å¤©å‰', time_str)
            if match:
                days = int(match.group(1))
                return now - timedelta(days=days)
            
            # å¤„ç†æ ‡å‡†æ—¥æœŸæ ¼å¼ "2025-12-10"
            date_match = re.search(r'(\d{4})-(\d{1,2})-(\d{1,2})', time_str)
            if date_match:
                year, month, day = map(int, date_match.groups())
                return datetime(year, month, day)
            
            # å¤„ç†"12-10"æ ¼å¼ï¼ˆå‡è®¾æ˜¯ä»Šå¹´ï¼‰
            date_match = re.search(r'(\d{1,2})-(\d{1,2})', time_str)
            if date_match:
                month, day = map(int, date_match.groups())
                return datetime(now.year, month, day)
            
            return None
            
        except Exception as e:
            logger.warning(f"æ—¶é—´è§£æå¤±è´¥: {time_str}, é”™è¯¯: {str(e)}")
            return None
    
    def is_recent(self, time_str: str) -> bool:
        """
        åˆ¤æ–­æ—¶é—´å­—ç¬¦ä¸²æ˜¯å¦åœ¨æœ€è¿‘3å¤©å†…
        
        Args:
            time_str: æ—¶é—´å­—ç¬¦ä¸²
        
        Returns:
            æ˜¯å¦åœ¨æœ€è¿‘3å¤©å†…
        """
        parsed_time = self.parse_time(time_str)
        if not parsed_time:
            return True  # æ— æ³•è§£ææ—¶é»˜è®¤ä¿ç•™
        
        three_days_ago = datetime.now() - timedelta(days=DAYS_BACK)
        return parsed_time >= three_days_ago
    
    def manual_login(self):
        """
        ç¬¬ä¸€é˜¶æ®µï¼šäººå·¥ç™»å½• (Blocking Wait)
        æ‰“å¼€ä¸‰ä¸ªæ ‡ç­¾é¡µï¼Œç­‰å¾…ç”¨æˆ·æ‰‹åŠ¨ç™»å½•
        """
        if not self.page:
            raise RuntimeError("æµè§ˆå™¨æœªåˆå§‹åŒ–ï¼Œæ— æ³•æ‰§è¡Œç™»å½•")
        
        logger.info("=" * 80)
        logger.info("ç¬¬ä¸€é˜¶æ®µï¼šäººå·¥ç™»å½•")
        logger.info("=" * 80)
        
        # æ‰“å¼€æŠ–éŸ³æ ‡ç­¾é¡µ (Tab 1)
        logger.info("æ­£åœ¨æ‰“å¼€æŠ–éŸ³...")
        self.page.get('https://www.douyin.com')
        time.sleep(3)
        print("âœ… æŠ–éŸ³æ ‡ç­¾é¡µå·²æ‰“å¼€")
        
        # æ‰“å¼€å°çº¢ä¹¦æ ‡ç­¾é¡µ (Tab 2)
        logger.info("æ­£åœ¨æ‰“å¼€å°çº¢ä¹¦...")
        self.page.new_tab()
        time.sleep(1)
        self.page.get('https://www.xiaohongshu.com')
        time.sleep(3)
        print("âœ… å°çº¢ä¹¦æ ‡ç­¾é¡µå·²æ‰“å¼€")
        
        # æ‰“å¼€æœç‹—å¾®ä¿¡æ ‡ç­¾é¡µ (Tab 3)
        logger.info("æ­£åœ¨æ‰“å¼€æœç‹—å¾®ä¿¡...")
        self.page.new_tab()
        time.sleep(1)
        self.page.get('https://weixin.sogou.com')
        time.sleep(3)
        print("âœ… æœç‹—å¾®ä¿¡æ ‡ç­¾é¡µå·²æ‰“å¼€")
        
        # ç­‰å¾…ç”¨æˆ·æ‰‹åŠ¨ç™»å½•
        print("\n" + "=" * 80)
        print("ğŸ”´ ã€é‡è¦ã€‘è¯·æ‰‹åŠ¨å®Œæˆä»¥ä¸‹æ“ä½œï¼š")
        print("")
        print("1ï¸âƒ£  æŠ–éŸ³ï¼šæ‰«ç æˆ–è¾“å…¥è´¦å·å¯†ç ç™»å½•ï¼Œç¡®ä¿èƒ½çœ‹åˆ°é¦–é¡µæ¨èå†…å®¹")
        print("2ï¸âƒ£  å°çº¢ä¹¦ï¼šæ‰«ç æˆ–è¾“å…¥è´¦å·å¯†ç ç™»å½•ï¼Œç¡®ä¿èƒ½çœ‹åˆ°é¦–é¡µæ¨èå†…å®¹")
        print("3ï¸âƒ£  æœç‹—å¾®ä¿¡ï¼šå¦‚æœå‡ºç°éªŒè¯ç ï¼Œè¯·æ‰‹åŠ¨å®ŒæˆéªŒè¯")
        print("")
        print("âš ï¸  è¯·ç¡®ä¿ä¸‰ä¸ªå¹³å°éƒ½å·²æˆåŠŸç™»å½•ï¼")
        print("   ç™»å½•å®Œæˆåï¼Œè¯·å›åˆ°è¿™é‡ŒæŒ‰ã€å›è½¦é”®ã€‘å¼€å§‹è‡ªåŠ¨åŒ–é‡‡é›†...")
        print("=" * 80)
        
        # é˜»å¡ç­‰å¾…ç”¨æˆ·æŒ‰å›è½¦
        try:
            input("\nğŸ‘‰ ç¡®è®¤å·²å…¨éƒ¨ç™»å½•åï¼ŒæŒ‰å›è½¦é”®ç»§ç»­...")
        except EOFError:
            # éäº¤äº’å¼ç¯å¢ƒï¼Œç­‰å¾…60ç§’
            logger.warning("æ£€æµ‹åˆ°éäº¤äº’å¼ç¯å¢ƒï¼Œç­‰å¾…60ç§’åè‡ªåŠ¨ç»§ç»­...")
            for i in range(60, 0, -10):
                print(f"â³ ç­‰å¾…ä¸­... {i}ç§’åè‡ªåŠ¨ç»§ç»­")
                time.sleep(10)
        
        logger.info("ç”¨æˆ·ç¡®è®¤ç™»å½•å®Œæˆï¼Œå¼€å§‹æ‰§è¡Œé‡‡é›†")
        time.sleep(2)  # é¢å¤–ç­‰å¾…2ç§’ç¡®ä¿é¡µé¢ç¨³å®š
    
    def crawl_douyin(self) -> List[Dict[str, Any]]:
        """
        ç¬¬äºŒé˜¶æ®µï¼šæŠ–éŸ³é‡‡é›†ï¼ˆçœŸäººæ·±æ½œæ¨¡å¼ï¼‰
        å½»åº•é‡å†™ï¼šä½¿ç”¨æš´åŠ›æ–‡æœ¬æå–ï¼Œä¸¥ç¦ä½¿ç”¨ HTML
        """
        if not self.page:
            logger.error("æµè§ˆå™¨æœªåˆå§‹åŒ–")
            return []
        
        logger.info("=" * 80)
        logger.info("ç¬¬äºŒé˜¶æ®µï¼šæŠ–éŸ³é‡‡é›†ï¼ˆçœŸäººæ·±æ½œæ¨¡å¼ï¼‰")
        logger.info("=" * 80)
        
        results = []
        
        try:
            # åˆ‡æ¢åˆ°æŠ–éŸ³æ ‡ç­¾é¡µ
            self.page.get('https://www.douyin.com')
            time.sleep(3)
            
            for keyword in KEYWORDS:
                try:
                    logger.info(f"æœç´¢å…³é”®è¯: {keyword}")
                    print(f"[æŠ–éŸ³æ·±æ½œ] æœç´¢å…³é”®è¯: {keyword}")
                    
                    # URL æ³¨å…¥ç­–ç•¥ï¼šç›´æ¥è®¿é—®æ’åºåçš„æœç´¢ç»“æœ
                    search_url = f"https://www.douyin.com/search/{keyword}?publish_time=1&sort_type=2&source=switch_tab&type=video"
                    self.page.get(search_url)
                    time.sleep(random.uniform(3, 5))
                    
                    logger.info(f"å½“å‰é¡µé¢ URL: {self.page.url}")
                    print(f"[æŠ–éŸ³æ·±æ½œ] å½“å‰é¡µé¢ URL: {self.page.url}")
                    
                    # ä½¿ç”¨ XPath æŸ¥æ‰¾æ‰€æœ‰è§†é¢‘é“¾æ¥
                    video_urls = []
                    try:
                        print("[æŠ–éŸ³æ·±æ½œ] ä½¿ç”¨ XPath æŸ¥æ‰¾è§†é¢‘é“¾æ¥...")
                        links = self.page.eles('xpath://a[contains(@href, "/video/")]', timeout=10)
                        logger.info(f"XPath æ‰¾åˆ° {len(links)} ä¸ªåŒ…å« /video/ çš„é“¾æ¥")
                        print(f"[æŠ–éŸ³æ·±æ½œ] XPath æ‰¾åˆ° {len(links)} ä¸ªé“¾æ¥")
                        
                        for link in links:
                            try:
                                href = link.attr('href') or ''
                                link_text = link.text or ''
                                
                                # è¿‡æ»¤æ¡ä»¶ï¼šé“¾æ¥æ–‡æœ¬é•¿åº¦ > 5
                                if '/video/' in href and len(link_text.strip()) > 5:
                                    # å¤„ç†ç›¸å¯¹é“¾æ¥
                                    if not href.startswith('http'):
                                        if href.startswith('//'):
                                            href = 'https:' + href
                                        elif href.startswith('/'):
                                            href = 'https://www.douyin.com' + href
                                        else:
                                            continue
                                    
                                    # å»é‡
                                    if href not in video_urls and 'douyin.com' in href:
                                        video_urls.append(href)
                                        print(f"[æŠ–éŸ³æ·±æ½œ] âœ“ æ‰¾åˆ°è§†é¢‘: {link_text[:30] if link_text else href[:50]}...")
                                        
                                        # é™åˆ¶ä¸ºå‰5ä¸ª
                                        if len(video_urls) >= 5:
                                            break
                            except Exception as e:
                                logger.debug(f"  å¤„ç†é“¾æ¥æ—¶å‡ºé”™: {str(e)}")
                                continue
                        
                        video_urls = video_urls[:5]
                        logger.info(f"æ‰¾åˆ° {len(video_urls)} ä¸ªè§†é¢‘é“¾æ¥")
                        print(f"[æŠ–éŸ³æ·±æ½œ] æœ€ç»ˆæ‰¾åˆ° {len(video_urls)} ä¸ªè§†é¢‘é“¾æ¥")
                        
                    except Exception as e:
                        logger.warning(f"æå–è§†é¢‘é“¾æ¥å¤±è´¥: {str(e)}")
                        print(f"[æŠ–éŸ³æ·±æ½œ] æå–å¤±è´¥: {str(e)}")
                        continue
                    
                    if not video_urls:
                        logger.warning(f"æœªæ‰¾åˆ°ä»»ä½•è§†é¢‘ï¼Œè·³è¿‡å…³é”®è¯: {keyword}")
                        print(f"[æŠ–éŸ³æ·±æ½œ] æœªæ‰¾åˆ°ä»»ä½•è§†é¢‘ï¼Œè·³è¿‡å…³é”®è¯: {keyword}")
                        continue
                    
                    # å¾ªç¯é‡‡é›†ï¼šçœŸäººæ·±æ½œæ¨¡å¼
                    for idx, video_url in enumerate(video_urls, 1):
                        new_tab = None
                        try:
                            logger.info(f"  å¤„ç†è§†é¢‘ {idx}/{len(video_urls)}: {video_url[:60]}...")
                            print(f"[æŠ–éŸ³æ·±æ½œ] æ­£åœ¨æ·±æ½œè§†é¢‘ {idx}/{len(video_urls)}: {video_url[:60]}...")
                            
                            # æ‰“å¼€æ–°æ ‡ç­¾é¡µ
                            new_tab = self.page.new_tab()
                            new_tab.get(video_url)
                            
                            # å¼ºåˆ¶ç­‰å¾…ï¼šå¿…é¡»ç­‰å¤Ÿæ—¶é—´
                            print(f"  [æŠ–éŸ³æ·±æ½œ] å¼ºåˆ¶ç­‰å¾… 3 ç§’ï¼Œç¡®ä¿é¡µé¢æ¸²æŸ“...")
                            time.sleep(3)
                            
                            # æå–æ ‡é¢˜
                            title = ""
                            try:
                                title_elem = new_tab.ele('tag:h1', timeout=2)
                                if not title_elem:
                                    title_elem = new_tab.ele('tag:div@class*=title', timeout=2)
                                if title_elem:
                                    title = title_elem.text or ""
                                    print(f"  [æŠ–éŸ³æ·±æ½œ] æå–åˆ°æ ‡é¢˜: {title[:50]}...")
                            except:
                                pass
                            
                            # æå–å‘å¸ƒæ—¶é—´
                            date_str = ""
                            try:
                                date_selectors = ['tag:span@class*=time', 'tag:time', 'tag:div@class*=date']
                                for sel in date_selectors:
                                    try:
                                        date_elem = new_tab.ele(sel, timeout=1)
                                        if date_elem:
                                            date_str = date_elem.text or ""
                                            break
                                    except:
                                        continue
                            except:
                                pass
                            
                            # æ—¶é—´è¿‡æ»¤
                            if date_str and not self.is_recent(date_str):
                                logger.info(f"    è§†é¢‘è¶…å‡º3å¤©èŒƒå›´ï¼Œè·³è¿‡: {date_str}")
                                print(f"  [æŠ–éŸ³æ·±æ½œ] è§†é¢‘è¶…å‡º3å¤©èŒƒå›´ï¼Œè·³è¿‡: {date_str}")
                                new_tab.close()
                                continue
                            
                            # æ»šåŠ¨è¯„è®ºåŒºï¼ˆå…³é”®æ­¥éª¤ï¼‰
                            print(f"  [æŠ–éŸ³æ·±æ½œ] æ»šåŠ¨è¯„è®ºåŒºåŠ è½½å†…å®¹...")
                            try:
                                new_tab.scroll.down(1000)
                                time.sleep(2)
                                new_tab.scroll.down(500)
                                time.sleep(1)
                            except Exception as e:
                                print(f"  [æŠ–éŸ³æ·±æ½œ] æ»šåŠ¨å¤±è´¥: {str(e)}")
                            
                            # æ–‡æœ¬æå–ï¼ˆä¸¥ç¦ä½¿ç”¨ HTMLï¼‰+ äºŒæ¬¡æ¸…æ´—
                            print(f"  [æŠ–éŸ³ä¼˜åŒ–] å¼€å§‹æå–è¯„è®ºæ–‡æœ¬ï¼ˆä¸¥ç¦ä½¿ç”¨HTMLï¼‰...")
                            comments = []
                            
                            # å°è¯•ç‚¹å‡»"å±•å¼€æ›´å¤šè¯„è®º"
                            try:
                                expand_btns = new_tab.eles('xpath://button[contains(text(), "å±•å¼€") or contains(text(), "æ›´å¤š") or contains(@class, "expand")]', timeout=2)
                                for btn in expand_btns[:3]:  # ç‚¹å‡»å‰3ä¸ªå±•å¼€æŒ‰é’®
                                    try:
                                        btn.click()
                                        time.sleep(1)
                                        print(f"  [æŠ–éŸ³ä¼˜åŒ–] âœ“ ç‚¹å‡»å±•å¼€æŒ‰é’®")
                                    except:
                                        continue
                            except:
                                pass
                            
                            try:
                                # ç­–ç•¥1ï¼šå°è¯•æŸ¥æ‰¾è¯„è®ºå®¹å™¨ï¼ˆä½¿ç”¨ class*="comment" æˆ– data-e2e="comment"ï¼‰
                                comment_selectors = [
                                    'xpath://div[contains(@class, "comment")]',
                                    'xpath://div[@data-e2e="comment"]',
                                    'xpath://div[contains(@class, "comment-item")]',
                                ]
                                
                                comment_items = []
                                for selector in comment_selectors:
                                    try:
                                        items = new_tab.eles(selector, timeout=3)
                                        if items:
                                            comment_items = items
                                            print(f"  [æŠ–éŸ³ä¼˜åŒ–] ä½¿ç”¨é€‰æ‹©å™¨ '{selector}' æ‰¾åˆ° {len(items)} ä¸ªè¯„è®ºå…ƒç´ ")
                                            break
                                    except:
                                        continue
                                
                                # ä»è¯„è®ºå…ƒç´ ä¸­æå–æ–‡æœ¬
                                for item in comment_items[:30]:  # æ£€æŸ¥å‰30ä¸ª
                                    try:
                                        text = item.text or ""
                                        text = self._clean_html_text(text)  # äºŒæ¬¡æ¸…æ´—HTMLæ®‹ç•™
                                        text = text.strip()
                                        
                                        # è¿‡æ»¤æ¡ä»¶ï¼šé•¿åº¦ 5-200ï¼Œä¸”ä¸åŒ…å«ç³»ç»Ÿè¯
                                        if 5 <= len(text) <= 200:
                                            system_words = ['å…³æ³¨', 'ç‚¹èµ', 'æ”¶è—', 'åˆ†äº«', 'è¯„è®º', 'å›å¤', 'æŸ¥çœ‹æ›´å¤š', 'å±•å¼€', 'æ”¶èµ·', 'è½¬å‘', 'ä¸¾æŠ¥', 'æŠ–éŸ³', 'è®°å½•ç¾å¥½ç”Ÿæ´»']
                                            if not any(sys_word in text for sys_word in system_words):
                                                if text not in comments:
                                                    comments.append(text)
                                                    print(f"  [æŠ–éŸ³ä¼˜åŒ–] âœ“ æ‰¾åˆ°è¯„è®º: {text[:50]}...")
                                                    print(f"  [æŠ–éŸ³ä¼˜åŒ–]   åŸå§‹æ–‡æœ¬å‰200å­—ç¬¦: {text[:200]}")
                                                    if len(comments) >= 5:
                                                        break
                                    except:
                                        continue
                                
                            except Exception as e:
                                print(f"  [æŠ–éŸ³ä¼˜åŒ–] ç­–ç•¥1å¤±è´¥: {str(e)}")
                            
                            # ç­–ç•¥2ï¼šå…œåº•ç­–ç•¥ - è·å–æ•´ä¸ªé¡µé¢æ–‡æœ¬ï¼ŒäºŒæ¬¡æ¸…æ´—åè¿‡æ»¤
                            if not comments:
                                print(f"  [æŠ–éŸ³ä¼˜åŒ–] ç­–ç•¥1æœªæ‰¾åˆ°è¯„è®ºï¼Œä½¿ç”¨å…œåº•ç­–ç•¥...")
                                try:
                                    body_elem = new_tab.ele('tag:body', timeout=2)
                                    if body_elem:
                                        full_text = body_elem.text or ""
                                        print(f"  [æŠ–éŸ³ä¼˜åŒ–] é¡µé¢æ–‡æœ¬æ€»é•¿åº¦: {len(full_text)} å­—ç¬¦")
                                        
                                        # æŒ‰è¡Œåˆ†å‰²ï¼ŒäºŒæ¬¡æ¸…æ´—ï¼Œä¿ç•™é•¿åº¦ 5-200 çš„è¡Œ
                                        lines = full_text.split('\n')
                                        for line in lines:
                                            line = self._clean_html_text(line)  # äºŒæ¬¡æ¸…æ´—
                                            line = line.strip()
                                            
                                            if 5 <= len(line) <= 200:
                                                system_words = ['å…³æ³¨', 'ç‚¹èµ', 'æ”¶è—', 'åˆ†äº«', 'è¯„è®º', 'å›å¤', 'æŸ¥çœ‹æ›´å¤š', 'å±•å¼€', 'æ”¶èµ·', 'è½¬å‘', 'ä¸¾æŠ¥', 'æŠ–éŸ³', 'è®°å½•ç¾å¥½ç”Ÿæ´»']
                                                if not any(sys_word in line for sys_word in system_words):
                                                    # æ£€æŸ¥æ˜¯å¦åƒè¯„è®ºï¼ˆåŒ…å«å¸¸è§è¯„è®ºå…³é”®è¯ï¼‰
                                                    if any(kw in line for kw in ['è¯´', 'è§‰å¾—', 'çœŸçš„', 'å¤ª', 'å¥½', 'å·®', 'é¿é›·', 'å‘', 'é€€è´¹', 'éª—å±€', 'æŠ•è¯‰', 'ä¸è¦', 'åƒä¸‡åˆ«', 'ï¼Ÿ', 'ï¼']):
                                                        if line not in comments:
                                                            comments.append(line)
                                                            print(f"  [æŠ–éŸ³ä¼˜åŒ–] âœ“ å…œåº•æ‰¾åˆ°è¯„è®º: {line[:50]}...")
                                                            print(f"  [æŠ–éŸ³ä¼˜åŒ–]   åŸå§‹æ–‡æœ¬å‰200å­—ç¬¦: {line[:200]}")
                                                            if len(comments) >= 5:
                                                                break
                                except Exception as e:
                                    print(f"  [æŠ–éŸ³ä¼˜åŒ–] å…œåº•ç­–ç•¥å¤±è´¥: {str(e)}")
                            
                            comments = comments[:5]  # é™åˆ¶ä¸ºå‰5æ¡
                            print(f"  [æŠ–éŸ³ä¼˜åŒ–] æœ€ç»ˆæå–åˆ° {len(comments)} æ¡è¯„è®º")
                            
                            # å…³é—­æ ‡ç­¾é¡µ
                            try:
                                new_tab.close()
                            except:
                                pass
                            
                            if title or video_url:
                                result = {
                                    "platform": "æŠ–éŸ³",
                                    "keyword": keyword,
                                    "title": title.strip() or f"è§†é¢‘ {idx}",
                                    "url": video_url,
                                    "date": date_str.strip(),
                                    "comments": comments,  # çº¯æ–‡æœ¬åˆ—è¡¨
                                    "comment_count": len(comments)
                                }
                                results.append(result)
                                logger.info(f"  âœ“ é‡‡é›†æˆåŠŸ: {title[:50] if title else 'æ— æ ‡é¢˜'}... (è¯„è®º: {len(comments)}æ¡)")
                                print(f"[æŠ–éŸ³æ·±æ½œ] âœ“ é‡‡é›†æˆåŠŸ: {title[:50] if title else 'æ— æ ‡é¢˜'}... (è¯„è®º: {len(comments)}æ¡)")
                            
                            time.sleep(random.uniform(1, 2))
                            
                        except Exception as e:
                            logger.warning(f"  å¤„ç†è§†é¢‘ {idx} å¤±è´¥: {str(e)}")
                            print(f"  [æŠ–éŸ³æ·±æ½œ] å¤„ç†è§†é¢‘ {idx} å¤±è´¥: {str(e)}")
                            # ç¡®ä¿å…³é—­æ ‡ç­¾é¡µ
                            try:
                                if new_tab:
                                    new_tab.close()
                            except:
                                pass
                            continue
                    
                    time.sleep(random.uniform(2, 3))
                    
                except Exception as e:
                    logger.error(f"é‡‡é›†å…³é”®è¯ {keyword} å¤±è´¥: {str(e)}")
                    print(f"[æŠ–éŸ³æ·±æ½œ] é‡‡é›†å…³é”®è¯ {keyword} å¤±è´¥: {str(e)}")
                    continue
            
        except Exception as e:
            logger.error(f"æŠ–éŸ³é‡‡é›†å¼‚å¸¸: {str(e)}", exc_info=True)
            print(f"[æŠ–éŸ³æ·±æ½œ] é‡‡é›†å¼‚å¸¸: {str(e)}")
        
        logger.info(f"æŠ–éŸ³é‡‡é›†å®Œæˆï¼Œå…±æ‰¾åˆ° {len(results)} æ¡æœ‰æ•ˆæ•°æ®")
        print(f"[æŠ–éŸ³æ·±æ½œ] é‡‡é›†å®Œæˆï¼Œå…±æ‰¾åˆ° {len(results)} æ¡æœ‰æ•ˆæ•°æ®")
        self.douyin_data = results
        return results
    
    def _clean_html_text(self, text: str) -> str:
        """
        äºŒæ¬¡æ¸…æ´—HTMLæ®‹ç•™æ–‡æœ¬
        
        Args:
            text: åŸå§‹æ–‡æœ¬ï¼ˆå¯èƒ½åŒ…å«HTMLæ ‡ç­¾æ®‹ç•™ï¼‰
        
        Returns:
            æ¸…æ´—åçš„çº¯æ–‡æœ¬
        """
        if not text:
            return ""
        
        # ç§»é™¤HTMLæ ‡ç­¾æ®‹ç•™
        text = re.sub(r'<[^>]+>', '', text)  # ç§»é™¤ <tag> æ ‡ç­¾
        text = re.sub(r'&[a-zA-Z]+;', '', text)  # ç§»é™¤ &nbsp; ç­‰å®ä½“
        text = re.sub(r'class\s*=\s*["\'][^"\']*["\']', '', text)  # ç§»é™¤ class="xxx"
        text = re.sub(r'id\s*=\s*["\'][^"\']*["\']', '', text)  # ç§»é™¤ id="xxx"
        text = re.sub(r'style\s*=\s*["\'][^"\']*["\']', '', text)  # ç§»é™¤ style="xxx"
        
        # ç§»é™¤å¤šä½™çš„ç©ºç™½å­—ç¬¦
        text = re.sub(r'\s+', ' ', text)
        text = text.strip()
        
        return text
    
    def get_xhs_comments(self, page) -> List[str]:
        """
        æå–å°çº¢ä¹¦è¯¦æƒ…é¡µçš„è¯„è®ºï¼ˆæ–‡æœ¬å—æš´åŠ›æå–æ³•ï¼‰
        
        Args:
            page: ChromiumPage å¯¹è±¡ï¼ˆè¯¦æƒ…é¡µï¼‰
        
        Returns:
            è¯„è®ºåˆ—è¡¨
        """
        comments = []
        print("  [å°çº¢ä¹¦è¯„è®ºæå–] å¼€å§‹æå–è¯„è®º...")
        
        try:
            # ç­‰å¾…é¡µé¢åŠ è½½
            time.sleep(3)
            print("  [å°çº¢ä¹¦è¯„è®ºæå–] é¡µé¢ç­‰å¾…å®Œæˆ")
            
            # å°è¯•å®šä½è¯„è®ºå®¹å™¨
            print("  [å°çº¢ä¹¦è¯„è®ºæå–] å°è¯•å®šä½è¯„è®ºå®¹å™¨...")
            comment_container = None
            try:
                # æŸ¥æ‰¾åŒ…å«"è¯„è®º"æ–‡æœ¬çš„ div
                comment_container = page.ele('xpath://div[contains(text(), "è¯„è®º")]', timeout=3)
                if comment_container:
                    print("  [å°çº¢ä¹¦è¯„è®ºæå–] æ‰¾åˆ°è¯„è®ºå®¹å™¨ï¼ˆé€šè¿‡æ–‡æœ¬å®šä½ï¼‰")
            except:
                pass
            
            # å¦‚æœæ‰¾ä¸åˆ°ï¼Œå°è¯• class åŒ…å« comments
            if not comment_container:
                try:
                    comment_container = page.ele('xpath://div[contains(@class, "comment")]', timeout=2)
                    if comment_container:
                        print("  [å°çº¢ä¹¦è¯„è®ºæå–] æ‰¾åˆ°è¯„è®ºå®¹å™¨ï¼ˆé€šè¿‡ class å®šä½ï¼‰")
                except:
                    pass
            
            # æš´åŠ›æ–¹æ¡ˆï¼šç›´æ¥è·å–é¡µé¢æ–‡æœ¬å—
            if not comment_container:
                print("  [å°çº¢ä¹¦è¯„è®ºæå–] æœªæ‰¾åˆ°ç‰¹å®šå®¹å™¨ï¼Œä½¿ç”¨æš´åŠ›æ–‡æœ¬æå–æ³•...")
                try:
                    # è·å–é¡µé¢æ‰€æœ‰ div
                    all_divs = page.eles('tag:div', timeout=3)
                    print(f"  [å°çº¢ä¹¦è¯„è®ºæå–] æ‰¾åˆ° {len(all_divs)} ä¸ª div å…ƒç´ ")
                    
                    system_words = ['å…³æ³¨', 'ç‚¹èµ', 'æ”¶è—', 'åˆ†äº«', 'è¯„è®º', 'å›å¤', 'æŸ¥çœ‹æ›´å¤š', 'å±•å¼€', 'æ”¶èµ·']
                    
                    for div in all_divs:
                        try:
                            text = div.text or ""
                            text = text.strip()
                            
                            # è¿‡æ»¤æ¡ä»¶ï¼šå­—æ•°åœ¨ 10-50 å­—ï¼Œä¸”ä¸åŒ…å«ç³»ç»Ÿè¯
                            if 10 <= len(text) <= 50:
                                # æ’é™¤ç³»ç»Ÿè¯
                                if not any(sys_word in text for sys_word in system_words):
                                    # æ£€æŸ¥æ˜¯å¦åƒè¯„è®ºï¼ˆåŒ…å«å¸¸è§è¯„è®ºå…³é”®è¯ï¼‰
                                    if any(kw in text for kw in ['è¯´', 'è§‰å¾—', 'çœŸçš„', 'å¤ª', 'å¥½', 'å·®', 'é¿é›·', 'å‘', 'é€€è´¹', 'éª—å±€', 'æŠ•è¯‰', 'ä¸è¦', 'åƒä¸‡åˆ«']):
                                        if text not in comments:
                                            comments.append(text)
                                            print(f"  [å°çº¢ä¹¦è¯„è®ºæå–] âœ“ æ‰¾åˆ°è¯„è®º: {text[:50]}...")
                                            if len(comments) >= 5:
                                                break
                        except:
                            continue
                    
                    print(f"  [å°çº¢ä¹¦è¯„è®ºæå–] æš´åŠ›æå–å®Œæˆï¼Œæ‰¾åˆ° {len(comments)} æ¡è¯„è®º")
                    
                except Exception as e:
                    print(f"  [å°çº¢ä¹¦è¯„è®ºæå–] æš´åŠ›æå–å¤±è´¥: {str(e)}")
            else:
                # å¦‚æœæ‰¾åˆ°äº†è¯„è®ºå®¹å™¨ï¼Œåœ¨å®¹å™¨å†…æå–
                print("  [å°çº¢ä¹¦è¯„è®ºæå–] åœ¨è¯„è®ºå®¹å™¨å†…æå–...")
                try:
                    comment_items = comment_container.eles('tag:div', timeout=2)
                    print(f"  [å°çº¢ä¹¦è¯„è®ºæå–] è¯„è®ºå®¹å™¨å†…æ‰¾åˆ° {len(comment_items)} ä¸ªå­å…ƒç´ ")
                    
                    for item in comment_items[:10]:
                        text = item.text or ""
                        text = text.strip()
                        if 10 <= len(text) <= 200 and text:
                            if text not in comments:
                                comments.append(text)
                                print(f"  [å°çº¢ä¹¦è¯„è®ºæå–] âœ“ æ‰¾åˆ°è¯„è®º: {text[:50]}...")
                                if len(comments) >= 5:
                                    break
                except Exception as e:
                    print(f"  [å°çº¢ä¹¦è¯„è®ºæå–] å®¹å™¨å†…æå–å¤±è´¥: {str(e)}")
            
            # å¦‚æœè¿˜æ˜¯æ‰¾ä¸åˆ°ï¼Œå°è¯•æ­£åˆ™æå–é¡µé¢ HTML
            if not comments:
                print("  [å°çº¢ä¹¦è¯„è®ºæå–] å°è¯•ä» HTML ä¸­æ­£åˆ™æå–...")
                try:
                    page_html = page.html or ""
                    # æŸ¥æ‰¾"è¯„è®º"å…³é”®å­—ä¹‹åçš„æ–‡æœ¬
                    comment_match = re.search(r'è¯„è®º[^<]*', page_html, re.IGNORECASE)
                    if comment_match:
                        comment_section = page_html[comment_match.start():comment_match.start()+500]
                        # æå–å¯èƒ½çš„è¯„è®ºæ–‡æœ¬
                        text_blocks = re.findall(r'>([^<>]{10,50})<', comment_section)
                        for block in text_blocks[:5]:
                            block = block.strip()
                            if block and len(block) >= 10:
                                comments.append(block)
                                print(f"  [å°çº¢ä¹¦è¯„è®ºæå–] âœ“ ä»HTMLæå–è¯„è®º: {block[:50]}...")
                except Exception as e:
                    print(f"  [å°çº¢ä¹¦è¯„è®ºæå–] HTML æå–å¤±è´¥: {str(e)}")
            
            comments = comments[:5]
            print(f"  [å°çº¢ä¹¦è¯„è®ºæå–] æœ€ç»ˆæå–åˆ° {len(comments)} æ¡è¯„è®º")
            
        except Exception as e:
            print(f"  [å°çº¢ä¹¦è¯„è®ºæå–] æå–è¿‡ç¨‹å‡ºé”™: {str(e)}")
        
        return comments
    
    def get_douyin_comments(self, page) -> List[str]:
        """
        æå–æŠ–éŸ³è§†é¢‘è¯¦æƒ…é¡µçš„è¯„è®ºï¼ˆä¾§è¾¹æ æ»šåŠ¨æ³•ï¼‰
        
        Args:
            page: ChromiumPage å¯¹è±¡ï¼ˆè§†é¢‘è¯¦æƒ…é¡µï¼‰
        
        Returns:
            è¯„è®ºåˆ—è¡¨
        """
        comments = []
        print("  [æŠ–éŸ³è¯„è®ºæå–] å¼€å§‹æå–è¯„è®º...")
        
        try:
            # ç­‰å¾…é¡µé¢åŠ è½½
            time.sleep(3)
            print("  [æŠ–éŸ³è¯„è®ºæå–] é¡µé¢ç­‰å¾…å®Œæˆ")
            
            # å®šä½è¯„è®ºåŒºï¼ˆé€šå¸¸åœ¨å³ä¾§ä¾§è¾¹æ ï¼‰
            print("  [æŠ–éŸ³è¯„è®ºæå–] å°è¯•å®šä½è¯„è®ºåŒº...")
            comment_container = None
            
            try:
                # å°è¯•å¤šç§æ–¹å¼å®šä½è¯„è®ºåŒº
                selectors = [
                    'xpath://div[contains(@class, "comment")]',
                    'xpath://div[contains(text(), "è¯„è®º")]',
                    'xpath://div[@id*="comment"]',
                ]
                
                for selector in selectors:
                    try:
                        comment_container = page.ele(selector, timeout=2)
                        if comment_container:
                            print(f"  [æŠ–éŸ³è¯„è®ºæå–] æ‰¾åˆ°è¯„è®ºå®¹å™¨ï¼ˆä½¿ç”¨: {selector}ï¼‰")
                            break
                    except:
                        continue
            except Exception as e:
                print(f"  [æŠ–éŸ³è¯„è®ºæå–] å®šä½è¯„è®ºå®¹å™¨å¤±è´¥: {str(e)}")
            
            # å¦‚æœæ‰¾åˆ°è¯„è®ºå®¹å™¨ï¼Œç§»åŠ¨é¼ æ ‡å¹¶æ»šåŠ¨
            if comment_container:
                try:
                    print("  [æŠ–éŸ³è¯„è®ºæå–] ç§»åŠ¨é¼ æ ‡åˆ°è¯„è®ºåŒº...")
                    comment_container.scroll.to_see()
                    time.sleep(1)
                    
                    print("  [æŠ–éŸ³è¯„è®ºæå–] å‘ä¸‹æ»šåŠ¨åŠ è½½è¯„è®º...")
                    page.scroll.down(500)
                    time.sleep(2)
                    page.scroll.down(500)
                    time.sleep(2)
                    print("  [æŠ–éŸ³è¯„è®ºæå–] æ»šåŠ¨å®Œæˆ")
                except Exception as e:
                    print(f"  [æŠ–éŸ³è¯„è®ºæå–] æ»šåŠ¨æ“ä½œå¤±è´¥: {str(e)}")
            else:
                # å¦‚æœæ‰¾ä¸åˆ°ï¼Œç›´æ¥æ»šåŠ¨é¡µé¢
                print("  [æŠ–éŸ³è¯„è®ºæå–] æœªæ‰¾åˆ°è¯„è®ºå®¹å™¨ï¼Œç›´æ¥æ»šåŠ¨é¡µé¢...")
                try:
                    page.scroll.to_bottom()
                    time.sleep(2)
                    page.scroll.up(300)
                    time.sleep(1)
                except:
                    pass
            
            # æå–è¯„è®ºæ–‡æœ¬
            print("  [æŠ–éŸ³è¯„è®ºæå–] å¼€å§‹æå–è¯„è®ºæ–‡æœ¬...")
            try:
                # å°è¯•æŸ¥æ‰¾è¯„è®ºå…ƒç´ 
                comment_selectors = [
                    'tag:p',
                    'tag:span',
                    'tag:div',
                ]
                
                for selector in comment_selectors:
                    try:
                        elems = page.eles(selector, timeout=3)
                        print(f"  [æŠ–éŸ³è¯„è®ºæå–] ä½¿ç”¨ {selector} æ‰¾åˆ° {len(elems)} ä¸ªå…ƒç´ ")
                        
                        for elem in elems[:100]:  # æ£€æŸ¥å‰100ä¸ªå…ƒç´ 
                            try:
                                text = elem.text or ""
                                text = text.strip()
                                
                                # è¿‡æ»¤æ¡ä»¶ï¼šé•¿åº¦åœ¨ 5-200 å­—ï¼Œä¸”ä¸åŒ…å«ç³»ç»Ÿè¯
                                if 5 <= len(text) <= 200:
                                    system_words = ['å…³æ³¨', 'ç‚¹èµ', 'æ”¶è—', 'åˆ†äº«', 'è¯„è®º', 'å›å¤', 'æŸ¥çœ‹æ›´å¤š', 'å±•å¼€', 'æ”¶èµ·', 'è½¬å‘']
                                    if not any(sys_word in text for sys_word in system_words):
                                        # æ£€æŸ¥æ˜¯å¦åƒè¯„è®ºï¼ˆåŒ…å«å¸¸è§è¯„è®ºå…³é”®è¯æˆ–è¡¨æƒ…ï¼‰
                                        if any(kw in text for kw in ['è¯´', 'è§‰å¾—', 'çœŸçš„', 'å¤ª', 'å¥½', 'å·®', 'é¿é›·', 'å‘', 'é€€è´¹', 'éª—å±€', 'æŠ•è¯‰', 'ä¸è¦', 'åƒä¸‡åˆ«']) or 'emoji' in str(type(elem)):
                                            if text not in comments:
                                                comments.append(text)
                                                print(f"  [æŠ–éŸ³è¯„è®ºæå–] âœ“ æ‰¾åˆ°è¯„è®º: {text[:50]}...")
                                                if len(comments) >= 5:
                                                    break
                            except:
                                continue
                        
                        if comments:
                            break
                    except:
                        continue
                
                print(f"  [æŠ–éŸ³è¯„è®ºæå–] æå–å®Œæˆï¼Œæ‰¾åˆ° {len(comments)} æ¡è¯„è®º")
                
            except Exception as e:
                print(f"  [æŠ–éŸ³è¯„è®ºæå–] æå–è¯„è®ºæ–‡æœ¬å¤±è´¥: {str(e)}")
            
            # å¦‚æœè¿˜æ˜¯æ‰¾ä¸åˆ°ï¼Œå°è¯•ä»é¡µé¢æ–‡æœ¬ä¸­æå–
            if not comments:
                print("  [æŠ–éŸ³è¯„è®ºæå–] å°è¯•ä»é¡µé¢æ–‡æœ¬ä¸­æå–...")
                try:
                    page_text = page.html or ""
                    # æŸ¥æ‰¾å¯èƒ½çš„è¯„è®ºæ¨¡å¼
                    comment_patterns = [
                        r'([^<>]{10,100}(?:è¯´|è§‰å¾—|çœŸçš„|å¤ª|å¥½|å·®|é¿é›·|å‘|é€€è´¹)[^<>]{0,50})',
                    ]
                    
                    for pattern in comment_patterns:
                        matches = re.findall(pattern, page_text)
                        for match in matches[:5]:
                            match = match.strip()
                            if match and 10 <= len(match) <= 200:
                                comments.append(match)
                                print(f"  [æŠ–éŸ³è¯„è®ºæå–] âœ“ ä»æ–‡æœ¬æå–è¯„è®º: {match[:50]}...")
                except Exception as e:
                    print(f"  [æŠ–éŸ³è¯„è®ºæå–] æ–‡æœ¬æå–å¤±è´¥: {str(e)}")
            
            comments = comments[:5]
            print(f"  [æŠ–éŸ³è¯„è®ºæå–] æœ€ç»ˆæå–åˆ° {len(comments)} æ¡è¯„è®º")
            
        except Exception as e:
            print(f"  [æŠ–éŸ³è¯„è®ºæå–] æå–è¿‡ç¨‹å‡ºé”™: {str(e)}")
        
        return comments
    
    def fetch_from_bing(self, keyword: str) -> List[Dict[str, Any]]:
        """
        å¤‡ç”¨æ–¹æ¡ˆï¼šä» Bing æœç´¢è·å–å°çº¢ä¹¦å†…å®¹
        
        Args:
            keyword: æœç´¢å…³é”®è¯
        
        Returns:
            åŒ…å«æ ‡é¢˜ã€é“¾æ¥ã€æ¥æºçš„å­—å…¸åˆ—è¡¨
        """
        if not BING_BACKUP_AVAILABLE:
            logger.warning("Bing å¤‡ç”¨æ–¹æ¡ˆä¸å¯ç”¨ï¼ˆç¼ºå°‘ requests æˆ– BeautifulSoupï¼‰")
            return []
        
        results = []
        try:
            logger.info(f"[Bingå¤‡ç”¨] æœç´¢å…³é”®è¯: {keyword}")
            
            # æ„é€ æœç´¢è¯ï¼šsite:xiaohongshu.com {keyword} after:2023-10-01
            search_query = f"site:xiaohongshu.com {keyword}"
            bing_url = f"https://www.bing.com/search?q={search_query}&count=10"
            
            headers = {
                'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36'
            }
            
            response = requests.get(bing_url, headers=headers, timeout=10)
            response.raise_for_status()
            
            soup = BeautifulSoup(response.text, 'html.parser')
            
            # æŸ¥æ‰¾æœç´¢ç»“æœ
            search_results = soup.find_all('li', class_='b_algo')[:10]
            
            for result in search_results:
                try:
                    # æå–æ ‡é¢˜
                    title_elem = result.find('h2')
                    if not title_elem:
                        continue
                    
                    title = title_elem.get_text(strip=True)
                    
                    # æå–é“¾æ¥
                    link_elem = title_elem.find('a')
                    if not link_elem:
                        continue
                    
                    url = link_elem.get('href', '')
                    
                    # æå–æ‘˜è¦
                    snippet_elem = result.find('p')
                    snippet = snippet_elem.get_text(strip=True) if snippet_elem else ""
                    
                    # åªä¿ç•™å°çº¢ä¹¦é“¾æ¥
                    if 'xiaohongshu.com' in url:
                        results.append({
                            "platform": "å°çº¢ä¹¦",
                            "keyword": keyword,
                            "title": title,
                            "url": url,
                            "snippet": snippet,
                            "date": "",
                            "has_negative": False,
                            "comments": [],
                            "comment_count": 0
                        })
                        logger.info(f"  [Bingå¤‡ç”¨] âœ“ æ‰¾åˆ°: {title[:50]}...")
                        
                except Exception as e:
                    logger.debug(f"  å¤„ç† Bing ç»“æœå¤±è´¥: {str(e)}")
                    continue
            
            logger.info(f"[Bingå¤‡ç”¨] å…±æ‰¾åˆ° {len(results)} æ¡ç»“æœ")
            
        except Exception as e:
            logger.warning(f"Bing å¤‡ç”¨æ–¹æ¡ˆå¤±è´¥: {str(e)}")
        
        return results
    
    def crawl_xhs(self) -> List[Dict[str, Any]]:
        """
        ç¬¬äºŒé˜¶æ®µï¼šå°çº¢ä¹¦é‡‡é›†ï¼ˆåˆ—è¡¨é¡µæŠ“å– - ä¸è¿›å…¥è¯¦æƒ…é¡µï¼‰
        
        æ ¸å¿ƒç­–ç•¥ï¼š
        1. ä½¿ç”¨ DrissionPage æ¥ç®¡å·²ç™»å½•çš„ Chrome æµè§ˆå™¨ï¼ˆç«¯å£ 9222ï¼‰
        2. åªæŠ“å–åˆ—è¡¨é¡µèƒ½çœ‹åˆ°çš„æ ‡é¢˜ã€é“¾æ¥ã€ä½œè€…å
        3. ä¸è¿›å…¥è¯¦æƒ…é¡µï¼Œé¿å…è¢«é£æ§å±è”½
        4. å¦‚æœåˆ—è¡¨é¡µè·å–å¤±è´¥ï¼Œä½¿ç”¨ Bing æœç´¢ä½œä¸ºå¤‡ç”¨æ–¹æ¡ˆ
        """
        if not self.page:
            logger.error("æµè§ˆå™¨æœªåˆå§‹åŒ–")
            return []
        
        logger.info("=" * 80)
        logger.info("ç¬¬äºŒé˜¶æ®µï¼šå°çº¢ä¹¦é‡‡é›†ï¼ˆåˆ—è¡¨é¡µæŠ“å–æ¨¡å¼ï¼‰")
        logger.info("=" * 80)
        
        results = []
        
        try:
            for keyword in KEYWORDS:
                try:
                    logger.info(f"æœç´¢å…³é”®è¯: {keyword}")
                    print(f"[å°çº¢ä¹¦åˆ—è¡¨é¡µ] æœç´¢å…³é”®è¯: {keyword}")
                    
                    # è®¿é—®å°çº¢ä¹¦æœç´¢ç»“æœé¡µ
                    search_url = f"https://www.xiaohongshu.com/search_result?keyword={keyword}&source=web_search_result_notes"
                    self.page.get(search_url)
                    
                    logger.info(f"å½“å‰é¡µé¢ URL: {self.page.url}")
                    print(f"[å°çº¢ä¹¦åˆ—è¡¨é¡µ] å½“å‰é¡µé¢ URL: {self.page.url}")
                    
                    # Smart Wait: ç­‰å¾…é¡µé¢å…ƒç´ åŠ è½½ï¼ˆæ£€æµ‹ .note-item æ˜¯å¦å‡ºç°ï¼‰
                    print("[å°çº¢ä¹¦åˆ—è¡¨é¡µ] Smart Wait: ç­‰å¾…ç¬”è®°å¡ç‰‡åŠ è½½...")
                    note_items = []
                    max_wait_time = 10  # æœ€å¤šç­‰å¾…10ç§’
                    wait_interval = 1
                    waited_time = 0
                    
                    while waited_time < max_wait_time:
                        try:
                            # å°è¯•å¤šç§é€‰æ‹©å™¨æŸ¥æ‰¾ç¬”è®°å¡ç‰‡
                            selectors = [
                                'css:.note-item',
                                'css:.feed-item',
                                'css:[class*="note"]',
                                'css:[class*="feed"]',
                                'xpath://div[contains(@class, "note")]',
                                'xpath://div[contains(@class, "feed")]',
                            ]
                            
                            for selector in selectors:
                                try:
                                    items = self.page.eles(selector, timeout=1)
                                    if items:
                                        note_items = items
                                        print(f"  [å°çº¢ä¹¦åˆ—è¡¨é¡µ] âœ“ æ‰¾åˆ° {len(items)} ä¸ªç¬”è®°å¡ç‰‡ï¼ˆä½¿ç”¨: {selector}ï¼‰")
                                        break
                                except:
                                    continue
                            
                            if note_items:
                                break
                            
                            time.sleep(wait_interval)
                            waited_time += wait_interval
                            print(f"  [å°çº¢ä¹¦åˆ—è¡¨é¡µ] ç­‰å¾…ä¸­... ({waited_time}/{max_wait_time}ç§’)")
                            
                        except Exception as e:
                            logger.debug(f"  Smart Wait æ£€æµ‹å¤±è´¥: {str(e)}")
                            time.sleep(wait_interval)
                            waited_time += wait_interval
                    
                    if not note_items:
                        logger.warning(f"æœªæ‰¾åˆ°ç¬”è®°å¡ç‰‡ï¼Œå°è¯•æ»šåŠ¨åŠ è½½...")
                        print("[å°çº¢ä¹¦åˆ—è¡¨é¡µ] æœªæ‰¾åˆ°ç¬”è®°å¡ç‰‡ï¼Œå°è¯•æ»šåŠ¨åŠ è½½...")
                    
                    # Scroll: æ¨¡æ‹Ÿé¼ æ ‡æ»šè½®å‘ä¸‹æ»šåŠ¨é¡µé¢ 3-5 æ¬¡ï¼ˆæ¯æ¬¡é—´éš” 2 ç§’ï¼‰
                    scroll_count = random.randint(3, 5)
                    print(f"[å°çº¢ä¹¦åˆ—è¡¨é¡µ] å¼€å§‹æ»šåŠ¨åŠ è½½ï¼ˆ{scroll_count} æ¬¡ï¼‰...")
                    for scroll_idx in range(scroll_count):
                        try:
                            self.page.scroll.down(800)
                            time.sleep(2)
                            print(f"  [å°çº¢ä¹¦åˆ—è¡¨é¡µ] æ»šåŠ¨ {scroll_idx + 1}/{scroll_count} å®Œæˆ")
                            
                            # æ»šåŠ¨åå†æ¬¡å°è¯•æŸ¥æ‰¾ç¬”è®°å¡ç‰‡
                            if not note_items:
                                try:
                                    items = self.page.eles('css:.note-item', timeout=2)
                                    if items:
                                        note_items = items
                                        print(f"  [å°çº¢ä¹¦åˆ—è¡¨é¡µ] âœ“ æ»šåŠ¨åæ‰¾åˆ° {len(items)} ä¸ªç¬”è®°å¡ç‰‡")
                                except:
                                    pass
                        except Exception as e:
                            print(f"  [å°çº¢ä¹¦åˆ—è¡¨é¡µ] æ»šåŠ¨å¤±è´¥: {str(e)}")
                    
                    # Data Extraction: ä»åˆ—è¡¨é¡µæå–æ•°æ®ï¼ˆä¸è¿›å…¥è¯¦æƒ…é¡µï¼‰
                    print("[å°çº¢ä¹¦åˆ—è¡¨é¡µ] å¼€å§‹æå–åˆ—è¡¨é¡µæ•°æ®...")
                    keyword_results = []
                    
                    if note_items:
                        print(f"[å°çº¢ä¹¦åˆ—è¡¨é¡µ] æ‰¾åˆ° {len(note_items)} ä¸ªç¬”è®°å¡ç‰‡ï¼Œå¼€å§‹æå–...")
                        
                        for idx, card in enumerate(note_items[:10], 1):  # æœ€å¤šå¤„ç†10ä¸ª
                            try:
                                # æå–æ ‡é¢˜
                                title = ""
                                try:
                                    # å°è¯•å¤šç§æ–¹å¼æå–æ ‡é¢˜
                                    title_selectors = [
                                        'tag:h2',
                                        'tag:h3',
                                        'tag:a',
                                        'tag:div@class*=title',
                                        'tag:span@class*=title',
                                    ]
                                    
                                    for sel in title_selectors:
                                        try:
                                            title_elem = card.ele(sel, timeout=1)
                                            if title_elem:
                                                title = title_elem.text or ""
                                                if title:
                                                    break
                                        except:
                                            continue
                                except:
                                    pass
                                
                                # æå–é“¾æ¥
                                url = ""
                                try:
                                    link_elem = card.ele('tag:a', timeout=1)
                                    if link_elem:
                                        href = link_elem.attr('href') or ''
                                        
                                        # å¤„ç†ç›¸å¯¹é“¾æ¥
                                        if href:
                                            if not href.startswith('http'):
                                                if href.startswith('//'):
                                                    href = 'https:' + href
                                                elif href.startswith('/'):
                                                    href = 'https://www.xiaohongshu.com' + href
                                                else:
                                                    continue
                                            
                                            if 'explore' in href and 'xiaohongshu.com' in href:
                                                url = href
                                except:
                                    pass
                                
                                # æå–ä½œè€…å
                                author = ""
                                try:
                                    author_selectors = [
                                        'tag:span@class*=author',
                                        'tag:div@class*=author',
                                        'tag:a@class*=user',
                                    ]
                                    
                                    for sel in author_selectors:
                                        try:
                                            author_elem = card.ele(sel, timeout=1)
                                            if author_elem:
                                                author = author_elem.text or ""
                                                if author:
                                                    break
                                        except:
                                            continue
                                except:
                                    pass
                                
                                # æå–å°é¢æ–‡å­—ï¼ˆå¦‚æœæœ‰ï¼‰
                                snippet = ""
                                try:
                                    desc_elem = card.ele('tag:div@class*=desc', timeout=1)
                                    if desc_elem:
                                        snippet = desc_elem.text or ""
                                except:
                                    pass
                                
                                # åªä¿å­˜æœ‰æ ‡é¢˜æˆ–é“¾æ¥çš„æ•°æ®
                                if title or url:
                                    # æ£€æŸ¥æ˜¯å¦åŒ…å«è´Ÿé¢å…³é”®è¯
                                    negative_keywords = ['é¿é›·', 'å‘', 'é€€è´¹', 'éª—å±€', 'æŠ•è¯‰', 'å·®è¯„', 'åƒåœ¾', 'ä¸è¦', 'åƒä¸‡åˆ«', 'åµæ¶']
                                    has_negative = any(kw in (title + snippet) for kw in negative_keywords)
                                    
                                    result = {
                                        "platform": "å°çº¢ä¹¦",
                                        "keyword": keyword,
                                        "title": title.strip() or f"ç¬”è®° {idx}",
                                        "url": url,
                                        "date": "",  # åˆ—è¡¨é¡µé€šå¸¸æ²¡æœ‰å‘å¸ƒæ—¶é—´
                                        "snippet": snippet.strip(),
                                        "author": author.strip(),  # æ–°å¢ä½œè€…å­—æ®µ
                                        "has_negative": has_negative,
                                        "comments": [],  # åˆ—è¡¨é¡µä¸æŠ“å–è¯„è®º
                                        "comment_count": 0
                                    }
                                    
                                    keyword_results.append(result)
                                    logger.info(f"  âœ“ æå–æˆåŠŸ: {title[:50] if title else 'æ— æ ‡é¢˜'}... (ä½œè€…: {author[:20] if author else 'æœªçŸ¥'})")
                                    print(f"[å°çº¢ä¹¦åˆ—è¡¨é¡µ] âœ“ æå–æˆåŠŸ: {title[:50] if title else 'æ— æ ‡é¢˜'}... (ä½œè€…: {author[:20] if author else 'æœªçŸ¥'})")
                                    
                                    if len(keyword_results) >= 5:
                                        break
                                    
                            except Exception as e:
                                logger.debug(f"  å¤„ç†ç¬”è®°å¡ç‰‡ {idx} å¤±è´¥: {str(e)}")
                                continue
                    
                    # å¦‚æœåˆ—è¡¨é¡µè·å–å¤±è´¥ï¼Œä½¿ç”¨ Bing å¤‡ç”¨æ–¹æ¡ˆ
                    if not keyword_results:
                        logger.warning(f"åˆ—è¡¨é¡µæœªè·å–åˆ°æ•°æ®ï¼Œå¯ç”¨ Bing å¤‡ç”¨æ–¹æ¡ˆ...")
                        print(f"[å°çº¢ä¹¦åˆ—è¡¨é¡µ] åˆ—è¡¨é¡µæœªè·å–åˆ°æ•°æ®ï¼Œå¯ç”¨ Bing å¤‡ç”¨æ–¹æ¡ˆ...")
                        keyword_results = self.fetch_from_bing(keyword)
                    
                    results.extend(keyword_results)
                    time.sleep(random.uniform(2, 3))
                    
                except Exception as e:
                    logger.error(f"é‡‡é›†å…³é”®è¯ {keyword} å¤±è´¥: {str(e)}")
                    print(f"[å°çº¢ä¹¦åˆ—è¡¨é¡µ] é‡‡é›†å…³é”®è¯ {keyword} å¤±è´¥: {str(e)}")
                    continue
            
        except Exception as e:
            logger.error(f"å°çº¢ä¹¦é‡‡é›†å¼‚å¸¸: {str(e)}", exc_info=True)
            print(f"[å°çº¢ä¹¦åˆ—è¡¨é¡µ] é‡‡é›†å¼‚å¸¸: {str(e)}")
        
        logger.info(f"å°çº¢ä¹¦é‡‡é›†å®Œæˆï¼Œå…±æ‰¾åˆ° {len(results)} æ¡æœ‰æ•ˆæ•°æ®")
        print(f"[å°çº¢ä¹¦åˆ—è¡¨é¡µ] é‡‡é›†å®Œæˆï¼Œå…±æ‰¾åˆ° {len(results)} æ¡æœ‰æ•ˆæ•°æ®")
        self.xhs_data = results
        return results
    
    def crawl_wechat(self) -> List[Dict[str, Any]]:
        """
        ç¬¬äºŒé˜¶æ®µï¼šæœç‹—å¾®ä¿¡é‡‡é›†
        æŠ“å–æ–‡ç« æ ‡é¢˜ã€æ‘˜è¦ã€æ—¶é—´ï¼ˆæ— éœ€è¿›å…¥è¯¦æƒ…é¡µï¼‰
        """
        if not self.page:
            logger.error("æµè§ˆå™¨æœªåˆå§‹åŒ–")
            return []
        
        logger.info("=" * 80)
        logger.info("ç¬¬äºŒé˜¶æ®µï¼šæœç‹—å¾®ä¿¡é‡‡é›†")
        logger.info("=" * 80)
        
        results = []
        
        try:
            # åˆ‡æ¢åˆ°æœç‹—å¾®ä¿¡æ ‡ç­¾é¡µ
            self.page.get('https://weixin.sogou.com')
            time.sleep(3)
            
            for keyword in KEYWORDS:
                try:
                    logger.info(f"æœç´¢å…³é”®è¯: {keyword}")
                    
                    # è®¿é—®æœç´¢ç»“æœé¡µ
                    search_url = f"https://weixin.sogou.com/weixin?type=2&query={keyword}"
                    self.page.get(search_url)
                    time.sleep(random.uniform(3, 5))
                    
                    # è°ƒè¯•ä¿¡æ¯ï¼šæ‰“å°å½“å‰é¡µé¢çŠ¶æ€
                    logger.info(f"å½“å‰é¡µé¢ URL: {self.page.url}")
                    
                    # å¤„ç†å¯èƒ½çš„éªŒè¯ç 
                    try:
                        verify_img = self.page.ele('tag:img@id=seccodeImage', timeout=2)
                        if verify_img:
                            logger.warning("âš ï¸ æ£€æµ‹åˆ°éªŒè¯ç ï¼Œè¯·æ‰‹åŠ¨å®ŒæˆéªŒè¯åæŒ‰å›è½¦ç»§ç»­...")
                            try:
                                input("éªŒè¯å®ŒæˆåæŒ‰å›è½¦ç»§ç»­...")
                            except EOFError:
                                time.sleep(10)
                    except:
                        pass
                    
                    # æå–æ–‡ç« åˆ—è¡¨ï¼ˆå‰5æ¡ï¼‰- ä½¿ç”¨ class="txt-box" ç­–ç•¥
                    txt_boxes = []
                    try:
                        # æ–°é€»è¾‘ï¼šç›´æ¥æŸ¥æ‰¾åŒ…å« class="txt-box" çš„ div å…ƒç´ 
                        txt_boxes = self.page.eles('tag:div@class=txt-box', timeout=5)
                        logger.info(f"é¡µé¢ä¸­å…±æ‰¾åˆ° {len(txt_boxes)} ä¸ª txt-box å…ƒç´ ")
                        
                        # å¦‚æœæ‰¾ä¸åˆ°ï¼Œå°è¯•éƒ¨åˆ†åŒ¹é…
                        if not txt_boxes:
                            txt_boxes = self.page.eles('tag:div@class*=txt-box', timeout=5)
                            logger.info(f"ä½¿ç”¨éƒ¨åˆ†åŒ¹é…æ‰¾åˆ° {len(txt_boxes)} ä¸ª txt-box å…ƒç´ ")
                        
                        # é™åˆ¶ä¸ºå‰5ä¸ª
                        txt_boxes = txt_boxes[:5]
                        
                        # å¦‚æœæœªæ‰¾åˆ°ï¼Œæ‰“å°è°ƒè¯•ä¿¡æ¯
                        if not txt_boxes:
                            logger.warning(f"âš ï¸ è­¦å‘Šï¼šæœªæ‰¾åˆ° txt-box å…ƒç´ ")
                            try:
                                page_html_preview = self.page.html[:500] if hasattr(self.page, 'html') else "æ— æ³•è·å–HTML"
                                logger.warning(f"é¡µé¢å‰500å­—ç¬¦é¢„è§ˆ: {page_html_preview}")
                            except:
                                logger.warning("æ— æ³•è·å–é¡µé¢HTMLé¢„è§ˆ")
                        
                    except Exception as e:
                        logger.warning(f"æå–æ–‡ç« åˆ—è¡¨å¤±è´¥: {str(e)}")
                        # æ‰“å°è°ƒè¯•ä¿¡æ¯
                        try:
                            page_html_preview = self.page.html[:500] if hasattr(self.page, 'html') else "æ— æ³•è·å–HTML"
                            logger.warning(f"é”™è¯¯æ—¶é¡µé¢å‰500å­—ç¬¦é¢„è§ˆ: {page_html_preview}")
                        except:
                            pass
                        continue
                    
                    if not txt_boxes:
                        logger.warning(f"æœªæ‰¾åˆ°ä»»ä½•æ–‡ç« ï¼Œè·³è¿‡å…³é”®è¯: {keyword}")
                        continue
                    
                    # æå–æ–‡ç« ä¿¡æ¯
                    for idx, txt_box in enumerate(txt_boxes, 1):
                        try:
                            # åœ¨ txt-box ä¸‹æŸ¥æ‰¾ h3 a è·å–æ ‡é¢˜å’Œé“¾æ¥
                            title = ""
                            url = ""
                            
                            try:
                                # æŸ¥æ‰¾ h3 ä¸‹çš„ a æ ‡ç­¾
                                h3_elem = txt_box.ele('tag:h3', timeout=1)
                                if h3_elem:
                                    link_elem = h3_elem.ele('tag:a', timeout=1)
                                    if link_elem:
                                        url = link_elem.attr('href') or ""
                                        title = link_elem.text or ""
                                    
                                    # å¦‚æœ h3 ä¸‹æ²¡æœ‰ aï¼Œç›´æ¥å– h3 æ–‡æœ¬
                                    if not title:
                                        title = h3_elem.text or ""
                            except Exception as e:
                                logger.debug(f"    æå–æ ‡é¢˜å¤±è´¥: {str(e)}")
                            
                            # å¤„ç†é“¾æ¥
                            if url and not url.startswith('http'):
                                if url.startswith('//'):
                                    url = 'https:' + url
                                elif url.startswith('/'):
                                    url = 'https://weixin.sogou.com' + url
                            
                            # åœ¨ txt-box ä¸‹æŸ¥æ‰¾ p è·å–æ‘˜è¦
                            snippet = ""
                            try:
                                p_elem = txt_box.ele('tag:p', timeout=1)
                                if p_elem:
                                    snippet = p_elem.text or ""
                            except:
                                pass
                            
                            # æå–å‘å¸ƒæ—¶é—´ï¼ˆé€šå¸¸åœ¨ txt-box åŒçº§æˆ–çˆ¶çº§ï¼‰
                            date_str = ""
                            try:
                                # å°è¯•åœ¨ txt-box å†…æŸ¥æ‰¾æ—¶é—´
                                date_elem = txt_box.ele('tag:span@class*=time', timeout=1)
                                if not date_elem:
                                    date_elem = txt_box.ele('tag:span', timeout=1)
                                if date_elem:
                                    date_text = date_elem.text or ""
                                    # æ£€æŸ¥æ˜¯å¦åŒ…å«æ—¥æœŸæ ¼å¼
                                    if any(kw in date_text for kw in ['-', 'å¹´', 'æœˆ', 'æ—¥', 'å‰', 'å°æ—¶', 'åˆ†é’Ÿ']):
                                        date_str = date_text
                            except:
                                pass
                            
                            # æ—¶é—´è¿‡æ»¤ï¼šåªä¿ç•™3å¤©å†…
                            if date_str and not self.is_recent(date_str):
                                logger.info(f"    æ–‡ç« è¶…å‡º3å¤©èŒƒå›´ï¼Œè·³è¿‡: {date_str}")
                                continue
                            
                            if title or url:
                                result = {
                                    "platform": "æœç‹—å¾®ä¿¡",
                                    "keyword": keyword,
                                    "title": title.strip() or f"æ–‡ç«  {idx}",
                                    "url": url or "",
                                    "date": date_str.strip(),
                                    "snippet": snippet.strip(),
                                    "comments": [],  # å¾®ä¿¡æ–‡ç« ä¸æŠ“è¯„è®º
                                    "comment_count": 0
                                }
                                results.append(result)
                                logger.info(f"  âœ“ é‡‡é›†æˆåŠŸ: {title[:50] if title else 'æ— æ ‡é¢˜'}...")
                            
                        except Exception as e:
                            logger.warning(f"  å¤„ç†æ–‡ç«  {idx} å¤±è´¥: {str(e)}")
                            continue
                    
                    time.sleep(random.uniform(2, 3))
                    
                except Exception as e:
                    logger.error(f"é‡‡é›†å…³é”®è¯ {keyword} å¤±è´¥: {str(e)}")
                    continue
            
        except Exception as e:
            logger.error(f"æœç‹—å¾®ä¿¡é‡‡é›†å¼‚å¸¸: {str(e)}", exc_info=True)
        
        logger.info(f"æœç‹—å¾®ä¿¡é‡‡é›†å®Œæˆï¼Œå…±æ‰¾åˆ° {len(results)} æ¡æœ‰æ•ˆæ•°æ®")
        self.wechat_data = results
        return results
    
    def save_raw_data(self):
        """ä¿å­˜åŸå§‹æ•°æ®åˆ° CSV"""
        all_data = self.douyin_data + self.xhs_data + self.wechat_data
        
        if not all_data:
            logger.warning("æ²¡æœ‰æ•°æ®å¯ä¿å­˜")
            return
        
        # è½¬æ¢ä¸º DataFrame
        df_data = []
        for item in all_data:
            row = {
                "å¹³å°": item.get("platform", ""),
                "å…³é”®è¯": item.get("keyword", ""),
                "æ ‡é¢˜": item.get("title", ""),
                "é“¾æ¥": item.get("url", ""),
                "å‘å¸ƒæ—¶é—´": item.get("date", ""),
                "æ‘˜è¦": item.get("snippet", ""),
                "ä½œè€…": item.get("author", ""),  # æ–°å¢ä½œè€…å­—æ®µï¼ˆå°çº¢ä¹¦ï¼‰
                "åŒ…å«è´Ÿé¢": item.get("has_negative", False),
                "è¯„è®ºæ•°": item.get("comment_count", 0),
                "è¯„è®ºå†…å®¹": "\n".join(item.get("comments", []))
            }
            df_data.append(row)
        
        df = pd.DataFrame(df_data)
        df.to_csv('raw_data.csv', index=False, encoding='utf-8-sig')
        logger.info(f"åŸå§‹æ•°æ®å·²ä¿å­˜åˆ°: raw_data.csv (å…± {len(df_data)} æ¡)")
    
    def format_data_for_ai(self, data_list: List[Dict[str, Any]]) -> str:
        """
        å°†æ•°æ®æ ¼å¼åŒ–ä¸º AI æ˜“è¯»çš„æ–‡æœ¬æ ¼å¼
        
        Args:
            data_list: æ•°æ®åˆ—è¡¨
        
        Returns:
            æ ¼å¼åŒ–åçš„æ–‡æœ¬å­—ç¬¦ä¸²
        """
        if not data_list:
            return "ï¼ˆæš‚æ— æ•°æ®ï¼‰"
        
        context = ""
        negative_keywords = ['é¿é›·', 'å‘', 'é€€è´¹', 'éª—å±€', 'æŠ•è¯‰', 'å·®è¯„', 'åƒåœ¾', 'ä¸è¦', 'åƒä¸‡åˆ«', 'åµæ¶']
        
        # ä¼˜å…ˆå¤„ç†åŒ…å«è´Ÿé¢å…³é”®è¯çš„æ•°æ®
        priority_data = []
        normal_data = []
        
        for item in data_list:
            title = item.get('title', '')
            snippet = item.get('snippet', '')
            has_negative = item.get('has_negative', False)
            
            if has_negative or any(kw in (title + snippet) for kw in negative_keywords):
                priority_data.append(item)
            else:
                normal_data.append(item)
        
        # åˆå¹¶ï¼šä¼˜å…ˆæ•°æ®åœ¨å‰
        sorted_data = priority_data + normal_data
        
        # æˆªæ–­ä¿æŠ¤ï¼šå¦‚æœæ•°æ®å¤ªå¤šï¼Œåªå–å‰50æ¡
        if len(sorted_data) > 50:
            sorted_data = sorted_data[:50]
            logger.warning(f"æ•°æ®é‡è¿‡å¤§ï¼Œæˆªå–å‰50æ¡æœ€é‡è¦çš„æ•°æ®")
        
        for item in sorted_data:
            platform = item.get('platform', 'æœªçŸ¥å¹³å°')
            # ç‰¹åˆ«æ ‡æ³¨å°çº¢ä¹¦ï¼Œå› ä¸ºè¿™é‡Œé»‘æ–™æœ€å¤š
            if platform == 'å°çº¢ä¹¦':
                icon = "ğŸ“•"
            elif platform == 'æŠ–éŸ³':
                icon = "ğŸµ"
            elif platform == 'æœç‹—å¾®ä¿¡':
                icon = "ğŸŸ¢"
            else:
                icon = "ğŸ“„"
            
            context += f"ã€å¹³å°ï¼š{icon} {platform}ã€‘\n"
            context += f"å…³é”®è¯ï¼š{item.get('keyword', '')}\n"
            context += f"æ ‡é¢˜ï¼š{item.get('title', 'æ— æ ‡é¢˜')}\n"
            context += f"é“¾æ¥ï¼š{item.get('url', '')}\n"
            
            # æ·»åŠ æ‘˜è¦ï¼ˆå¦‚æœæœ‰ï¼‰
            snippet = item.get('snippet', '')
            if snippet:
                context += f"æ‘˜è¦ï¼š{snippet[:200]}\n"
            
            # é‡ç‚¹ï¼šå¦‚æœæœ‰è¯„è®ºï¼Œå¿…é¡»å…¨éƒ¨æ‹¼è¿›å»ï¼Œè¿™æ˜¯é”€å”®å›æ€¼çš„å…³é”®
            comments = item.get('comments', [])
            if comments:
                comments_text = "\n".join([f"  - {c}" for c in comments if c])
                context += f"ğŸ”¥ ç”¨æˆ·é«˜èµåæ§½ï¼š\n{comments_text}\n"
            
            # æ ‡è®°æ˜¯å¦åŒ…å«è´Ÿé¢
            if item.get('has_negative', False):
                context += "âš ï¸ åŒ…å«è´Ÿé¢å…³é”®è¯\n"
            
            context += "-------------------\n"
        
        # æ£€æŸ¥æ€»é•¿åº¦ï¼Œå¦‚æœè¶…è¿‡5000å­—ï¼Œæˆªå–
        if len(context) > 5000:
            logger.warning(f"æ ¼å¼åŒ–æ•°æ®è¿‡é•¿ ({len(context)} å­—ç¬¦)ï¼Œæˆªå–å‰5000å­—ç¬¦")
            context = context[:5000] + "\n...ï¼ˆæ•°æ®å·²æˆªæ–­ï¼‰"
        
        return context
    
    def load_data_from_csv(self) -> Dict[str, List[Dict[str, Any]]]:
        """
        ä» CSV æ–‡ä»¶åŠ è½½æ•°æ®ï¼ˆæ•°æ®æºåŒé‡ä¿éšœï¼‰
        
        Returns:
            åŒ…å« douyin_data, xhs_data, wechat_data çš„å­—å…¸
        """
        try:
            if not os.path.exists('raw_data.csv'):
                logger.warning("raw_data.csv æ–‡ä»¶ä¸å­˜åœ¨")
                return {"douyin_data": [], "xhs_data": [], "wechat_data": []}
            
            df = pd.read_csv('raw_data.csv', encoding='utf-8-sig')
            logger.info(f"ä» CSV åŠ è½½æ•°æ®ï¼Œå…± {len(df)} æ¡")
            
            douyin_data = []
            xhs_data = []
            wechat_data = []
            
            for _, row in df.iterrows():
                platform = row.get('å¹³å°', '')
                comments_text = row.get('è¯„è®ºå†…å®¹', '')
                comments = [c.strip() for c in comments_text.split('\n') if c.strip()] if comments_text else []
                
                item = {
                    "platform": platform,
                    "keyword": row.get('å…³é”®è¯', ''),
                    "title": row.get('æ ‡é¢˜', ''),
                    "url": row.get('é“¾æ¥', ''),
                    "date": row.get('å‘å¸ƒæ—¶é—´', ''),
                    "snippet": row.get('æ‘˜è¦', ''),
                    "has_negative": row.get('åŒ…å«è´Ÿé¢', False) if isinstance(row.get('åŒ…å«è´Ÿé¢'), bool) else False,
                    "comments": comments,
                    "comment_count": len(comments)
                }
                
                if platform == 'æŠ–éŸ³':
                    douyin_data.append(item)
                elif platform == 'å°çº¢ä¹¦':
                    xhs_data.append(item)
                elif platform == 'æœç‹—å¾®ä¿¡':
                    wechat_data.append(item)
            
            logger.info(f"CSV æ•°æ®åŠ è½½å®Œæˆï¼šæŠ–éŸ³ {len(douyin_data)} æ¡ï¼Œå°çº¢ä¹¦ {len(xhs_data)} æ¡ï¼Œæœç‹—å¾®ä¿¡ {len(wechat_data)} æ¡")
            return {
                "douyin_data": douyin_data,
                "xhs_data": xhs_data,
                "wechat_data": wechat_data
            }
            
        except Exception as e:
            logger.error(f"ä» CSV åŠ è½½æ•°æ®å¤±è´¥: {str(e)}")
            return {"douyin_data": [], "xhs_data": [], "wechat_data": []}
    
    def generate_report(self) -> str:
        """
        ç¬¬ä¸‰é˜¶æ®µï¼šé˜¿é‡Œåƒé—® (Qwen) é”€å† åˆ†æ
        è°ƒç”¨ dashscope.Generation.call ç”ŸæˆæŠ¥å‘Š
        """
        logger.info("=" * 80)
        logger.info("ç¬¬ä¸‰é˜¶æ®µï¼šé˜¿é‡Œåƒé—® (Qwen) é”€å† åˆ†æ")
        logger.info("=" * 80)
        
        # æ•°æ®æºåŒé‡ä¿éšœï¼šå¦‚æœå†…å­˜æ•°æ®ä¸ºç©ºï¼Œä» CSV è¯»å–
        douyin_data = self.douyin_data
        xhs_data = self.xhs_data
        wechat_data = self.wechat_data
        
        if not douyin_data and not xhs_data and not wechat_data:
            logger.warning("å†…å­˜æ•°æ®ä¸ºç©ºï¼Œå°è¯•ä» CSV æ–‡ä»¶åŠ è½½...")
            csv_data = self.load_data_from_csv()
            douyin_data = csv_data.get('douyin_data', [])
            xhs_data = csv_data.get('xhs_data', [])
            wechat_data = csv_data.get('wechat_data', [])
            logger.info(f"ä» CSV åŠ è½½åï¼šæŠ–éŸ³ {len(douyin_data)} æ¡ï¼Œå°çº¢ä¹¦ {len(xhs_data)} æ¡ï¼Œæœç‹—å¾®ä¿¡ {len(wechat_data)} æ¡")
        
        # åˆå¹¶æ‰€æœ‰æ•°æ®
        all_data_list = douyin_data + xhs_data + wechat_data
        
        if not all_data_list:
            logger.warning("æ‰€æœ‰æ•°æ®æºéƒ½ä¸ºç©ºï¼Œæ— æ³•ç”ŸæˆæŠ¥å‘Š")
            return f"""# æµ·é©¬èŒåŠ Â·å¸‚åœºé›·è¾¾æ—¥æŠ¥

**ç”Ÿæˆæ—¶é—´**: {CURRENT_DATE}

## âš ï¸ æ•°æ®é‡‡é›†ç»“æœ

- æŠ–éŸ³æ•°æ®: 0 æ¡
- å°çº¢ä¹¦æ•°æ®: 0 æ¡
- æœç‹—å¾®ä¿¡æ•°æ®: 0 æ¡

*æ³¨ï¼šæœªé‡‡é›†åˆ°ä»»ä½•æ•°æ®ï¼Œè¯·æ£€æŸ¥é‡‡é›†é€»è¾‘æˆ–ç½‘ç»œè¿æ¥ã€‚*
"""
        
        # æ ¼å¼åŒ–æ•°æ®
        formatted_data = self.format_data_for_ai(all_data_list)
        
        # è°ƒè¯•æ‰“å°
        logger.info(f"æ­£åœ¨å‘é€ç»™ AI çš„æ•°æ®é•¿åº¦: {len(formatted_data)} å­—ç¬¦")
        logger.info(f"æ•°æ®ç»Ÿè®¡ï¼šæŠ–éŸ³ {len(douyin_data)} æ¡ï¼Œå°çº¢ä¹¦ {len(xhs_data)} æ¡ï¼Œæœç‹—å¾®ä¿¡ {len(wechat_data)} æ¡")
        
        system_prompt = """ä½ ä¸æ˜¯é”€å”®ï¼Œä½ æ˜¯æµ·é©¬èŒåŠ çš„**é¦–å¸­æˆ˜ç•¥å®˜ (CSO)**ã€‚
ä½ æ‹¥æœ‰æ•é”çš„å¸‚åœºæ´å¯ŸåŠ›ã€‚è¯·æ ¹æ®é‡‡é›†åˆ°çš„å…¨ç½‘æ•°æ®ï¼ˆæŠ–éŸ³/å°çº¢ä¹¦/å¾®ä¿¡ï¼‰ï¼Œä¸ºç®¡ç†å±‚æ’°å†™ä¸€ä»½ã€Šå…¨ç½‘å¸‚åœºé›·è¾¾æ—¥æŠ¥ã€‹ã€‚

**åˆ†æé€»è¾‘ä¸è¾“å‡ºæ ¼å¼ (Markdown)**ï¼š

**ç¬¬ä¸€éƒ¨åˆ†ï¼šâš”ï¸ ç«å“åŠ¨ä½œç›‘æµ‹ (Competitor Moves)**
- æ ¸å¿ƒå…³æ³¨ï¼šç«å“ï¼ˆDBCã€é€”é¸½ã€Offerå…ˆç”Ÿç­‰ï¼‰æœ€è¿‘å‘äº†ä»€ä¹ˆæ–°äº§å“ï¼Ÿæäº†ä»€ä¹ˆæ´»åŠ¨ï¼Ÿæœ‰ä»€ä¹ˆä»·æ ¼å˜åŠ¨ï¼Ÿ
- æ ¼å¼ï¼š`[å¹³å°] ç«å“åï¼šå…·ä½“åŠ¨ä½œ`ã€‚

**ç¬¬äºŒéƒ¨åˆ†ï¼šğŸ“¢ ç”¨æˆ·èˆ†æƒ…é€è§† (Voice of Customer)**
- æ ¸å¿ƒå…³æ³¨ï¼šç”¨æˆ·åœ¨è¯„è®ºåŒºéª‚ä»€ä¹ˆï¼Ÿç—›ç‚¹åœ¨å“ªé‡Œï¼Ÿ
- **å¿…é¡»æ‘˜å½•**ï¼šä»æ•°æ®ä¸­æå– 3-5 æ¡æœ€å…·ä»£è¡¨æ€§çš„**è´Ÿé¢è¯„è®ºåŸè¯**ï¼Œä½œä¸º"ç”¨æˆ·åŸå£°"å±•ç¤ºã€‚
- æ€»ç»“ï¼šå½“å‰çš„èˆ†æƒ…çƒ­è¯æ˜¯ä»€ä¹ˆï¼ˆå¦‚ï¼šé€€è´¹éš¾ã€å¯¼å¸ˆæ°´ï¼‰ã€‚

**ç¬¬ä¸‰éƒ¨åˆ†ï¼šğŸ§­ æˆ‘ä»¬çš„æˆ˜ç•¥å¯ç¤º (Strategic Insights)**
- **è¿™æ˜¯æœ€é‡è¦çš„éƒ¨åˆ†**ã€‚åŸºäºä¸Šè¿°ç«å“åŠ¨ä½œå’Œç”¨æˆ·èˆ†æƒ…ï¼Œç»™æˆ‘ä»¬ï¼ˆæµ·é©¬èŒåŠ ï¼‰æå‡º 3 æ¡å…·ä½“çš„æˆ˜ç•¥å»ºè®®ã€‚
- *ä¸è¦å†™è¯æœ¯*ï¼Œè¦å†™ç­–ç•¥ã€‚
- ä¾‹å¦‚ï¼š'ç«å“Aå› ä¸ºé€€è´¹éš¾è¢«éª‚ -> å¯ç¤ºï¼šæˆ‘ä»¬åº”åœ¨å®£å‘ä¸­å¼ºè°ƒèµ„é‡‘ç›‘ç®¡å’Œé€æ˜é€€è´¹æµç¨‹ï¼Œå»ºç«‹ä¿¡ä»»å£å’ã€‚'

**é£æ ¼è¦æ±‚**ï¼š
- è¯­è¨€ç®€ç»ƒã€ä¸“ä¸šã€æ¯’è¾£ã€‚
- æ‹’ç»åºŸè¯ï¼Œç›´å‡»æœ¬è´¨ã€‚
- **å»ºè®®å¿…é¡»åŸºäºä»Šæ—¥æŠ“å–çš„å…·ä½“æ•°æ®ï¼Œä¸¥ç¦ç”Ÿæˆé€šç”¨å»ºè®®ã€‚**"""

        user_prompt = f"""ä»¥ä¸‹æ˜¯ä»Šæ—¥é‡‡é›†åˆ°çš„æœ€æ–°ç«å“æƒ…æŠ¥æ•°æ®ï¼ˆæœ€è¿‘3å¤©ï¼‰ï¼š

{formatted_data}

è¯·æ ¹æ®ä»¥ä¸Šæ•°æ®ç”Ÿæˆã€Šå…¨ç½‘å¸‚åœºé›·è¾¾æ—¥æŠ¥ã€‹ï¼Œæ ¼å¼ä¸º Markdownã€‚
æ¯æ¡æƒ…æŠ¥å¿…é¡»åŒ…å«åŸå§‹é“¾æ¥ï¼Œä¸¥ç¦ç¼–é€ ä¿¡æ¯ã€‚
**ç‰¹åˆ«æ³¨æ„**ï¼š
1. å¦‚æœæ•°æ®ä¸­åŒ…å«ç”¨æˆ·è¯„è®ºï¼Œå¿…é¡»æ‘˜å½•åŸè¯å±•ç¤ºã€‚
2. æˆ˜ç•¥å»ºè®®å¿…é¡»åŸºäºä¸Šè¿°å…·ä½“æ•°æ®ï¼Œä¸èƒ½å†™é€šç”¨å»ºè®®ã€‚
3. å¦‚æœæ•°æ®ä¸­æ²¡æœ‰è¯„è®ºï¼Œè¯·æ˜ç¡®è¯´æ˜"æœ¬æ¬¡æœªé‡‡é›†åˆ°ç”¨æˆ·è¯„è®ºæ•°æ®"ã€‚
4. ä¼˜å…ˆåˆ†æå°çº¢ä¹¦å¹³å°çš„è´Ÿé¢è¯„ä»·ï¼Œå› ä¸ºé€šå¸¸æœ€çœŸå®ã€‚"""

        try:
            logger.info("è°ƒç”¨é˜¿é‡Œåƒé—® (qwen-plus) API ç”ŸæˆæŠ¥å‘Š...")
            
            # ä½¿ç”¨ dashscope.Generation.call
            response = Generation.call(
                model='qwen-plus',
                messages=[
                    {'role': 'system', 'content': system_prompt},
                    {'role': 'user', 'content': user_prompt}
                ],
                result_format='message'
            )
            
            if response.status_code == 200:
                report = response.output.choices[0].message.content
                
                # æ·»åŠ æ ‡é¢˜å’Œæ—¥æœŸ
                full_report = f"# æµ·é©¬èŒåŠ Â·å¸‚åœºé›·è¾¾æ—¥æŠ¥\n\n**ç”Ÿæˆæ—¶é—´**: {CURRENT_DATE}\n\n---\n\n{report}"
                
                logger.info("é˜¿é‡Œåƒé—® æŠ¥å‘Šç”Ÿæˆå®Œæˆ")
                return full_report
            else:
                raise Exception(f"API è°ƒç”¨å¤±è´¥: {response.status_code}, {response.message}")
            
        except Exception as e:
            logger.error(f"é˜¿é‡Œåƒé—® ç”Ÿæˆå¤±è´¥: {str(e)}")
            # å¦‚æœç”Ÿæˆå¤±è´¥ï¼Œè¿”å›åŸºç¡€æŠ¥å‘Š
            return f"""# æµ·é©¬èŒåŠ Â·å¸‚åœºé›·è¾¾æ—¥æŠ¥

**ç”Ÿæˆæ—¶é—´**: {CURRENT_DATE}

## æ•°æ®ç»Ÿè®¡

- æŠ–éŸ³æ•°æ®: {len(douyin_data)} æ¡
- å°çº¢ä¹¦æ•°æ®: {len(xhs_data)} æ¡
- æœç‹—å¾®ä¿¡æ•°æ®: {len(wechat_data)} æ¡

*æ³¨ï¼šAI åˆ†æå¤±è´¥ï¼Œè¯·æŸ¥çœ‹ raw_data.csv è·å–åŸå§‹æ•°æ®ã€‚*
"""
    
    def run(self, skip_login: bool = False):
        """
        æ‰§è¡Œå®Œæ•´çš„é‡‡é›†æµç¨‹
        
        Args:
            skip_login: æ˜¯å¦è·³è¿‡ç™»å½•æ­¥éª¤ï¼ˆå‡è®¾å·²ç™»å½•ï¼‰
        """
        logger.info("=" * 80)
        logger.info("æµ·é©¬èŒåŠ Â·å¸‚åœºé›·è¾¾æ—¥æŠ¥ - å¼€å§‹è¿è¡Œ")
        logger.info("=" * 80)
        
        try:
            if not skip_login:
                # ç¬¬ä¸€é˜¶æ®µï¼šäººå·¥ç™»å½•
                self.manual_login()
            else:
                logger.info("è·³è¿‡ç™»å½•æ­¥éª¤ï¼Œå‡è®¾æµè§ˆå™¨å·²ç™»å½•")
                # æ‰“å¼€ä¸‰ä¸ªæ ‡ç­¾é¡µï¼ˆå¦‚æœè¿˜æ²¡æœ‰ï¼‰
                try:
                    tabs = self.page.tab_ids
                    if len(tabs) == 0:
                        self.page.get('https://www.douyin.com')
                        time.sleep(2)
                        self.page.new_tab()
                        self.page.get('https://www.xiaohongshu.com')
                        time.sleep(2)
                        self.page.new_tab()
                        self.page.get('https://weixin.sogou.com')
                        time.sleep(2)
                except:
                    pass
            
            # ç¬¬äºŒé˜¶æ®µï¼šå¤šå¹³å°é‡‡é›†
            self.crawl_douyin()
            self.crawl_xhs()
            self.crawl_wechat()
            
            # ä¿å­˜åŸå§‹æ•°æ®
            self.save_raw_data()
            
            # ç¬¬ä¸‰é˜¶æ®µï¼šç”ŸæˆæŠ¥å‘Š
            report = self.generate_report()
            
            # ä¿å­˜æŠ¥å‘Š
            report_file = f"Market_Radar_{CURRENT_DATE}.md"
            with open(report_file, 'w', encoding='utf-8') as f:
                f.write(report)
            
            logger.info(f"æŠ¥å‘Šå·²ä¿å­˜åˆ°: {report_file}")
            
            print(f"\nâœ… é‡‡é›†å®Œæˆï¼")
            print(f"ğŸ“Š æ•°æ®ç»Ÿè®¡ï¼šæŠ–éŸ³ {len(self.douyin_data)} æ¡ï¼Œå°çº¢ä¹¦ {len(self.xhs_data)} æ¡ï¼Œæœç‹—å¾®ä¿¡ {len(self.wechat_data)} æ¡")
            print(f"ğŸ“„ åŸå§‹æ•°æ®ï¼šraw_data.csv")
            print(f"ğŸ“‹ åˆ†ææŠ¥å‘Šï¼š{report_file}")
            
            logger.info("=" * 80)
            logger.info("å¸‚åœºé›·è¾¾æ—¥æŠ¥ç”Ÿæˆå®Œæˆ")
            logger.info("=" * 80)
            
            # å‘é€åˆ°é’‰é’‰ç¾¤
            self.send_to_dingtalk(report)
            
        except Exception as e:
            logger.error(f"è¿è¡Œå¼‚å¸¸: {str(e)}", exc_info=True)
            raise
        finally:
            # ä¸è‡ªåŠ¨å…³é—­æµè§ˆå™¨ï¼Œè®©ç”¨æˆ·æŸ¥çœ‹ç»“æœ
            logger.info("æµè§ˆå™¨ä¿æŒæ‰“å¼€çŠ¶æ€ï¼Œè¯·æ‰‹åŠ¨å…³é—­")
    
    def send_to_dingtalk(self, report_content: str):
        """
        å‘é€æŠ¥å‘Šåˆ°é’‰é’‰ç¾¤ï¼ˆç¡®ä¿æ‰€æœ‰é“¾æ¥å®Œæ•´ï¼‰
        
        Args:
            report_content: æŠ¥å‘Šå†…å®¹ï¼ˆMarkdownæ ¼å¼ï¼‰
        """
        try:
            import requests
            
            # é’‰é’‰Webhookåœ°å€
            DINGTALK_WEBHOOK = "https://oapi.dingtalk.com/robot/send?access_token=ac8d1c6332c8a047b8786a930ab08d7f6db490843edca2de1bb65c68301c3113"
            
            # è¯»å–åŸå§‹æ•°æ®ï¼Œè¡¥å……é“¾æ¥ä¿¡æ¯
            try:
                if os.path.exists('raw_data.csv'):
                    df = pd.read_csv('raw_data.csv', encoding='utf-8-sig')
                    
                    # æ„å»ºé“¾æ¥è¡¥å……ä¿¡æ¯
                    links_section = "\n\n---\n\n## ğŸ“ å®Œæ•´åŸæ–‡é“¾æ¥æ¸…å•\n\n"
                    
                    # æŒ‰å¹³å°åˆ†ç»„
                    for platform in ['æŠ–éŸ³', 'å°çº¢ä¹¦', 'æœç‹—å¾®ä¿¡']:
                        platform_data = df[df['å¹³å°'] == platform]
                        if len(platform_data) > 0:
                            links_section += f"### {platform}å¹³å°\n\n"
                            for _, row in platform_data.head(10).iterrows():
                                title = str(row.get('æ ‡é¢˜', ''))[:60]
                                url = str(row.get('é“¾æ¥', ''))
                                keyword = str(row.get('å…³é”®è¯', ''))
                                
                                if url and url != 'nan' and url.strip():
                                    links_section += f"- **{keyword}** - {title}  ğŸ”— [æŸ¥çœ‹åŸæ–‡]({url})\n"
                            links_section += "\n"
                    
                    # å°†é“¾æ¥è¡¥å……ä¿¡æ¯æ·»åŠ åˆ°æŠ¥å‘Šæœ«å°¾
                    report_content = report_content.rstrip() + links_section
            except Exception as e:
                logger.warning(f"è¡¥å……é“¾æ¥ä¿¡æ¯å¤±è´¥: {str(e)}")
            
            # å‘é€åˆ°é’‰é’‰
            payload = {
                "msgtype": "markdown",
                "markdown": {
                    "title": "æµ·é©¬èŒåŠ Â·å¸‚åœºé›·è¾¾æ—¥æŠ¥",
                    "text": report_content
                }
            }
            
            logger.info("æ­£åœ¨å‘é€æŠ¥å‘Šåˆ°é’‰é’‰ç¾¤...")
            response = requests.post(DINGTALK_WEBHOOK, json=payload, timeout=10)
            result = response.json()
            
            if result.get('errcode') == 0:
                logger.info("âœ“ é’‰é’‰æ¶ˆæ¯å‘é€æˆåŠŸ")
                print("âœ“ æŠ¥å‘Šå·²å‘é€åˆ°é’‰é’‰ç¾¤")
            else:
                logger.warning(f"âœ— é’‰é’‰æ¶ˆæ¯å‘é€å¤±è´¥: {result.get('errmsg')}")
                print(f"âœ— é’‰é’‰æ¶ˆæ¯å‘é€å¤±è´¥: {result.get('errmsg')}")
                
        except ImportError:
            logger.warning("requests åº“æœªå®‰è£…ï¼Œè·³è¿‡é’‰é’‰æ¨é€")
            print("âš   requests åº“æœªå®‰è£…ï¼Œè·³è¿‡é’‰é’‰æ¨é€")
        except Exception as e:
            logger.error(f"å‘é€é’‰é’‰æ¶ˆæ¯å¤±è´¥: {str(e)}")
            print(f"âœ— å‘é€é’‰é’‰æ¶ˆæ¯å¤±è´¥: {str(e)}")


def main():
    """ä¸»å‡½æ•°"""
    import sys
    # å¦‚æœå‘½ä»¤è¡Œå‚æ•°åŒ…å« --skip-loginï¼Œåˆ™è·³è¿‡ç™»å½•æ­¥éª¤
    skip_login = '--skip-login' in sys.argv or '--skip' in sys.argv
    
    radar = MarketRadarQwen()
    radar.run(skip_login=skip_login)


if __name__ == "__main__":
    main()
